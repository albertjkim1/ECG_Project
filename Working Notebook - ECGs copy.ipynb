{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from base64 import b64decode as decode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_data_id</th>\n",
       "      <th>waveform_id</th>\n",
       "      <th>WavfmType</th>\n",
       "      <th>lead_id</th>\n",
       "      <th>lead_byte_count_total</th>\n",
       "      <th>lead_time_offset</th>\n",
       "      <th>waveform_data</th>\n",
       "      <th>lead_sample_count_total</th>\n",
       "      <th>lead_amplitude</th>\n",
       "      <th>lead_units</th>\n",
       "      <th>...</th>\n",
       "      <th>exam_id</th>\n",
       "      <th>waveform_type</th>\n",
       "      <th>number_of_leads</th>\n",
       "      <th>Waveform_Start_Time</th>\n",
       "      <th>Sample_Type</th>\n",
       "      <th>Sample_Base</th>\n",
       "      <th>Sample_Exponent</th>\n",
       "      <th>High_Pass_Filter</th>\n",
       "      <th>Low_Pass_Filter</th>\n",
       "      <th>AC_Filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9078054</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>+P/4//j/+P/4//j/+P/5//r/+//8//z//P/7//r/+f/4/...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>9081703</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>II</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>9v/2//b/8//w//D/8P/x//L/8//0//T/9P/z//L/8f/w/...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9074278</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>/v/+//7//v/+////AAAAAAAAAQACAAIAAgACAAIAAgACA...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9066887</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V2</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>9v/1//T/9P/0//T/9P/0//T/9f/2//b/9v/2//b/9v/2/...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>9082771</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V3</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>7v/u/+7/7f/s/+z/7P/t/+7/7v/u/+7/7v/u/+7/7v/u/...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>9187141</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V4</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>KAApACoAKwAsACwALQAtAC4ALgAuAC4ALgAuAC4ALgAvA...</td>\n",
       "      <td>600</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>9190675</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V5</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>FgAXABkAGQAbABsAGwAbABsAGwAbABwAHQAeAB4AHgAfA...</td>\n",
       "      <td>600</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>9177603</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V5</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>+v/6//r/+v/7//z//f/+//z//P/8//z//v/+//7//v/+/...</td>\n",
       "      <td>5000</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>9172851</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>7v/u/+7/7v/x//L/8//0//T/9P/0//T/9P/0//T/9P/0/...</td>\n",
       "      <td>5000</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>9173608</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>CgAKAAoACwAMAAwADAAMAAwADQAOAA8AEAASABIAEgASA...</td>\n",
       "      <td>600</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lead_data_id  waveform_id  WavfmType lead_id  lead_byte_count_total  \\\n",
       "10        9078054      1095618        NaN       I                   5000   \n",
       "15        9081703      1095618        NaN      II                   5000   \n",
       "8         9074278      1095618        NaN      V1                   5000   \n",
       "1         9066887      1095618        NaN      V2                   5000   \n",
       "18        9082771      1095618        NaN      V3                   5000   \n",
       "..            ...          ...        ...     ...                    ...   \n",
       "150       9187141      1109067        NaN      V4                   1200   \n",
       "152       9190675      1109067        NaN      V5                   1200   \n",
       "155       9177603      1109067        NaN      V5                  10000   \n",
       "140       9172851      1109067        NaN      V6                  10000   \n",
       "141       9173608      1109067        NaN      V6                   1200   \n",
       "\n",
       "     lead_time_offset                                      waveform_data  \\\n",
       "10                  0   +P/4//j/+P/4//j/+P/5//r/+//8//z//P/7//r/+f/4/...   \n",
       "15                  0   9v/2//b/8//w//D/8P/x//L/8//0//T/9P/z//L/8f/w/...   \n",
       "8                   0   /v/+//7//v/+////AAAAAAAAAQACAAIAAgACAAIAAgACA...   \n",
       "1                   0   9v/1//T/9P/0//T/9P/0//T/9f/2//b/9v/2//b/9v/2/...   \n",
       "18                  0   7v/u/+7/7f/s/+z/7P/t/+7/7v/u/+7/7v/u/+7/7v/u/...   \n",
       "..                ...                                                ...   \n",
       "150                 0   KAApACoAKwAsACwALQAtAC4ALgAuAC4ALgAuAC4ALgAvA...   \n",
       "152                 0   FgAXABkAGQAbABsAGwAbABsAGwAbABwAHQAeAB4AHgAfA...   \n",
       "155                 0   +v/6//r/+v/7//z//f/+//z//P/8//z//v/+//7//v/+/...   \n",
       "140                 0   7v/u/+7/7v/x//L/8//0//T/9P/0//T/9P/0//T/9P/0/...   \n",
       "141                 0   CgAKAAoACwAMAAwADAAMAAwADQAOAA8AEAASABIAEgASA...   \n",
       "\n",
       "     lead_sample_count_total  lead_amplitude  lead_units  ...  exam_id  \\\n",
       "10                      2500            4.88  MICROVOLTS  ...   549871   \n",
       "15                      2500            4.88  MICROVOLTS  ...   549871   \n",
       "8                       2500            4.88  MICROVOLTS  ...   549871   \n",
       "1                       2500            4.88  MICROVOLTS  ...   549871   \n",
       "18                      2500            4.88  MICROVOLTS  ...   549871   \n",
       "..                       ...             ...         ...  ...      ...   \n",
       "150                      600            4.88  MICROVOLTS  ...   554080   \n",
       "152                      600            4.88  MICROVOLTS  ...   554080   \n",
       "155                     5000            4.88  MICROVOLTS  ...   554080   \n",
       "140                     5000            4.88  MICROVOLTS  ...   554080   \n",
       "141                      600            4.88  MICROVOLTS  ...   554080   \n",
       "\n",
       "     waveform_type  number_of_leads  Waveform_Start_Time         Sample_Type  \\\n",
       "10          Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "15          Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "8           Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "1           Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "18          Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "..             ...              ...                  ...                 ...   \n",
       "150         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "152         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "155         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "140         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "141         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "\n",
       "     Sample_Base  Sample_Exponent  High_Pass_Filter Low_Pass_Filter  AC_Filter  \n",
       "10           250                0                 5             150       NONE  \n",
       "15           250                0                 5             150       NONE  \n",
       "8            250                0                 5             150       NONE  \n",
       "1            250                0                 5             150       NONE  \n",
       "18           250                0                 5             150       NONE  \n",
       "..           ...              ...               ...             ...        ...  \n",
       "150          500                0                16             150       NONE  \n",
       "152          500                0                16             150       NONE  \n",
       "155          500                0                16             150       NONE  \n",
       "140          500                0                16             150       NONE  \n",
       "141          500                0                16             150       NONE  \n",
       "\n",
       "[160 rows x 29 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/d_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/d_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/d_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/d_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead data\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "waveform_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     [[4, 3, 2, -1, -4, -4, -4, -4, -4, -7, -10, -8...\n",
       "3     [[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, ...\n",
       "5     [[-22, -20, -18, -16, -14, -14, -14, -12, -10,...\n",
       "7     [[46, 45, 44, 42, 40, 35, 30, 26, 22, 18, 14, ...\n",
       "9     [[-7, -4, -1, -6, -10, -12, -14, -11, -11, -14...\n",
       "11    [[-32, -32, -32, -33, -34, -34, -34, -33, -32,...\n",
       "14    [[-8, -5, -2, -2, -2, -5, -8, -8, -8, -7, -6, ...\n",
       "16    [[-12, -12, -12, -12, -12, -12, -12, -12, -12,...\n",
       "Name: decoded_waveform, dtype: object"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "waveform_lead_concat\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: np.vstack(x))#.apply(lambda x: np.transpose(x))\n",
    "\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "#waveform_lead_median = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Median\"]\n",
    "\n",
    "waveform_lead_rhythm[\"decoded_waveform\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal sinus rhythm low voltage qrs borderline ecg\n",
      "sinus bradycardia otherwise normal ecg\n",
      "sinus tachycardia otherwise normal ecg\n",
      "normal sinus rhythm normal ecg\n",
      "normal sinus rhythm normal ecg\n",
      "normal sinus rhythm with sinus arrhythmia minimal voltage criteria for lvh may be normal variant borderline ecg\n",
      "atrial fibrillation abnormal ecg normal sinus rhythm with sinus arrhythmia normal ecg\n"
     ]
    }
   ],
   "source": [
    "# Adding the labels/sentences\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "# Let's look over this tomorrow\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "#searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "#diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "#\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if row[\"statement_order\"] == 1 and curr_string != \"\":\n",
    "        curr_string = curr_string.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        val = [curr_id, curr_string[1:]]\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    curr_string += \" \" + row[\"Full_text\"]\n",
    "\n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "#waveform_lead_rhythm_diag\n",
    "for i in waveform_lead_rhythm_diag[\"diagnosis\"]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'be', 't', 'lvh', 'abnormality', 'otherwise', 'sinus', 'ischemia', 'criteria', 'atrial', 'borderline', 'may', 'with', 'ecg', 'normal', 'variant', 'consider', 'arrhythmia', 'rhythm', 'inferior', 'tachycardia', 'fibrillation', 'voltage', 'qrs', 'wave', 'bradycardia', 'minimal', 'low', 'abnormal'}\n",
      "{'for': 1, 'be': 2, 't': 3, 'lvh': 4, 'abnormality': 5, 'otherwise': 6, 'sinus': 7, 'ischemia': 8, 'criteria': 9, 'atrial': 10, 'borderline': 11, 'may': 12, 'with': 13, 'ecg': 14, 'normal': 15, 'variant': 16, 'consider': 17, 'arrhythmia': 18, 'rhythm': 19, 'inferior': 20, 'tachycardia': 21, 'fibrillation': 22, 'voltage': 23, 'qrs': 24, 'wave': 25, 'bradycardia': 26, 'minimal': 27, 'low': 28, 'abnormal': 29, '': 0}\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for num, sentence in diagnoses:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word)\n",
    "print(unique_words)\n",
    "unique_words = list(unique_words)\n",
    "word_map = dict()\n",
    "for i, word in enumerate(unique_words):\n",
    "    word_map[word] = i+1\n",
    "word_map[\"\"] = 0\n",
    "print(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 8, 2500])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into training and testing datasets\n",
    "# y not included for now\n",
    "def one_hot(x, dict_words):\n",
    "    x = x.split(\" \")\n",
    "    array = []\n",
    "    for i in x:\n",
    "        array.append([0] + [1 if y == i else 0 for y in dict_words] + [0,0])\n",
    "    for i in range(17-len(x)):\n",
    "        array.append([1 if i == 30 else 0 for i in range(32)])\n",
    "    return array\n",
    "\n",
    "dict_words = list(unique_words)\n",
    "#waveform_lead_rhythm_diag['diagnosis'] = waveform_lead_rhythm_diag['diagnosis'].apply(lambda x: one_hot(x, dict_words))\n",
    "\n",
    "len(waveform_lead_rhythm_diag[\"diagnosis\"][5])\n",
    "train_x, test_x, train_y, test_y = train_test_split(waveform_lead_rhythm_diag['decoded_waveform'], waveform_lead_rhythm_diag['diagnosis'], test_size = 0.1, random_state = 2021)\n",
    "train_x = torch.tensor(list(train_x)).float()\n",
    "train_x.shape\n",
    "train_x = torch.tensor(list(waveform_lead_rhythm_diag['decoded_waveform'])).float()\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Conv1D Encoder w/ LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2500])\n",
      "torch.Size([1, 64, 31])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 8 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# 1D grouped encoder model\n",
    "encoder_conv = nn.Sequential()\n",
    "encoder_conv.add_module('initial_norm', nn.BatchNorm1d(8))\n",
    "encoder_conv.add_module('conv_1', nn.Conv1d(in_channels = 8, out_channels = 8, groups = 8, kernel_size = 5, padding = 2))\n",
    "for i in range(2, (J+2), 2):\n",
    "    if (i-2) == 0: \n",
    "        prev = 8\n",
    "    else:\n",
    "        prev = (i-2)*8\n",
    "    encoder_conv.add_module('conv_{num}'.format(num = int(i / 2 + 1)), nn.Conv1d(in_channels = prev, out_channels = i*8, groups = 8, kernel_size = 5, padding = 2, stride = 3))\n",
    "    encoder_conv.add_module('activation_{num}'.format(num = int(i / 2 + 1)), nn.ELU())\n",
    "    encoder_conv.add_module('batch_norm_{num}'.format(num = int(i / 2 + 1)), nn.BatchNorm1d(i*8))\n",
    "    \n",
    "#encoder_conv.add_module('final_conv', nn.Conv1d(in_channels = J * 8, out_channels = 8, groups = 8, kernel_size = 5, padding = 2))\n",
    "#encoder_conv.add_module('max_pool', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "encoder_conv.add_module('reshape', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "\n",
    "\n",
    "# summarize model, verify output is of desired shape\n",
    "print(train_x[0].shape)\n",
    "print(encoder_conv(torch.unsqueeze(train_x[0], 0)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - LSTM Encoder w/ Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 8, 500])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 512\n",
    "embedding_dim = 8\n",
    "num_words = len(unique_words)\n",
    "\n",
    "class ECG_LSTM(nn.Module):\n",
    "    def __init__(self, encoder, h_dim, e_dim, word_list_length):\n",
    "        super(ECG_LSTM, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim)\n",
    "        self.linear = nn.Linear(h_dim, word_list_length)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq_embedded = self.encoder(seq)\n",
    "        final_hidd, _ = self.lstm(seq_embedded)\n",
    "        dec_seq = self.linear(final_hidd)\n",
    "        return F.log_softmax(dec_seq)\n",
    "    \n",
    "lstm_dec = ECG_LSTM(encoder_conv, hidden_layers, embedding_dim, num_words)\n",
    "lstm_dec(train_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Basic Transformer Architecture with Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ECGTransformerEncoder(nn.Module):\n",
    "    # Takes the ECG discrete signals sequence and maps into a probability distribution of diagnosis\n",
    "    # For working/verification purposes\n",
    "    def __init__(self, vector_size, n_inputs, n_heads, hidden_linear_dim, n_layers, dropout):\n",
    "        super(ECGTransformerEncoder, self).__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.positional_encoder = PositionalEncoder(64, dropout)\n",
    "        \n",
    "        #Since our data is already discrete numbers, might need some tweaking for this\n",
    "        self.embedder = encoder_conv # SignalEmbedder(n_inputs, vector_size)\n",
    "                        #64 31              #39        64\n",
    "        \n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(64, n_heads, hidden_linear_dim, dropout),\n",
    "            n_layers)\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Simple linear decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        Transpose(64, 31),\n",
    "                        nn.Linear(31, 17),\n",
    "                        Transpose(17, 64),\n",
    "                        nn.Linear(64, 30),\n",
    "                        nn.LogSoftmax()\n",
    "                        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #self.embedder.weight.data.uniform_(-.1, .1)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        #self.decoder.weight.data.uniform_(-.1, .1)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x) # * math.sqrt(self.n_inputs)\n",
    "        \n",
    "        x = x.squeeze(0)\n",
    "        x = x.view(31,64)\n",
    "        x = self.positional_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(0)   \n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the number of the last batch sample in the data set is smaller than the defined batch_batch size, mismatch problems will occur. You can modify it yourself, for example, just pass in the shape behind, and then enter it through x.szie(0).\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class SignalEmbedder(nn.Module):\n",
    "    # Necessary to convert the signal into \"word\" vectors for transformer processing.\n",
    "    # Currently a simple group and slice method, but will modify later for multi-channel inputs\n",
    "    \n",
    "    def __init__(self, num_slices, size_of_slice):\n",
    "        super(SignalEmbedder, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.size_of_slice = size_of_slice\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[: self.num_slices * self.size_of_slice]\n",
    "        x = x.reshape((self.num_slices, self.size_of_slice))\n",
    "        return x\n",
    "'''\n",
    "class OneHotConverter(nn.Module):\n",
    "    # Converts the sigmoid output into one-hots\n",
    "    \n",
    "    def __init__(self, size, sentence_length):\n",
    "        super(OneHotConverter, self).__init__()\n",
    "        self.arr_length = size\n",
    "        self.num_words = sentence_length\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for num in x:\n",
    "            num = num.item()\n",
    "            num *= self.arr_length\n",
    "            val = np.zeros(self.arr_length)\n",
    "            val[int(round(num))] = 1\n",
    "        \n",
    "            output.append(val)\n",
    "        output = torch.as_tensor(output)\n",
    "        output.requires_grad_()\n",
    "        return output\n",
    "'''    \n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    # Necessary to store positional data about the input data\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        divisor = torch.exp(torch.arange(0, d_model, 2).float() * (- math.log(10000.0) / d_model))\n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(position * divisor)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * divisor)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoding[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 2500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ECGTransformerEncoder(\n",
       "  (positional_encoder): PositionalEncoder(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (embedder): Sequential(\n",
       "    (initial_norm): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_1): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(2,), groups=8)\n",
       "    (conv_2): Conv1d(8, 16, kernel_size=(5,), stride=(3,), padding=(2,), groups=8)\n",
       "    (activation_2): ELU(alpha=1.0)\n",
       "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_3): Conv1d(16, 32, kernel_size=(5,), stride=(3,), padding=(2,), groups=8)\n",
       "    (activation_3): ELU(alpha=1.0)\n",
       "    (batch_norm_3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_4): Conv1d(32, 48, kernel_size=(5,), stride=(3,), padding=(2,), groups=8)\n",
       "    (activation_4): ELU(alpha=1.0)\n",
       "    (batch_norm_4): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv_5): Conv1d(48, 64, kernel_size=(5,), stride=(3,), padding=(2,), groups=8)\n",
       "    (activation_5): ELU(alpha=1.0)\n",
       "    (batch_norm_5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (reshape): MaxPool1d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Transpose()\n",
       "    (1): Linear(in_features=31, out_features=17, bias=True)\n",
       "    (2): Transpose()\n",
       "    (3): Linear(in_features=64, out_features=30, bias=True)\n",
       "    (4): LogSoftmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model to set to\n",
    "model = ECGTransformerEncoder(vector_size=31, n_inputs=64, n_heads=8, hidden_linear_dim=2048, n_layers=4, dropout=0.3).to(device)\n",
    "\n",
    "# Training params\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "data = []\n",
    "for arr in waveform_lead_rhythm_diag[\"decoded_waveform\"]:\n",
    "    #print(arr)\n",
    "    data.append(arr[0])\n",
    "\n",
    "labels = []\n",
    "for sentence in waveform_lead_rhythm_diag[\"diagnosis\"]:\n",
    "    #label = one_hot(sentence, dict_words)\n",
    "    label = []\n",
    "    for word in sentence.split():\n",
    "        label.append(word_map[word])\n",
    "    \n",
    "    while len(label) < 17:\n",
    "        label.append(0)\n",
    "    labels.append(label)\n",
    "data = torch.from_numpy(np.array(data)).type(torch.FloatTensor)\n",
    "labels = torch.from_numpy(np.array(labels)).type(torch.LongTensor)\n",
    "print(data.shape)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.7437, grad_fn=<AddBackward0>)\n",
      "tensor(17.6244, grad_fn=<AddBackward0>)\n",
      "tensor(17.3278, grad_fn=<AddBackward0>)\n",
      "tensor(17.2335, grad_fn=<AddBackward0>)\n",
      "tensor(17.0108, grad_fn=<AddBackward0>)\n",
      "tensor(16.9378, grad_fn=<AddBackward0>)\n",
      "tensor(16.9251, grad_fn=<AddBackward0>)\n",
      "tensor(16.7457, grad_fn=<AddBackward0>)\n",
      "tensor(16.3992, grad_fn=<AddBackward0>)\n",
      "tensor(16.2745, grad_fn=<AddBackward0>)\n",
      "tensor(16.0325, grad_fn=<AddBackward0>)\n",
      "tensor(15.8914, grad_fn=<AddBackward0>)\n",
      "tensor(15.6513, grad_fn=<AddBackward0>)\n",
      "tensor(15.3564, grad_fn=<AddBackward0>)\n",
      "tensor(15.1028, grad_fn=<AddBackward0>)\n",
      "tensor(15.1565, grad_fn=<AddBackward0>)\n",
      "tensor(14.8512, grad_fn=<AddBackward0>)\n",
      "tensor(14.7445, grad_fn=<AddBackward0>)\n",
      "tensor(14.5130, grad_fn=<AddBackward0>)\n",
      "tensor(14.1693, grad_fn=<AddBackward0>)\n",
      "tensor(13.9283, grad_fn=<AddBackward0>)\n",
      "tensor(13.6691, grad_fn=<AddBackward0>)\n",
      "tensor(13.2860, grad_fn=<AddBackward0>)\n",
      "tensor(13.1719, grad_fn=<AddBackward0>)\n",
      "tensor(12.9785, grad_fn=<AddBackward0>)\n",
      "tensor(12.5579, grad_fn=<AddBackward0>)\n",
      "tensor(12.4086, grad_fn=<AddBackward0>)\n",
      "tensor(11.8899, grad_fn=<AddBackward0>)\n",
      "tensor(11.8181, grad_fn=<AddBackward0>)\n",
      "tensor(11.3436, grad_fn=<AddBackward0>)\n",
      "tensor(11.0903, grad_fn=<AddBackward0>)\n",
      "tensor(10.7955, grad_fn=<AddBackward0>)\n",
      "tensor(10.2665, grad_fn=<AddBackward0>)\n",
      "tensor(10.0842, grad_fn=<AddBackward0>)\n",
      "tensor(9.7433, grad_fn=<AddBackward0>)\n",
      "tensor(9.5405, grad_fn=<AddBackward0>)\n",
      "tensor(9.0923, grad_fn=<AddBackward0>)\n",
      "tensor(8.8996, grad_fn=<AddBackward0>)\n",
      "tensor(8.4435, grad_fn=<AddBackward0>)\n",
      "tensor(8.2380, grad_fn=<AddBackward0>)\n",
      "tensor(8.0809, grad_fn=<AddBackward0>)\n",
      "tensor(7.9268, grad_fn=<AddBackward0>)\n",
      "tensor(7.5118, grad_fn=<AddBackward0>)\n",
      "tensor(7.2700, grad_fn=<AddBackward0>)\n",
      "tensor(6.9308, grad_fn=<AddBackward0>)\n",
      "tensor(6.7759, grad_fn=<AddBackward0>)\n",
      "tensor(6.6683, grad_fn=<AddBackward0>)\n",
      "tensor(6.4472, grad_fn=<AddBackward0>)\n",
      "tensor(6.1093, grad_fn=<AddBackward0>)\n",
      "tensor(5.8859, grad_fn=<AddBackward0>)\n",
      "tensor(5.6710, grad_fn=<AddBackward0>)\n",
      "tensor(5.6541, grad_fn=<AddBackward0>)\n",
      "tensor(5.6148, grad_fn=<AddBackward0>)\n",
      "tensor(5.1147, grad_fn=<AddBackward0>)\n",
      "tensor(4.9612, grad_fn=<AddBackward0>)\n",
      "tensor(5.0569, grad_fn=<AddBackward0>)\n",
      "tensor(4.7781, grad_fn=<AddBackward0>)\n",
      "tensor(4.3908, grad_fn=<AddBackward0>)\n",
      "tensor(4.3530, grad_fn=<AddBackward0>)\n",
      "tensor(4.2765, grad_fn=<AddBackward0>)\n",
      "tensor(4.2431, grad_fn=<AddBackward0>)\n",
      "tensor(3.9578, grad_fn=<AddBackward0>)\n",
      "tensor(3.6812, grad_fn=<AddBackward0>)\n",
      "tensor(3.9332, grad_fn=<AddBackward0>)\n",
      "tensor(3.6134, grad_fn=<AddBackward0>)\n",
      "tensor(3.5062, grad_fn=<AddBackward0>)\n",
      "tensor(3.2626, grad_fn=<AddBackward0>)\n",
      "tensor(3.1704, grad_fn=<AddBackward0>)\n",
      "tensor(2.9335, grad_fn=<AddBackward0>)\n",
      "tensor(3.1234, grad_fn=<AddBackward0>)\n",
      "tensor(2.9748, grad_fn=<AddBackward0>)\n",
      "tensor(2.7491, grad_fn=<AddBackward0>)\n",
      "tensor(2.6833, grad_fn=<AddBackward0>)\n",
      "tensor(2.6969, grad_fn=<AddBackward0>)\n",
      "tensor(2.5239, grad_fn=<AddBackward0>)\n",
      "tensor(2.3738, grad_fn=<AddBackward0>)\n",
      "tensor(2.3257, grad_fn=<AddBackward0>)\n",
      "tensor(2.0283, grad_fn=<AddBackward0>)\n",
      "tensor(1.9608, grad_fn=<AddBackward0>)\n",
      "tensor(2.0943, grad_fn=<AddBackward0>)\n",
      "tensor(1.8676, grad_fn=<AddBackward0>)\n",
      "tensor(1.8753, grad_fn=<AddBackward0>)\n",
      "tensor(1.7958, grad_fn=<AddBackward0>)\n",
      "tensor(1.7188, grad_fn=<AddBackward0>)\n",
      "tensor(1.6708, grad_fn=<AddBackward0>)\n",
      "tensor(1.6140, grad_fn=<AddBackward0>)\n",
      "tensor(1.6295, grad_fn=<AddBackward0>)\n",
      "tensor(1.4450, grad_fn=<AddBackward0>)\n",
      "tensor(1.3569, grad_fn=<AddBackward0>)\n",
      "tensor(1.3275, grad_fn=<AddBackward0>)\n",
      "tensor(1.2898, grad_fn=<AddBackward0>)\n",
      "tensor(1.1245, grad_fn=<AddBackward0>)\n",
      "tensor(1.3317, grad_fn=<AddBackward0>)\n",
      "tensor(1.1318, grad_fn=<AddBackward0>)\n",
      "tensor(1.1745, grad_fn=<AddBackward0>)\n",
      "tensor(1.0045, grad_fn=<AddBackward0>)\n",
      "tensor(0.9913, grad_fn=<AddBackward0>)\n",
      "tensor(0.9478, grad_fn=<AddBackward0>)\n",
      "tensor(0.8898, grad_fn=<AddBackward0>)\n",
      "tensor(0.9830, grad_fn=<AddBackward0>)\n",
      "tensor(0.9514, grad_fn=<AddBackward0>)\n",
      "tensor(0.8825, grad_fn=<AddBackward0>)\n",
      "tensor(0.6986, grad_fn=<AddBackward0>)\n",
      "tensor(0.7266, grad_fn=<AddBackward0>)\n",
      "tensor(0.7058, grad_fn=<AddBackward0>)\n",
      "tensor(0.6514, grad_fn=<AddBackward0>)\n",
      "tensor(0.8006, grad_fn=<AddBackward0>)\n",
      "tensor(0.6107, grad_fn=<AddBackward0>)\n",
      "tensor(0.6285, grad_fn=<AddBackward0>)\n",
      "tensor(0.5831, grad_fn=<AddBackward0>)\n",
      "tensor(0.6661, grad_fn=<AddBackward0>)\n",
      "tensor(0.7104, grad_fn=<AddBackward0>)\n",
      "tensor(0.5680, grad_fn=<AddBackward0>)\n",
      "tensor(0.6463, grad_fn=<AddBackward0>)\n",
      "tensor(0.4566, grad_fn=<AddBackward0>)\n",
      "tensor(0.4987, grad_fn=<AddBackward0>)\n",
      "tensor(0.4978, grad_fn=<AddBackward0>)\n",
      "tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "tensor(0.6004, grad_fn=<AddBackward0>)\n",
      "tensor(0.4748, grad_fn=<AddBackward0>)\n",
      "tensor(0.5200, grad_fn=<AddBackward0>)\n",
      "tensor(0.3617, grad_fn=<AddBackward0>)\n",
      "tensor(0.5563, grad_fn=<AddBackward0>)\n",
      "tensor(0.4898, grad_fn=<AddBackward0>)\n",
      "tensor(0.4643, grad_fn=<AddBackward0>)\n",
      "tensor(0.4307, grad_fn=<AddBackward0>)\n",
      "tensor(0.4738, grad_fn=<AddBackward0>)\n",
      "tensor(0.4469, grad_fn=<AddBackward0>)\n",
      "tensor(0.4451, grad_fn=<AddBackward0>)\n",
      "tensor(0.3359, grad_fn=<AddBackward0>)\n",
      "tensor(0.4395, grad_fn=<AddBackward0>)\n",
      "tensor(0.4014, grad_fn=<AddBackward0>)\n",
      "tensor(0.3018, grad_fn=<AddBackward0>)\n",
      "tensor(0.3539, grad_fn=<AddBackward0>)\n",
      "tensor(0.3706, grad_fn=<AddBackward0>)\n",
      "tensor(0.3111, grad_fn=<AddBackward0>)\n",
      "tensor(0.3242, grad_fn=<AddBackward0>)\n",
      "tensor(0.2439, grad_fn=<AddBackward0>)\n",
      "tensor(0.3000, grad_fn=<AddBackward0>)\n",
      "tensor(0.2493, grad_fn=<AddBackward0>)\n",
      "tensor(0.2989, grad_fn=<AddBackward0>)\n",
      "tensor(0.2776, grad_fn=<AddBackward0>)\n",
      "tensor(0.2514, grad_fn=<AddBackward0>)\n",
      "tensor(0.2807, grad_fn=<AddBackward0>)\n",
      "tensor(0.3566, grad_fn=<AddBackward0>)\n",
      "tensor(0.3177, grad_fn=<AddBackward0>)\n",
      "tensor(0.2645, grad_fn=<AddBackward0>)\n",
      "tensor(0.2426, grad_fn=<AddBackward0>)\n",
      "tensor(0.2066, grad_fn=<AddBackward0>)\n",
      "tensor(0.3464, grad_fn=<AddBackward0>)\n",
      "tensor(0.1880, grad_fn=<AddBackward0>)\n",
      "tensor(0.2646, grad_fn=<AddBackward0>)\n",
      "tensor(0.1968, grad_fn=<AddBackward0>)\n",
      "tensor(0.2250, grad_fn=<AddBackward0>)\n",
      "tensor(0.2414, grad_fn=<AddBackward0>)\n",
      "tensor(0.2003, grad_fn=<AddBackward0>)\n",
      "tensor(0.2051, grad_fn=<AddBackward0>)\n",
      "tensor(0.1723, grad_fn=<AddBackward0>)\n",
      "tensor(0.2677, grad_fn=<AddBackward0>)\n",
      "tensor(0.2519, grad_fn=<AddBackward0>)\n",
      "tensor(0.2282, grad_fn=<AddBackward0>)\n",
      "tensor(0.1793, grad_fn=<AddBackward0>)\n",
      "tensor(0.1918, grad_fn=<AddBackward0>)\n",
      "tensor(0.2039, grad_fn=<AddBackward0>)\n",
      "tensor(0.1571, grad_fn=<AddBackward0>)\n",
      "tensor(0.2203, grad_fn=<AddBackward0>)\n",
      "tensor(0.3362, grad_fn=<AddBackward0>)\n",
      "tensor(0.1592, grad_fn=<AddBackward0>)\n",
      "tensor(0.1542, grad_fn=<AddBackward0>)\n",
      "tensor(0.1464, grad_fn=<AddBackward0>)\n",
      "tensor(0.1479, grad_fn=<AddBackward0>)\n",
      "tensor(0.1887, grad_fn=<AddBackward0>)\n",
      "tensor(0.1587, grad_fn=<AddBackward0>)\n",
      "tensor(0.1284, grad_fn=<AddBackward0>)\n",
      "tensor(0.1931, grad_fn=<AddBackward0>)\n",
      "tensor(0.1548, grad_fn=<AddBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "tensor(0.1817, grad_fn=<AddBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "tensor(0.1076, grad_fn=<AddBackward0>)\n",
      "tensor(0.1371, grad_fn=<AddBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "tensor(0.1968, grad_fn=<AddBackward0>)\n",
      "tensor(0.1153, grad_fn=<AddBackward0>)\n",
      "tensor(0.1267, grad_fn=<AddBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "tensor(0.1163, grad_fn=<AddBackward0>)\n",
      "tensor(0.1544, grad_fn=<AddBackward0>)\n",
      "tensor(0.0826, grad_fn=<AddBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "tensor(0.1615, grad_fn=<AddBackward0>)\n",
      "tensor(0.1225, grad_fn=<AddBackward0>)\n",
      "tensor(0.1349, grad_fn=<AddBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "tensor(0.1277, grad_fn=<AddBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "tensor(0.0809, grad_fn=<AddBackward0>)\n",
      "tensor(0.1228, grad_fn=<AddBackward0>)\n",
      "tensor(0.1474, grad_fn=<AddBackward0>)\n",
      "tensor(0.1148, grad_fn=<AddBackward0>)\n",
      "tensor(0.1181, grad_fn=<AddBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "tensor(0.0688, grad_fn=<AddBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "tensor(0.0744, grad_fn=<AddBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "tensor(0.0691, grad_fn=<AddBackward0>)\n",
      "tensor(0.0762, grad_fn=<AddBackward0>)\n",
      "tensor(0.0805, grad_fn=<AddBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "tensor(0.0633, grad_fn=<AddBackward0>)\n",
      "tensor(0.0751, grad_fn=<AddBackward0>)\n",
      "tensor(0.0618, grad_fn=<AddBackward0>)\n",
      "tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0555, grad_fn=<AddBackward0>)\n",
      "tensor(0.0733, grad_fn=<AddBackward0>)\n",
      "tensor(0.0772, grad_fn=<AddBackward0>)\n",
      "tensor(0.0669, grad_fn=<AddBackward0>)\n",
      "tensor(0.0779, grad_fn=<AddBackward0>)\n",
      "tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(0.1210, grad_fn=<AddBackward0>)\n",
      "tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0681, grad_fn=<AddBackward0>)\n",
      "tensor(0.0457, grad_fn=<AddBackward0>)\n",
      "tensor(0.0730, grad_fn=<AddBackward0>)\n",
      "tensor(0.0713, grad_fn=<AddBackward0>)\n",
      "tensor(0.0690, grad_fn=<AddBackward0>)\n",
      "tensor(0.0558, grad_fn=<AddBackward0>)\n",
      "tensor(0.0538, grad_fn=<AddBackward0>)\n",
      "tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0697, grad_fn=<AddBackward0>)\n",
      "tensor(0.0769, grad_fn=<AddBackward0>)\n",
      "tensor(0.0728, grad_fn=<AddBackward0>)\n",
      "tensor(0.0610, grad_fn=<AddBackward0>)\n",
      "tensor(0.0759, grad_fn=<AddBackward0>)\n",
      "tensor(0.0659, grad_fn=<AddBackward0>)\n",
      "tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "tensor(0.0463, grad_fn=<AddBackward0>)\n",
      "tensor(0.0377, grad_fn=<AddBackward0>)\n",
      "tensor(0.0711, grad_fn=<AddBackward0>)\n",
      "tensor(0.0558, grad_fn=<AddBackward0>)\n",
      "tensor(0.0467, grad_fn=<AddBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "tensor(0.0695, grad_fn=<AddBackward0>)\n",
      "tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "tensor(0.0649, grad_fn=<AddBackward0>)\n",
      "tensor(0.0699, grad_fn=<AddBackward0>)\n",
      "tensor(0.0660, grad_fn=<AddBackward0>)\n",
      "tensor(0.0786, grad_fn=<AddBackward0>)\n",
      "tensor(0.0619, grad_fn=<AddBackward0>)\n",
      "tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0557, grad_fn=<AddBackward0>)\n",
      "tensor(0.0441, grad_fn=<AddBackward0>)\n",
      "tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0475, grad_fn=<AddBackward0>)\n",
      "tensor(0.0398, grad_fn=<AddBackward0>)\n",
      "tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0402, grad_fn=<AddBackward0>)\n",
      "tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(0.0778, grad_fn=<AddBackward0>)\n",
      "tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0417, grad_fn=<AddBackward0>)\n",
      "tensor(0.0404, grad_fn=<AddBackward0>)\n",
      "tensor(0.1176, grad_fn=<AddBackward0>)\n",
      "tensor(0.0601, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    losses = 0\n",
    "    for x, y in zip(train_x, labels):\n",
    "        #print(x.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.unsqueeze(x, 0))\n",
    "        #print(outputs.shape)\n",
    "        loss = loss_function(outputs, y)\n",
    "        losses += loss\n",
    "    losses.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    print(losses)\n",
    "    if losses < .001:\n",
    "        break\n",
    "\n",
    "for x, y in zip(train_x, labels):\n",
    "    print(np.argmax(model(x).detach().numpy(), axis=1))\n",
    "    print(y.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 - FNET/Basic Mixup Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
