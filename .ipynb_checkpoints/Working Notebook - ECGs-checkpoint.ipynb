{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from base64 import b64decode as decode\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.fft as fft\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2PreTrainedModel, GPT2Model\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "from typing import Optional, Tuple\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# The only time we need to define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_id</th>\n",
       "      <th>waveform_type</th>\n",
       "      <th>decoded_waveform</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>548759</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.50390625, 0.5029297, 0.5019531, 0.49902344...</td>\n",
       "      <td>normal sinus rhythm low voltage qrs borderline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.4921875, 0.4921875, 0.4921875, 0.4921875, ...</td>\n",
       "      <td>sinus bradycardia otherwise normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>550602</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.47851562, 0.48046875, 0.48242188, 0.484375...</td>\n",
       "      <td>sinus tachycardia otherwise normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>551485</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.5449219, 0.5439453, 0.54296875, 0.5410156,...</td>\n",
       "      <td>normal sinus rhythm normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>552077</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.49316406, 0.49609375, 0.49902344, 0.494140...</td>\n",
       "      <td>normal sinus rhythm normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>552856</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.46875, 0.46875, 0.46875, 0.46777344, 0.466...</td>\n",
       "      <td>normal sinus rhythm with sinus arrhythmia mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>553115</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.4921875, 0.4951172, 0.49804688, 0.49804688...</td>\n",
       "      <td>atrial fibrillation abnormal ecg normal sinus ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exam_id waveform_type                                   decoded_waveform  \\\n",
       "0   548759        Rhythm  [[0.50390625, 0.5029297, 0.5019531, 0.49902344...   \n",
       "1   549871        Rhythm  [[0.4921875, 0.4921875, 0.4921875, 0.4921875, ...   \n",
       "2   550602        Rhythm  [[0.47851562, 0.48046875, 0.48242188, 0.484375...   \n",
       "3   551485        Rhythm  [[0.5449219, 0.5439453, 0.54296875, 0.5410156,...   \n",
       "4   552077        Rhythm  [[0.49316406, 0.49609375, 0.49902344, 0.494140...   \n",
       "5   552856        Rhythm  [[0.46875, 0.46875, 0.46875, 0.46777344, 0.466...   \n",
       "6   553115        Rhythm  [[0.4921875, 0.4951172, 0.49804688, 0.49804688...   \n",
       "\n",
       "                                           diagnosis  \n",
       "0  normal sinus rhythm low voltage qrs borderline...  \n",
       "1             sinus bradycardia otherwise normal ecg  \n",
       "2             sinus tachycardia otherwise normal ecg  \n",
       "3                     normal sinus rhythm normal ecg  \n",
       "4                     normal sinus rhythm normal ecg  \n",
       "5  normal sinus rhythm with sinus arrhythmia mini...  \n",
       "6  atrial fibrillation abnormal ecg normal sinus ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16).astype(np.float32)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/d_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/d_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/d_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/d_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead dataz\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "\n",
    "\n",
    "# adding the diagnosis and labels\n",
    "waveform_and_diag = pd.merge(waveform_lead[['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']], diagnosis_data[[\"exam_id\", \"Full_text\", \"Original_Diag\"]], left_on= \"exam_id\", right_on=\"exam_id\")\n",
    "\n",
    "\n",
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: np.vstack(x))\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "\n",
    "for value in waveform_lead_rhythm[\"decoded_waveform\"]:\n",
    "    value /= 1024\n",
    "    value += .5\n",
    "\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if row[\"statement_order\"] == 1 and curr_string != \"\":\n",
    "        curr_string = curr_string.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        val = [curr_id, curr_string[1:]]\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    curr_string += \" \" + row[\"Full_text\"]\n",
    "\n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "waveform_lead_rhythm_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ecg_data, the dataset we will be using for training purposes\n",
    "ecg_data = torch.tensor(list(waveform_lead_rhythm_diag['decoded_waveform'])).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we define components to be used for both the Conv1D encoder, and a Conv1D pre-embedder into a Transformer Encoder.\n",
    "\n",
    "LR = 1e-3\n",
    "KER_SIZE = 11\n",
    "PADDING = 5\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3016, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2688, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1736, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1416, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0946, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0625, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0360, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9886, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9671, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9323, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9133, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8988, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8857, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8753, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8668, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8571, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8494, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8425, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8355, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8292, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8223, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8157, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8091, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8028, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7972, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7916, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7860, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7811, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7762, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7710, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7659, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7611, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7569, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7524, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7435, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7392, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7353, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7312, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7268, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7230, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7194, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7158, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7120, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7083, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7045, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7013, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6980, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6947, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6915, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6881, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6845, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6814, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6786, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6763, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6728, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6694, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6667, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6638, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6608, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6576, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6548, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6521, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6488, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6458, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6428, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6369, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6341, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6310, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6275, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6244, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6208, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6134, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6098, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6070, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6035, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5958, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5914, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5851, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5820, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5780, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5737, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5709, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5683, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5651, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5616, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5590, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5560, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5532, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5504, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5454, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5429, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5405, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5379, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5353, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5329, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5305, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5282, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5261, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5233, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5210, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5188, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5166, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5145, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5122, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5102, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5085, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5072, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5061, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5044, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5000, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4979, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4963, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4943, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4928, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4889, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4861, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4844, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4818, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4800, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4779, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4757, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4740, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4718, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4700, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4681, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4660, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4641, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4624, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4603, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4586, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4567, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4549, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4536, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4529, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4535, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4553, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4550, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4533, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4517, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4486, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4447, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4432, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4399, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4395, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4361, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4350, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4324, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4310, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4287, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4273, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4251, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4235, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4216, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4199, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4182, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4165, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4149, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4131, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4114, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4099, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4082, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4066, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4051, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4034, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3988, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3973, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3960, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3949, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3942, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3925, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3909, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 1: ResNet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# build resent model and display the shape of feed through\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(5):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = 249, padding = 124, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = 249, padding = 124))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "    \n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = 249, padding = 124))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 250\n",
    "embedding_dim = 8\n",
    "num_words = len(dict_words)\n",
    "\n",
    "class LSTM_EncoderDecoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim, word_list_length):\n",
    "        super(ECG_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim, num_layers = 4, bidirectional = True)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq_embedded = seq.view(len(seq), -1, embedding_dim)\n",
    "        final_hidd, _ = self.lstm(seq_embedded)\n",
    "        dec_seq = self.linear(final_hidd)\n",
    "        return F.log_softmax(dec_seq, dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 3 - Multi-Head Attention Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress, will clean later\n",
    "\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ECGTransformerEncoder(nn.Module):\n",
    "    # Takes the ECG discrete signals sequence and maps into a probability distribution of diagnosis\n",
    "    # For working/verification purposes\n",
    "    def __init__(self, vector_size, embed_dim, n_heads, hidden_linear_dim, n_layers, dropout):\n",
    "        super(ECGTransformerEncoder, self).__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.positional_encoder = PositionalEncoder(embed_dim, dropout)\n",
    "    \n",
    "        #Since our data is already discrete numbers, might need some tweaking for this\n",
    "        self.embedder = conv_embedder\n",
    "                        #64 31              #39        64\n",
    "        \n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(embed_dim, n_heads, hidden_linear_dim, dropout),\n",
    "            n_layers)\n",
    "        \n",
    "        self.n_inputs = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Simple linear decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(768, 17),\n",
    "                        Transpose(17, 2500),\n",
    "                        nn.Linear(2500, 30),\n",
    "                        nn.LogSoftmax()\n",
    "                        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #self.embedder.weight.data.uniform_(-.1, .1)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        #self.decoder.weight.data.uniform_(-.1, .1)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.embedder(x) # * math.sqrt(self.n_inputs)\n",
    "        x = x.squeeze(0)\n",
    "        #x = x.view(2500, 8)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1) \n",
    "        #x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(features, features * expansion)\n",
    "        self.linear_2 = nn.Linear(features * expansion, features)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        #self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.norm_1(x + res)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FNETLayer(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FNETLayer, self).__init__()\n",
    "        self.feed_forward = FeedForwardNet(features, expansion, dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = fft.fftn(x, dim = (-2, -1)).real\n",
    "        x = self.norm_1(x + res)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "    \n",
    "class FNETEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self, features, expansion=2, dropout=0.5, num_layers=6):\n",
    "        encoder_layer = FNETLayer(features, expansion, dropout)\n",
    "        super().__init__(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Encoder Helper Functions/Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    # Necessary to store positional data about the input data\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=2500, batch_size = 1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, 1, embed_dim)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        divisor = torch.exp(torch.arange(0, embed_dim, 2).float() * (- math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pos_encoding[:, 0, 0::2] = torch.sin(position * divisor)\n",
    "        pos_encoding[:, 0, 1::2] = torch.cos(position * divisor)\n",
    "        pos_encoding = pos_encoding.repeat(1, batch_size, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoding[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "   \n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the number of the last batch sample in the data set is smaller than the defined batch_batch size, mismatch problems will occur. You can modify it yourself, for example, just pass in the shape behind, and then enter it through x.szie(0).\n",
    "        return x.view(self.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class WindowEmbedder(nn.Module):\n",
    "    # Necessary to convert the signal into \"word\" vectors for transformer processing.\n",
    "    # Currently a simple group and slice method, but will modify later for multi-channel inputs\n",
    "    \n",
    "    def __init__(self, num_slices, size_of_slice):\n",
    "        super(SignalEmbedder, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.size_of_slice = size_of_slice\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[: self.num_slices * self.size_of_slice]\n",
    "        x = x.reshape((self.num_slices, self.size_of_slice))\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 1 - Huggingface GPT2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with child class of GPT2LMHeadModel\n",
    "\n",
    "class GPT2LMHeadModel(GPT2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"attn.masked_bias\", r\"attn.bias\", r\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        #self.transformer.forward = forward2.__get__(self.transformer, GPT2Model)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "    def parallelize(self, device_map=None):\n",
    "        self.device_map = (\n",
    "            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n",
    "            if device_map is None\n",
    "            else device_map\n",
    "        )\n",
    "        assert_device_map(self.device_map, len(self.transformer.h))\n",
    "        self.transformer.parallelize(self.device_map)\n",
    "        self.lm_head = self.lm_head.to(self.transformer.first_device)\n",
    "        self.model_parallel = True\n",
    "        \n",
    "    def deparallelize(self):\n",
    "        self.transformer.deparallelize()\n",
    "        self.transformer = self.transformer.to(\"cpu\")\n",
    "        self.lm_head = self.lm_head.to(\"cpu\")\n",
    "        self.model_parallel = False\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            position_ids = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"encoder_hidden_states\": kwargs.get(\"encoder_hidden_states\", None), # The one line changed hehe\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.transformer.first_device)\n",
    "            hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reorder_cache(past: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the :obj:`past_key_values` cache if\n",
    "        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# preprocess training labels and tokenize\n",
    "train_labels = list(waveform_lead_rhythm_diag['diagnosis'])\n",
    "inputs = tokenizer(train_labels, padding = True, verbose = False, return_tensors=\"pt\")\n",
    "\n",
    "#Necessary to add for generating first word\n",
    "inputs[\"input_ids\"] = torch.cat((torch.tensor([[50256] for i in range(len(inputs[\"input_ids\"]))]), inputs[\"input_ids\"]), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((torch.tensor([[1] for i in range(len(inputs[\"attention_mask\"]))]), inputs[\"attention_mask\"]), dim=1)\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder - FNET Encoder Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder decoder model with GPT2 \n",
    "class CustEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder):\n",
    "        super(CustEncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pos_enb = PositionalEncoder(embed_dim = 768)\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ecgs, labels = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(**labels, labels = labels[\"input_ids\"], encoder_hidden_states = x.contiguous())\n",
    "        return out\n",
    "    \n",
    "    # Should only take 1 input at a time?\n",
    "    def predict(self, x):\n",
    "        ecgs = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        output = []\n",
    "        for ecg in x:\n",
    "            output.append(tokenizer.decode(self.decoder.generate(encoder_hidden_states = ecg.unsqueeze(0).contiguous())[0]))\n",
    "        return output\n",
    "    \n",
    "    def return_enc(self):\n",
    "        return self.encoder\n",
    "\n",
    "    \n",
    "# Connect an embedder and de-embedder for training (we will then isolate the Encoder portion of this autoencoder as our embedder)\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def make_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def make_decoder(self):\n",
    "        return self.decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is for the pre-embedder\n",
    "    \n",
    "# Make embedder\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(2):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 768, kernel_size = KER_SIZE, padding = PADDING))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(768))\n",
    "\n",
    "\n",
    "# Make de-embedder\n",
    "deconv_model = nn.Sequential()\n",
    "init_channels = 768\n",
    "for i in range(2):\n",
    "    next_channels = init_channels // 2\n",
    "    deconv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    deconv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    deconv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    deconv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    deconv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "deconv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = KER_SIZE, padding = PADDING))\n",
    "deconv_model.add_module('act_fin', nn.ReLU())\n",
    "deconv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "\n",
    "auto_model = ConvAutoEncoder(conv_model, deconv_model).to(device)\n",
    "auto_optimizer = torch.optim.Adam(auto_model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "# Training params\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    auto_optimizer.zero_grad()\n",
    "    outputs = auto_model(ecg_data)\n",
    "    loss = loss_function(outputs, ecg_data)\n",
    "    loss.backward(retain_graph=True)\n",
    "    auto_optimizer.step()\n",
    "    print(loss)\n",
    "        \n",
    "# Saving/loading weights\n",
    "torch.save(auto_model.state_dict(), 'model/autoencoder.pt')\n",
    "auto_model.load_state_dict(torch.load('model/autoencoder.pt'))\n",
    "conv_embedder = auto_model.make_encoder()\n",
    "torch.save(conv_embedder.state_dict(), \"model/embedder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define encoder, we don't need to pretrain rn\n",
    "encoder = FNETEncoder(768, expansion = 2, dropout=0.1, num_layers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.1.crossattention.masked_bias', 'h.2.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.3.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.4.crossattention.masked_bias', 'h.10.crossattention.q_attn.weight', 'h.0.ln_cross_attn.weight', 'h.3.ln_cross_attn.weight', 'h.9.crossattention.bias', 'h.1.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.7.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.6.crossattention.bias', 'h.10.crossattention.masked_bias', 'h.11.crossattention.masked_bias', 'h.5.crossattention.c_proj.bias', 'h.0.crossattention.masked_bias', 'h.0.crossattention.bias', 'h.4.crossattention.bias', 'h.9.crossattention.masked_bias', 'h.8.crossattention.q_attn.weight', 'h.9.ln_cross_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.8.crossattention.bias', 'h.9.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.7.crossattention.bias', 'h.4.ln_cross_attn.weight', 'h.8.ln_cross_attn.weight', 'h.2.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.1.ln_cross_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.8.crossattention.masked_bias', 'h.10.crossattention.bias', 'h.11.crossattention.c_proj.weight', 'h.10.ln_cross_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.2.crossattention.masked_bias', 'h.5.crossattention.bias', 'h.7.crossattention.c_proj.bias', 'h.3.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.6.ln_cross_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.6.crossattention.masked_bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.bias', 'h.9.crossattention.c_proj.weight', 'h.11.crossattention.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.weight', 'h.3.crossattention.bias', 'h.4.crossattention.q_attn.weight', 'h.7.crossattention.masked_bias', 'h.11.ln_cross_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.2856, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# define and pretrain Decoder\n",
    "decoder = GPT2LMHeadModel.from_pretrained('gpt2', config = GPT2Config(add_cross_attention = True, is_encoder_decoder = True))\n",
    "\n",
    "# pretrain decoder\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = decoder(**inputs, labels = inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "torch.save(decoder.state_dict(), 'model/gpt2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'GPT2LMHeadModel' object has no attribute 'get_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-3625007d00be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0menc_dec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustEncoderDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_embedder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0menc_dec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-c0c7b05481db>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_enb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreturn_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;31m# add encoder_outputs to model_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_encoder_decoder_kwargs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;31m# set input_ids as decoder_input_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, input_ids, model_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;31m# retrieve encoder hidden states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             encoder_kwargs = {\n\u001b[1;32m    413\u001b[0m                 \u001b[0margument\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'get_encoder'"
     ]
    }
   ],
   "source": [
    "# define component models\n",
    "\n",
    "conv_embedder = auto_model.make_encoder()\n",
    "\n",
    "encoder = FNETEncoder(768, expansion = 2, dropout=0.1, num_layers = 6)\n",
    "\n",
    "#decoder.load_state_dict(torch.load('model/gpt2.pt'))\n",
    "\n",
    "enc_dec_model = CustEncoderDecoder(encoder, decoder, conv_embedder)\n",
    "\n",
    "enc_dec_model.predict(ecg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbang/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:162.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7518, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0280, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# train encoder decoder model!\n",
    "optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-5)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 2\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = enc_dec_model((ecg_data, inputs))\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "torch.save(enc_dec_model.state_dict(), 'model/gpt2_enc_dec.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_model.load_state_dict(torch.load('model/gpt2_enc_dec.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/gpt2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
