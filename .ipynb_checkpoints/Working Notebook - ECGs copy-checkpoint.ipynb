{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from base64 import b64decode as decode\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_data_id</th>\n",
       "      <th>waveform_id</th>\n",
       "      <th>WavfmType</th>\n",
       "      <th>lead_id</th>\n",
       "      <th>lead_byte_count_total</th>\n",
       "      <th>lead_time_offset</th>\n",
       "      <th>waveform_data</th>\n",
       "      <th>lead_sample_count_total</th>\n",
       "      <th>lead_amplitude</th>\n",
       "      <th>lead_units</th>\n",
       "      <th>...</th>\n",
       "      <th>exam_id</th>\n",
       "      <th>waveform_type</th>\n",
       "      <th>number_of_leads</th>\n",
       "      <th>Waveform_Start_Time</th>\n",
       "      <th>Sample_Type</th>\n",
       "      <th>Sample_Base</th>\n",
       "      <th>Sample_Exponent</th>\n",
       "      <th>High_Pass_Filter</th>\n",
       "      <th>Low_Pass_Filter</th>\n",
       "      <th>AC_Filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9078054</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>+P/4//j/+P/4//j/+P/5//r/+//8//z//P/7//r/+f/4/...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>9081703</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>II</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>9v/2//b/8//w//D/8P/x//L/8//0//T/9P/z//L/8f/w/...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9074278</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>/v/+//7//v/+////AAAAAAAAAQACAAIAAgACAAIAAgACA...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9066887</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V2</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>9v/1//T/9P/0//T/9P/0//T/9f/2//b/9v/2//b/9v/2/...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>9082771</td>\n",
       "      <td>1095618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V3</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>7v/u/+7/7f/s/+z/7P/t/+7/7v/u/+7/7v/u/+7/7v/u/...</td>\n",
       "      <td>2500</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>9187141</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V4</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>KAApACoAKwAsACwALQAtAC4ALgAuAC4ALgAuAC4ALgAvA...</td>\n",
       "      <td>600</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>9190675</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V5</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>FgAXABkAGQAbABsAGwAbABsAGwAbABwAHQAeAB4AHgAfA...</td>\n",
       "      <td>600</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>9177603</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V5</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>+v/6//r/+v/7//z//f/+//z//P/8//z//v/+//7//v/+/...</td>\n",
       "      <td>5000</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>9172851</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>7v/u/+7/7v/x//L/8//0//T/9P/0//T/9P/0//T/9P/0/...</td>\n",
       "      <td>5000</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>9173608</td>\n",
       "      <td>1109067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V6</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>CgAKAAoACwAMAAwADAAMAAwADQAOAA8AEAASABIAEgASA...</td>\n",
       "      <td>600</td>\n",
       "      <td>4.88</td>\n",
       "      <td>MICROVOLTS</td>\n",
       "      <td>...</td>\n",
       "      <td>554080</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>CONTINUOUS_SAMPLES</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lead_data_id  waveform_id  WavfmType lead_id  lead_byte_count_total  \\\n",
       "10        9078054      1095618        NaN       I                   5000   \n",
       "15        9081703      1095618        NaN      II                   5000   \n",
       "8         9074278      1095618        NaN      V1                   5000   \n",
       "1         9066887      1095618        NaN      V2                   5000   \n",
       "18        9082771      1095618        NaN      V3                   5000   \n",
       "..            ...          ...        ...     ...                    ...   \n",
       "150       9187141      1109067        NaN      V4                   1200   \n",
       "152       9190675      1109067        NaN      V5                   1200   \n",
       "155       9177603      1109067        NaN      V5                  10000   \n",
       "140       9172851      1109067        NaN      V6                  10000   \n",
       "141       9173608      1109067        NaN      V6                   1200   \n",
       "\n",
       "     lead_time_offset                                      waveform_data  \\\n",
       "10                  0   +P/4//j/+P/4//j/+P/5//r/+//8//z//P/7//r/+f/4/...   \n",
       "15                  0   9v/2//b/8//w//D/8P/x//L/8//0//T/9P/z//L/8f/w/...   \n",
       "8                   0   /v/+//7//v/+////AAAAAAAAAQACAAIAAgACAAIAAgACA...   \n",
       "1                   0   9v/1//T/9P/0//T/9P/0//T/9f/2//b/9v/2//b/9v/2/...   \n",
       "18                  0   7v/u/+7/7f/s/+z/7P/t/+7/7v/u/+7/7v/u/+7/7v/u/...   \n",
       "..                ...                                                ...   \n",
       "150                 0   KAApACoAKwAsACwALQAtAC4ALgAuAC4ALgAuAC4ALgAvA...   \n",
       "152                 0   FgAXABkAGQAbABsAGwAbABsAGwAbABwAHQAeAB4AHgAfA...   \n",
       "155                 0   +v/6//r/+v/7//z//f/+//z//P/8//z//v/+//7//v/+/...   \n",
       "140                 0   7v/u/+7/7v/x//L/8//0//T/9P/0//T/9P/0//T/9P/0/...   \n",
       "141                 0   CgAKAAoACwAMAAwADAAMAAwADQAOAA8AEAASABIAEgASA...   \n",
       "\n",
       "     lead_sample_count_total  lead_amplitude  lead_units  ...  exam_id  \\\n",
       "10                      2500            4.88  MICROVOLTS  ...   549871   \n",
       "15                      2500            4.88  MICROVOLTS  ...   549871   \n",
       "8                       2500            4.88  MICROVOLTS  ...   549871   \n",
       "1                       2500            4.88  MICROVOLTS  ...   549871   \n",
       "18                      2500            4.88  MICROVOLTS  ...   549871   \n",
       "..                       ...             ...         ...  ...      ...   \n",
       "150                      600            4.88  MICROVOLTS  ...   554080   \n",
       "152                      600            4.88  MICROVOLTS  ...   554080   \n",
       "155                     5000            4.88  MICROVOLTS  ...   554080   \n",
       "140                     5000            4.88  MICROVOLTS  ...   554080   \n",
       "141                      600            4.88  MICROVOLTS  ...   554080   \n",
       "\n",
       "     waveform_type  number_of_leads  Waveform_Start_Time         Sample_Type  \\\n",
       "10          Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "15          Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "8           Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "1           Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "18          Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "..             ...              ...                  ...                 ...   \n",
       "150         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "152         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "155         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "140         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "141         Rhythm                8                    0  CONTINUOUS_SAMPLES   \n",
       "\n",
       "     Sample_Base  Sample_Exponent  High_Pass_Filter Low_Pass_Filter  AC_Filter  \n",
       "10           250                0                 5             150       NONE  \n",
       "15           250                0                 5             150       NONE  \n",
       "8            250                0                 5             150       NONE  \n",
       "1            250                0                 5             150       NONE  \n",
       "18           250                0                 5             150       NONE  \n",
       "..           ...              ...               ...             ...        ...  \n",
       "150          500                0                16             150       NONE  \n",
       "152          500                0                16             150       NONE  \n",
       "155          500                0                16             150       NONE  \n",
       "140          500                0                16             150       NONE  \n",
       "141          500                0                16             150       NONE  \n",
       "\n",
       "[160 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16).astype(np.float32)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/d_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/d_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/d_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/d_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead data\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "waveform_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63183594\n",
      "0.3154297\n",
      "0.75097656\n",
      "0.31347656\n",
      "0.7832031\n",
      "0.08496094\n",
      "0.8984375\n",
      "0.29296875\n",
      "0.78222656\n",
      "0.31152344\n",
      "0.8417969\n",
      "0.19335938\n",
      "0.6904297\n",
      "0.33691406\n",
      "0.6972656\n",
      "0.40234375\n"
     ]
    }
   ],
   "source": [
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "waveform_lead_concat\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: np.vstack(x))#.apply(lambda x: np.transpose(x))\n",
    "\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "#waveform_lead_median = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Median\"]\n",
    "\n",
    "for value in waveform_lead_rhythm[\"decoded_waveform\"]:\n",
    "    value /= 1024\n",
    "    value += .5\n",
    "    print(np.max(value))\n",
    "    print(np.min(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal sinus rhythm low voltage qrs borderline ecg\n",
      "sinus bradycardia otherwise normal ecg\n",
      "sinus tachycardia otherwise normal ecg\n",
      "normal sinus rhythm normal ecg\n",
      "normal sinus rhythm normal ecg\n",
      "normal sinus rhythm with sinus arrhythmia minimal voltage criteria for lvh may be normal variant borderline ecg\n",
      "atrial fibrillation abnormal ecg normal sinus rhythm with sinus arrhythmia normal ecg\n"
     ]
    }
   ],
   "source": [
    "# Adding the labels/sentences\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "# Let's look over this tomorrow\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "#\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if row[\"statement_order\"] == 1 and curr_string != \"\":\n",
    "        curr_string = curr_string.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        val = [curr_id, curr_string[1:]]\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    curr_string += \" \" + row[\"Full_text\"]\n",
    "\n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "#waveform_lead_rhythm_diag\n",
    "for i in waveform_lead_rhythm_diag[\"diagnosis\"]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'variant', 'arrhythmia', 'qrs', 'rhythm', 'ecg', 't', 'low', 'abnormality', 'sinus', 'for', 'tachycardia', 'be', 'inferior', 'otherwise', 'may', 'minimal', 'with', 'wave', 'fibrillation', 'bradycardia', 'ischemia', 'lvh', 'normal', 'voltage', 'atrial', 'borderline', 'consider', 'abnormal', 'criteria'}\n",
      "{'variant': 1, 'arrhythmia': 2, 'qrs': 3, 'rhythm': 4, 'ecg': 5, 't': 6, 'low': 7, 'abnormality': 8, 'sinus': 9, 'for': 10, 'tachycardia': 11, 'be': 12, 'inferior': 13, 'otherwise': 14, 'may': 15, 'minimal': 16, 'with': 17, 'wave': 18, 'fibrillation': 19, 'bradycardia': 20, 'ischemia': 21, 'lvh': 22, 'normal': 23, 'voltage': 24, 'atrial': 25, 'borderline': 26, 'consider': 27, 'abnormal': 28, 'criteria': 29, '': 0}\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for num, sentence in diagnoses:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word)\n",
    "print(unique_words)\n",
    "unique_words = list(unique_words)\n",
    "word_map = dict()\n",
    "for i, word in enumerate(unique_words):\n",
    "    word_map[word] = i+1\n",
    "word_map[\"\"] = 0\n",
    "print(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 8, 2500])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into training and testing datasets\n",
    "# y not included for now\n",
    "def one_hot(x, dict_words):\n",
    "    x = x.split(\" \")\n",
    "    array = []\n",
    "    for i in x:\n",
    "        array.append([0] + [1 if y == i else 0 for y in dict_words] + [0,0])\n",
    "    for i in range(17-len(x)):\n",
    "        array.append([1 if i == 30 else 0 for i in range(32)])\n",
    "    return array\n",
    "\n",
    "dict_words = list(unique_words)\n",
    "#waveform_lead_rhythm_diag['diagnosis'] = waveform_lead_rhythm_diag['diagnosis'].apply(lambda x: one_hot(x, dict_words))\n",
    "\n",
    "len(waveform_lead_rhythm_diag[\"diagnosis\"][5])\n",
    "train_x, test_x, train_y, test_y = train_test_split(waveform_lead_rhythm_diag['decoded_waveform'], waveform_lead_rhythm_diag['diagnosis'], test_size = 0.1, random_state = 2021)\n",
    "train_x = torch.tensor(list(train_x)).float()\n",
    "train_x.shape\n",
    "train_x = torch.tensor(list(waveform_lead_rhythm_diag['decoded_waveform'])).float()\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Conv1D Encoder w/ LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2500])\n",
      "torch.Size([1, 64, 31])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 8 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# 1D grouped encoder model\n",
    "encoder_conv = nn.Sequential()\n",
    "encoder_conv.add_module('initial_norm', nn.BatchNorm1d(8))\n",
    "encoder_conv.add_module('conv_1', nn.Conv1d(in_channels = 8, out_channels = 8, groups = 8, kernel_size = 5, padding = 2))\n",
    "for i in range(2, (J+2), 2):\n",
    "    if (i-2) == 0: \n",
    "        prev = 8\n",
    "    else:\n",
    "        prev = (i-2)*8\n",
    "    encoder_conv.add_module('conv_{num}'.format(num = int(i / 2 + 1)), nn.Conv1d(in_channels = prev, out_channels = i*8, groups = 8, kernel_size = 5, padding = 2, stride = 3))\n",
    "    encoder_conv.add_module('activation_{num}'.format(num = int(i / 2 + 1)), nn.ELU())\n",
    "    encoder_conv.add_module('batch_norm_{num}'.format(num = int(i / 2 + 1)), nn.BatchNorm1d(i*8))\n",
    "    \n",
    "#encoder_conv.add_module('final_conv', nn.Conv1d(in_channels = J * 8, out_channels = 8, groups = 8, kernel_size = 5, padding = 2))\n",
    "#encoder_conv.add_module('max_pool', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "encoder_conv.add_module('reshape', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "\n",
    "\n",
    "# summarize model, verify output is of desired shape\n",
    "print(train_x[0].shape)\n",
    "print(encoder_conv(torch.unsqueeze(train_x[0], 0)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv_0): Conv1d(8, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_0): ReLU()\n",
      "  (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_0): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_0): ReLU()\n",
      "  (conv_1): Conv1d(16, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_1): ReLU()\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_1): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_1): ReLU()\n",
      "  (conv_2): Conv1d(32, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_2): ReLU()\n",
      "  (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_2): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_2): ReLU()\n",
      "  (conv_3): Conv1d(64, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_3): ReLU()\n",
      "  (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_3): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_3): ReLU()\n",
      "  (conv_4): Conv1d(128, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_4): ReLU()\n",
      "  (batch_norm_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_4): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_4): ReLU()\n",
      "  (conv_fin): Conv1d(256, 768, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_fin): ReLU()\n",
      "  (batch_fin): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ResConv\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "KER_SIZE = 11\n",
    "PADDING = 5\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(5):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 768, kernel_size = KER_SIZE, padding = PADDING))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(768))\n",
    "print(conv_model)\n",
    "#print(conv_model(train_x).shape)\n",
    "conv_embedder = conv_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 8, 2500])\n",
      "torch.Size([7, 2500, 8])\n",
      "torch.Size([7, 768, 2500])\n",
      "torch.Size([7, 8, 2500])\n"
     ]
    }
   ],
   "source": [
    "deconv_model = nn.Sequential()\n",
    "init_channels = 768\n",
    "for i in range(5):\n",
    "    next_channels = init_channels // 2\n",
    "    deconv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    deconv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    deconv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    deconv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    deconv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "deconv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = KER_SIZE, padding = PADDING))\n",
    "deconv_model.add_module('act_fin', nn.ReLU())\n",
    "deconv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "\n",
    "print(train_x.shape)\n",
    "print(data.shape)\n",
    "print(conv_model(train_x).shape)\n",
    "print(deconv_model(conv_model(train_x)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def make_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def make_decoder(self):\n",
    "        return self.decoder\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model to set to\n",
    "auto_model = ConvAutoEncoder(conv_model, deconv_model)\n",
    "auto_optimizer = torch.optim.Adam(auto_model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Training params\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#print(train_x[0])\n",
    "for i in range(180):\n",
    "    #print(train_x.shape)\n",
    "    auto_optimizer.zero_grad()\n",
    "    outputs = auto_model(train_x)\n",
    "    #print(outputs.shape)\n",
    "    losses = loss_function(outputs, train_x)\n",
    "    losses.backward(retain_graph=True)\n",
    "    auto_optimizer.step()\n",
    "    print(losses)\n",
    "    if losses < .001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(auto_model.state_dict(), 'model/autoencoder.pt')\n",
    "\n",
    "conv_embedder = auto_model.make_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - LSTM Encoder w/ Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2f894452ad6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mhidden_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnum_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mECG_LSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_words' is not defined"
     ]
    }
   ],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 512\n",
    "embedding_dim = 8\n",
    "num_words = len(unique_words)\n",
    "\n",
    "class ECG_LSTM(nn.Module):\n",
    "    def __init__(self, encoder, h_dim, e_dim, word_list_length):\n",
    "        super(ECG_LSTM, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim)\n",
    "        self.linear = nn.Linear(h_dim, word_list_length)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq_embedded = self.encoder(seq)\n",
    "        final_hidd, _ = self.lstm(seq_embedded)\n",
    "        dec_seq = self.linear(final_hidd)\n",
    "        return F.log_softmax(dec_seq)\n",
    "    \n",
    "lstm_dec = ECG_LSTM(encoder_conv, hidden_layers, embedding_dim, num_words)\n",
    "lstm_dec(train_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Basic Transformer Architecture with Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 2500, 8])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "new_data = conv_embedder(train_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0995, -0.5898, -0.4277,  ...,  3.1599, -0.5898,  7.7721],\n",
      "         [-0.5547,  0.6758, -0.5547,  ..., -0.5547, -0.5547,  3.1045],\n",
      "         [-0.5680, -0.5680,  2.9091,  ...,  0.8775, -0.5680,  1.5757],\n",
      "         ...,\n",
      "         [-0.5168,  1.1706,  0.0231,  ..., -0.5168, -0.5168,  3.2656],\n",
      "         [-0.4377,  2.8713,  0.2086,  ..., -0.4377,  2.6868,  0.6346],\n",
      "         [ 0.8298, -0.3843, -0.3843,  ..., -0.3843, -0.3843, -0.3843]],\n",
      "\n",
      "        [[-0.5898, -0.5898, -0.5898,  ..., 11.0307, -0.5898,  6.4339],\n",
      "         [-0.5547,  1.8430, -0.5547,  ..., -0.5547, -0.5547,  0.7982],\n",
      "         [-0.5680, -0.4991,  1.5885,  ..., -0.5680,  3.4990, -0.5680],\n",
      "         ...,\n",
      "         [-0.5168, -0.5168, -0.5168,  ..., 12.3154, -0.0480, -0.5168],\n",
      "         [-0.4377, -0.4377, -0.4377,  ..., -0.4377,  4.6131, -0.4377],\n",
      "         [ 1.8815, -0.3843, -0.3843,  ..., -0.3843, -0.3843, -0.3843]],\n",
      "\n",
      "        [[-0.5767, -0.5898, -0.5898,  ..., -0.5898, -0.5898,  2.2787],\n",
      "         [ 1.3027, -0.5547, -0.0715,  ..., -0.5547, -0.5547,  1.6808],\n",
      "         [-0.5680, -0.5680,  5.7637,  ..., -0.5680,  1.8905,  2.2515],\n",
      "         ...,\n",
      "         [ 0.3355, -0.5168,  1.6160,  ..., -0.5168, -0.5168, -0.3128],\n",
      "         [-0.4377,  0.3116, -0.4377,  ..., -0.4377,  8.5061, -0.4377],\n",
      "         [-0.3843, -0.3843, -0.3843,  ..., -0.3843, -0.3843, -0.3843]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.6090, -0.5898, -0.5898,  ...,  4.6626, -0.5898,  8.7354],\n",
      "         [-0.5547,  0.1743, -0.5547,  ..., -0.5547, -0.5547,  0.9166],\n",
      "         [-0.5680, -0.5680, -0.5680,  ...,  0.6356, -0.5680, -0.1458],\n",
      "         ...,\n",
      "         [ 1.3961,  0.1610, -0.5168,  ..., -0.5168, -0.4644,  4.8531],\n",
      "         [-0.4377, -0.4370,  2.3456,  ..., -0.4377,  3.8776,  2.4654],\n",
      "         [ 1.9787, -0.3843, -0.3843,  ..., -0.3843, -0.3843, -0.3843]],\n",
      "\n",
      "        [[ 2.8148, -0.5898, -0.5898,  ...,  5.8397, -0.5898,  8.4426],\n",
      "         [-0.5547,  1.8083, -0.5547,  ..., -0.5547, -0.5547,  3.1898],\n",
      "         [-0.5680, -0.5680,  1.4758,  ...,  0.4967, -0.5680,  1.4144],\n",
      "         ...,\n",
      "         [-0.2653,  2.1502, -0.5168,  ..., -0.5168,  3.2571,  1.5762],\n",
      "         [-0.4377, -0.4377, -0.4377,  ..., -0.4377,  3.4432,  4.0219],\n",
      "         [ 4.6946, -0.3843, -0.3843,  ..., -0.3843, -0.3843, -0.3837]],\n",
      "\n",
      "        [[-0.5898, -0.5898, -0.5898,  ...,  4.3355, -0.5898,  7.8368],\n",
      "         [-0.5547,  0.9471, -0.5547,  ..., -0.5547, -0.5547,  3.8864],\n",
      "         [-0.5680,  0.4552,  2.3579,  ..., -0.0696, -0.5680,  3.0510],\n",
      "         ...,\n",
      "         [-0.5168, -0.5168, -0.1033,  ..., -0.5168, -0.5168, -0.1218],\n",
      "         [-0.4377,  0.2357,  0.3837,  ..., -0.4377, -0.4377, -0.4377],\n",
      "         [ 1.7133, -0.3843, -0.3843,  ..., -0.3843, -0.3843, -0.3843]]])\n"
     ]
    }
   ],
   "source": [
    "new_data = new_data.detach()\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ECGTransformerEncoder(nn.Module):\n",
    "    # Takes the ECG discrete signals sequence and maps into a probability distribution of diagnosis\n",
    "    # For working/verification purposes\n",
    "    def __init__(self, vector_size, embed_dim, n_heads, hidden_linear_dim, n_layers, dropout):\n",
    "        super(ECGTransformerEncoder, self).__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.positional_encoder = PositionalEncoder(embed_dim, dropout)\n",
    "    \n",
    "        #Since our data is already discrete numbers, might need some tweaking for this\n",
    "        self.embedder = conv_embedder\n",
    "                        #64 31              #39        64\n",
    "        \n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(embed_dim, n_heads, hidden_linear_dim, dropout),\n",
    "            n_layers)\n",
    "        \n",
    "        self.n_inputs = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Simple linear decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(768, 17),\n",
    "                        Transpose(17, 2500),\n",
    "                        nn.Linear(2500, 30),\n",
    "                        nn.LogSoftmax()\n",
    "                        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #self.embedder.weight.data.uniform_(-.1, .1)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        #self.decoder.weight.data.uniform_(-.1, .1)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.embedder(x) # * math.sqrt(self.n_inputs)\n",
    "        x = x.squeeze(0)\n",
    "        #x = x.view(2500, 8)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1) \n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the number of the last batch sample in the data set is smaller than the defined batch_batch size, mismatch problems will occur. You can modify it yourself, for example, just pass in the shape behind, and then enter it through x.szie(0).\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class SignalEmbedder(nn.Module):\n",
    "    # Necessary to convert the signal into \"word\" vectors for transformer processing.\n",
    "    # Currently a simple group and slice method, but will modify later for multi-channel inputs\n",
    "    \n",
    "    def __init__(self, num_slices, size_of_slice):\n",
    "        super(SignalEmbedder, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.size_of_slice = size_of_slice\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[: self.num_slices * self.size_of_slice]\n",
    "        x = x.reshape((self.num_slices, self.size_of_slice))\n",
    "        return x\n",
    "'''\n",
    "class OneHotConverter(nn.Module):\n",
    "    # Converts the sigmoid output into one-hots\n",
    "    \n",
    "    def __init__(self, size, sentence_length):\n",
    "        super(OneHotConverter, self).__init__()\n",
    "        self.arr_length = size\n",
    "        self.num_words = sentence_length\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for num in x:\n",
    "            num = num.item()\n",
    "            num *= self.arr_length\n",
    "            val = np.zeros(self.arr_length)\n",
    "            val[int(round(num))] = 1\n",
    "        \n",
    "            output.append(val)\n",
    "        output = torch.as_tensor(output)\n",
    "        output.requires_grad_()\n",
    "        return output\n",
    "'''    \n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    # Necessary to store positional data about the input data\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=2500, batch_size = 1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, 1, embed_dim)\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        divisor = torch.exp(torch.arange(0, embed_dim, 2).float() * (- math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pos_encoding[:, 0, 0::2] = torch.sin(position * divisor)\n",
    "        pos_encoding[:, 0, 1::2] = torch.cos(position * divisor)\n",
    "        pos_encoding = pos_encoding.repeat(1, batch_size, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoding[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 20 14 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "tensor([[[0.5039, 0.5059, 0.4980,  ..., 0.5117, 0.5078, 0.5059],\n",
      "         [0.5029, 0.5049, 0.4980,  ..., 0.5088, 0.5049, 0.5039],\n",
      "         [0.5020, 0.5039, 0.4980,  ..., 0.5059, 0.5020, 0.5020],\n",
      "         ...,\n",
      "         [0.4932, 0.4941, 0.5020,  ..., 0.4814, 0.4824, 0.4844],\n",
      "         [0.4902, 0.4922, 0.5059,  ..., 0.4805, 0.4805, 0.4805],\n",
      "         [0.4902, 0.4922, 0.5059,  ..., 0.4805, 0.4805, 0.4805]],\n",
      "\n",
      "        [[0.4922, 0.4902, 0.4980,  ..., 0.4746, 0.4746, 0.4844],\n",
      "         [0.4922, 0.4902, 0.4980,  ..., 0.4756, 0.4756, 0.4854],\n",
      "         [0.4922, 0.4902, 0.4980,  ..., 0.4766, 0.4766, 0.4863],\n",
      "         ...,\n",
      "         [0.5098, 0.5088, 0.4980,  ..., 0.5088, 0.5107, 0.5068],\n",
      "         [0.5059, 0.5020, 0.5000,  ..., 0.4990, 0.5020, 0.5000],\n",
      "         [0.5059, 0.5020, 0.5000,  ..., 0.4990, 0.5020, 0.5000]],\n",
      "\n",
      "        [[0.4785, 0.4961, 0.5078,  ..., 0.5195, 0.5176, 0.5137],\n",
      "         [0.4805, 0.4961, 0.5088,  ..., 0.5215, 0.5195, 0.5146],\n",
      "         [0.4824, 0.4961, 0.5098,  ..., 0.5234, 0.5215, 0.5156],\n",
      "         ...,\n",
      "         [0.3877, 0.4775, 0.4717,  ..., 0.3057, 0.3516, 0.4209],\n",
      "         [0.3672, 0.4609, 0.4590,  ..., 0.2871, 0.3320, 0.3984],\n",
      "         [0.3672, 0.4609, 0.4590,  ..., 0.2871, 0.3320, 0.3984]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.4932, 0.4854, 0.5068,  ..., 0.4805, 0.4775, 0.4834],\n",
      "         [0.4961, 0.4873, 0.5107,  ..., 0.4795, 0.4795, 0.4824],\n",
      "         [0.4990, 0.4893, 0.5137,  ..., 0.4795, 0.4805, 0.4824],\n",
      "         ...,\n",
      "         [0.4844, 0.4883, 0.4941,  ..., 0.4824, 0.4795, 0.4854],\n",
      "         [0.4873, 0.4912, 0.4951,  ..., 0.4814, 0.4795, 0.4854],\n",
      "         [0.4873, 0.4912, 0.4951,  ..., 0.4814, 0.4795, 0.4854]],\n",
      "\n",
      "        [[0.4688, 0.4824, 0.5078,  ..., 0.4746, 0.4766, 0.4844],\n",
      "         [0.4688, 0.4814, 0.5088,  ..., 0.4746, 0.4766, 0.4834],\n",
      "         [0.4688, 0.4805, 0.5098,  ..., 0.4746, 0.4766, 0.4824],\n",
      "         ...,\n",
      "         [0.4893, 0.5020, 0.5176,  ..., 0.4814, 0.4766, 0.4814],\n",
      "         [0.4893, 0.5010, 0.5176,  ..., 0.4805, 0.4766, 0.4814],\n",
      "         [0.4893, 0.5010, 0.5176,  ..., 0.4805, 0.4766, 0.4814]],\n",
      "\n",
      "        [[0.4922, 0.4961, 0.5059,  ..., 0.4941, 0.4941, 0.4922],\n",
      "         [0.4951, 0.4961, 0.5068,  ..., 0.4932, 0.4932, 0.4922],\n",
      "         [0.4980, 0.4961, 0.5078,  ..., 0.4922, 0.4922, 0.4922],\n",
      "         ...,\n",
      "         [0.4941, 0.4824, 0.5088,  ..., 0.4834, 0.4814, 0.4795],\n",
      "         [0.4912, 0.4814, 0.5088,  ..., 0.4824, 0.4805, 0.4795],\n",
      "         [0.4912, 0.4814, 0.5088,  ..., 0.4824, 0.4805, 0.4795]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ECGTransformerEncoder(\n",
       "  (positional_encoder): PositionalEncoder(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (embedder): Sequential(\n",
       "    (conv_0): Conv1d(8, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (act_0): ReLU()\n",
       "    (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res_0): ResBlock1D(\n",
       "      (act): ReLU()\n",
       "      (conv1d_1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (conv1d_2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_res_0): ReLU()\n",
       "    (conv_1): Conv1d(16, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (act_1): ReLU()\n",
       "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res_1): ResBlock1D(\n",
       "      (act): ReLU()\n",
       "      (conv1d_1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (conv1d_2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_res_1): ReLU()\n",
       "    (conv_2): Conv1d(32, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (act_2): ReLU()\n",
       "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res_2): ResBlock1D(\n",
       "      (act): ReLU()\n",
       "      (conv1d_1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (conv1d_2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_res_2): ReLU()\n",
       "    (conv_3): Conv1d(64, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (act_3): ReLU()\n",
       "    (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res_3): ResBlock1D(\n",
       "      (act): ReLU()\n",
       "      (conv1d_1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (conv1d_2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_res_3): ReLU()\n",
       "    (conv_4): Conv1d(128, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (act_4): ReLU()\n",
       "    (batch_norm_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (res_4): ResBlock1D(\n",
       "      (act): ReLU()\n",
       "      (conv1d_1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (conv1d_2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act_res_4): ReLU()\n",
       "    (conv_fin): Conv1d(256, 768, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (act_fin): ReLU()\n",
       "    (batch_fin): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=17, bias=True)\n",
       "    (1): Transpose()\n",
       "    (2): Linear(in_features=2500, out_features=30, bias=True)\n",
       "    (3): LogSoftmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model to set to\n",
    "model = ECGTransformerEncoder(vector_size=5, embed_dim=768, n_heads=16, hidden_linear_dim=2048, n_layers=2, dropout=0.3).to(device)\n",
    "\n",
    "# Training params\n",
    "loss_function = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "data = []\n",
    "for arr in waveform_lead_rhythm_diag[\"decoded_waveform\"]:\n",
    "    #print(arr)\n",
    "    arr = arr.transpose()\n",
    "    data.append(arr)\n",
    "\n",
    "labels = []\n",
    "for sentence in waveform_lead_rhythm_diag[\"diagnosis\"]:\n",
    "    #label = one_hot(sentence, dict_words)\n",
    "    label = []\n",
    "    for word in sentence.split():\n",
    "        label.append(word_map[word])\n",
    "    \n",
    "    while len(label) < 17:\n",
    "        label.append(0)\n",
    "    labels.append(np.array(label))\n",
    "data = torch.from_numpy(np.array(data, dtype=np.float64)).type(torch.FloatTensor)\n",
    "print(labels[1])\n",
    "labels = torch.from_numpy(np.array(labels))\n",
    "print(data)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23  9  4  7 24  3 26  5  0  0  0  0  0  0  0  0  0]\n",
      "[23  9  4  7 24  3 26  5  0  0  0  0  0  0  0  0  0]\n",
      "[ 9 20 14 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 9 20 14 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 9 11 14 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[ 9 11 14 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[23  9  4 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[23  9  4 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[23  9  4 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[23  9  4 23  5  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[23  9  4 17  9  2 16 24 29 10 22 15 12 23  1 26  5]\n",
      "[23  9  4 17  9  2 16 24 29 10 22 15 12 23  1 26  5]\n",
      "[25 19 28  5 23  9  4 17  9  2 23  5  0  0  0  0  0]\n",
      "[25 19 28  5 23  9  4 17  9  2 23  5  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "for i in range(0):\n",
    "    losses = 0\n",
    "    for x, y in zip(new_data, labels):\n",
    "        print(x.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.unsqueeze(x.view(2500,768), 0))\n",
    "        #print(outputs.shape)\n",
    "        loss = loss_function(outputs, y)\n",
    "        losses += loss\n",
    "    losses.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    print(losses)\n",
    "    if losses < .001:\n",
    "        break\n",
    "        \n",
    "for x, y in zip(new_data, labels):\n",
    "    print(np.argmax(model(x.view(2500,768)).detach().numpy(), axis=1))\n",
    "    print(y.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/transformer_8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "logits = outputs.logits\n",
    "#assert logits[0, 0] < logits[0, 1] # next sentence was random\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.4789, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'GPT2LMHeadModel' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e5e4bb09b885>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "# define tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# preprocess training labels and tokenize\n",
    "train_labels = list(waveform_lead_rhythm_diag['diagnosis'])\n",
    "inputs = tokenizer(train_labels, padding = True, pad_token = tokenizer.add_special_tokens({'pad_token': '[PAD]'}), verbose = False, return_tensors=\"pt\")\n",
    "\n",
    "# adjust model parameters to account for padding token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for i in range(epochs):\n",
    "#model_gpt2DoubleHeadsModel.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels = inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "logits = model.logits\n",
    "print(np.argmax(logits[0].detach().numpy(), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Cohesive 1 Wrapper Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 - FNET/Basic Mixup Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
