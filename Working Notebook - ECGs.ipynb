{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from base64 import b64decode as decode\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/d_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/d_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/d_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/d_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead dataz\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "\n",
    "\n",
    "# adding the diagnosis and labels\n",
    "waveform_and_diag = pd.merge(waveform_lead[['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']], diagnosis_data[[\"exam_id\", \"Full_text\", \"Original_Diag\"]], left_on= \"exam_id\", right_on=\"exam_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: MinMaxScaler().fit_transform(np.vstack(x)))\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "waveform_lead_median = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Median\"]\n",
    "\n",
    "waveform_lead_rhythm['decoded_waveform'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_id</th>\n",
       "      <th>waveform_type</th>\n",
       "      <th>decoded_waveform</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>548759</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.42857142857142855, 0.45454545454545453, 0....</td>\n",
       "      <td>normal sinus rhythm low voltage qrs borderline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.75, 0.7391304347826086, 0.7272727272727272...</td>\n",
       "      <td>sinus bradycardia otherwise normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550602</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>sinus tachycardia otherwise normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551485</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.5, 0.5079365079365079, 0.5161290322580645,...</td>\n",
       "      <td>normal sinus rhythm normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552077</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.5333333333333332, 0.53125, 0.5714285714285...</td>\n",
       "      <td>normal sinus rhythm normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>552856</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>normal sinus rhythm with sinus arrhythmia mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>553115</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[0.0, 0.2, 0.375, 0.375, 0.375, 0.1875, 0.0, ...</td>\n",
       "      <td>atrial fibrillation abnormal ecg normal sinus ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exam_id waveform_type                                   decoded_waveform  \\\n",
       "0   548759        Rhythm  [[0.42857142857142855, 0.45454545454545453, 0....   \n",
       "1   549871        Rhythm  [[0.75, 0.7391304347826086, 0.7272727272727272...   \n",
       "2   550602        Rhythm  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3   551485        Rhythm  [[0.5, 0.5079365079365079, 0.5161290322580645,...   \n",
       "4   552077        Rhythm  [[0.5333333333333332, 0.53125, 0.5714285714285...   \n",
       "5   552856        Rhythm  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "6   553115        Rhythm  [[0.0, 0.2, 0.375, 0.375, 0.375, 0.1875, 0.0, ...   \n",
       "\n",
       "                                           diagnosis  \n",
       "0  normal sinus rhythm low voltage qrs borderline...  \n",
       "1             sinus bradycardia otherwise normal ecg  \n",
       "2             sinus tachycardia otherwise normal ecg  \n",
       "3                     normal sinus rhythm normal ecg  \n",
       "4                     normal sinus rhythm normal ecg  \n",
       "5  normal sinus rhythm with sinus arrhythmia mini...  \n",
       "6  atrial fibrillation abnormal ecg normal sinus ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the labels/sentences\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "# Let's look over this tomorrow\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "#\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if row[\"statement_order\"] == 1 and curr_string != \"\":\n",
    "        curr_string = curr_string.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        val = [curr_id, curr_string[1:]]\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    curr_string += \" \" + row[\"Full_text\"]\n",
    "\n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "#waveform_lead_rhythm_diag\n",
    "waveform_lead_rhythm_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criteria', 'borderline', 'variant', 'otherwise', 'fibrillation', 'lvh', 'ischemia', 'abnormal', 'abnormality', 'tachycardia', 'normal', 'wave', 'consider', 'inferior', 'atrial', 'voltage', 'for', 'with', 'may', 'ecg', 'low', 'bradycardia', 'qrs', 'be', 'rhythm', 'sinus', 't', 'arrhythmia', 'minimal'}\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for num, sentence in diagnoses:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word)\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10, 25, 24, 10, 19, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29],\n",
       "        [14,  4,  7, 19, 10, 25, 24, 17, 25, 27, 10, 19, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29],\n",
       "        [10, 25, 24, 20, 15, 22,  1, 19, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29],\n",
       "        [25, 21,  3, 10, 19, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29],\n",
       "        [10, 25, 24, 17, 25, 27, 28, 15,  0, 16,  5, 18, 23, 10,  2,  1, 19, 29,\n",
       "         29, 29],\n",
       "        [10, 25, 24, 10, 19, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
       "         29, 29]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into training and testing datasets\n",
    "# y not included for now\n",
    "def one_hot(x, dict_words, max_length):\n",
    "    x = x.split(\" \")\n",
    "    array = []\n",
    "    for i in x:\n",
    "        array.append(dict_words.index(i))\n",
    "    while(len(array) < max_length):\n",
    "        array.append(29)\n",
    "    return array\n",
    "\n",
    "dict_words = list(unique_words)\n",
    "dict_words.append([\" \"])\n",
    "print(len(dict_words))\n",
    "Y = waveform_lead_rhythm_diag['diagnosis'].apply(lambda x: one_hot(x, dict_words, 20))\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(waveform_lead_rhythm_diag['decoded_waveform'], Y, test_size = 0.1, random_state = 2021)\n",
    "train_x = torch.tensor(list(train_x)).float()\n",
    "train_y = torch.tensor(list(train_y))\n",
    "\n",
    "test_x = torch.tensor(list(test_x)).float()\n",
    "test_y = torch.tensor(list(test_y))\n",
    "\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Conv1D Encoder w/ LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv_0): Conv1d(8, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_0): ReLU()\n",
      "  (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_0): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_0): ReLU()\n",
      "  (conv_1): Conv1d(16, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_1): ReLU()\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_1): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_1): ReLU()\n",
      "  (conv_2): Conv1d(32, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_2): ReLU()\n",
      "  (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_2): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_2): ReLU()\n",
      "  (conv_3): Conv1d(64, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_3): ReLU()\n",
      "  (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_3): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_3): ReLU()\n",
      "  (conv_4): Conv1d(128, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_4): ReLU()\n",
      "  (batch_norm_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_4): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_4): ReLU()\n",
      "  (conv_fin): Conv1d(256, 8, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_fin): ReLU()\n",
      "  (batch_fin): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "torch.Size([6, 8, 2500])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(5):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = 249, padding = 124, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = 249, padding = 124))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = 249, padding = 124))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "print(conv_model)\n",
    "print(conv_model(train_x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (initial_norm): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_1): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,))\n",
      "  (conv_2): Conv1d(8, 16, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "  (activation_2): ELU(alpha=1.0)\n",
      "  (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_3): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(2,))\n",
      "  (activation_3): ELU(alpha=1.0)\n",
      "  (batch_norm_3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (final_conv): Conv1d(32, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "  (max_pool): MaxPool1d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      ")\n",
      "torch.Size([6, 8, 626])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 4 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# 1D grouped encoder model\n",
    "encoder_conv = nn.Sequential()\n",
    "encoder_conv.add_module('initial_norm', nn.BatchNorm1d(8))\n",
    "encoder_conv.add_module('conv_1', nn.Conv1d(in_channels = 8, out_channels = 8, kernel_size = 5, padding = 4, stride = 1))\n",
    "for i in range(2, (J+2), 2):\n",
    "    if (i-2) == 0: \n",
    "        prev = 8\n",
    "    else:\n",
    "        prev = (i-2)*8\n",
    "    encoder_conv.add_module('conv_{num}'.format(num = int(i / 2 + 1)), nn.Conv1d(in_channels = prev, out_channels = i*8, kernel_size = 5, padding = 2, stride = 2))\n",
    "    encoder_conv.add_module('activation_{num}'.format(num = int(i / 2 + 1)), nn.ELU())\n",
    "    encoder_conv.add_module('batch_norm_{num}'.format(num = int(i / 2 + 1)), nn.BatchNorm1d(i*8))\n",
    "    \n",
    "encoder_conv.add_module('final_conv', nn.Conv1d(in_channels = J * 8, out_channels = 8, kernel_size = 5, padding = 2))\n",
    "encoder_conv.add_module('max_pool', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "#encoder_conv.add_module('reshape', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "\n",
    "# summarize model, verify output is of desired shape\n",
    "print(encoder_conv)\n",
    "print(encoder_conv(train_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - LSTM Encoder w/ Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 250\n",
    "embedding_dim = 8\n",
    "num_words = len(dict_words)\n",
    "\n",
    "class LSTM_EncoderDecoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim, word_list_length):\n",
    "        super(ECG_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim, num_layers = 4, bidirectional = True)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq_embedded = seq.view(len(seq), -1, embedding_dim)\n",
    "        final_hidd, _ = self.lstm(seq_embedded)\n",
    "        dec_seq = self.linear(final_hidd)\n",
    "        return F.log_softmax(dec_seq, dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-5708ab1cc4ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_mod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-9d533def8691>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mseq_embedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mfinal_hidd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_embedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdec_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_hidd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2058\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2059\u001b[0m     )\n\u001b[0;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(lstm_mod.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j, k in zip(train_x, train_y):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_mod(j.unsqueeze(0)).squeeze(0)\n",
    "        loss = loss_fn(outputs, k)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 30)\n",
      "[21 11 10 21  5 26 23 25 22  6 29 25 27 26  4 26  4 26 26  7]\n",
      "tensor([21, 11, 10, 21,  5, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29])\n"
     ]
    }
   ],
   "source": [
    "#torch.save(lstm_mod.state_dict(), 'model/lstm.pt')\n",
    "lstm_mod = ECG_LSTM(encoder_conv, hidden_layers, embedding_dim, num_words)\n",
    "lstm_mod.load_state_dict(torch.load('model/lstm.pt'))\n",
    "\n",
    "out = lstm_mod(train_x[5].unsqueeze(0))\n",
    "print(out.squeeze(0).detach().numpy().shape)\n",
    "out = np.argmax(out.squeeze(0).detach().numpy(), axis = 1)\n",
    "print(out)\n",
    "print(train_y[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Basic Transformer Architecture with Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(features, features * expansion)\n",
    "        self.linear_2 = nn.Linear(features * expansion, features)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        #self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.norm_1(x + res)\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class FNETLayer(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FNETLayer, self).__init__()\n",
    "        self.feed_forward = FeedForwardNet(features, expansion, dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = torch.fft.fftn(x, dim = (-1, -2)).real\n",
    "        x = self.norm_1(x + res)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "    \n",
    "class FNETEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self, features, expansion=2, dropout=0.5, num_layers=6):\n",
    "        encoder_layer = FNETLayer(features, expansion, dropout)\n",
    "        super().__init__(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class FNETModel(nn.Module):\n",
    "    def __init__(self, expansion, dropout, d_model, num_layers, decoder):\n",
    "        super(FNETModel, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.pos_enb = PositionalEncoding(d_model = d_model)\n",
    "        self.encoder = FNETEncoder(features = d_model, expansion = expansion, dropout = dropout, num_layers = num_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pos_enb(x)\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(x)\n",
    "        return out    \n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "decoder = nn.Sequential(Transpose(), \n",
    "                        nn.Linear(8, 15),\n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(15, 20), \n",
    "                        nn.ReLU(), \n",
    "                        Transpose(), \n",
    "                        nn.Linear(2500, 2000), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(2000, 1500), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(1500, 1000), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(1000, 500), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(500, 100), \n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(100, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.1033, grad_fn=<AddBackward0>)\n",
      "tensor(13.5739, grad_fn=<AddBackward0>)\n",
      "tensor(21.0495, grad_fn=<AddBackward0>)\n",
      "tensor(15.1338, grad_fn=<AddBackward0>)\n",
      "tensor(10.9025, grad_fn=<AddBackward0>)\n",
      "tensor(9.9206, grad_fn=<AddBackward0>)\n",
      "tensor(9.7928, grad_fn=<AddBackward0>)\n",
      "tensor(9.9478, grad_fn=<AddBackward0>)\n",
      "tensor(9.3364, grad_fn=<AddBackward0>)\n",
      "tensor(8.5085, grad_fn=<AddBackward0>)\n",
      "tensor(8.4253, grad_fn=<AddBackward0>)\n",
      "tensor(8.1964, grad_fn=<AddBackward0>)\n",
      "tensor(8.8776, grad_fn=<AddBackward0>)\n",
      "tensor(8.2562, grad_fn=<AddBackward0>)\n",
      "tensor(7.9390, grad_fn=<AddBackward0>)\n",
      "tensor(7.6749, grad_fn=<AddBackward0>)\n",
      "tensor(7.3612, grad_fn=<AddBackward0>)\n",
      "tensor(7.3261, grad_fn=<AddBackward0>)\n",
      "tensor(7.1017, grad_fn=<AddBackward0>)\n",
      "tensor(6.7182, grad_fn=<AddBackward0>)\n",
      "tensor(6.6983, grad_fn=<AddBackward0>)\n",
      "tensor(7.1226, grad_fn=<AddBackward0>)\n",
      "tensor(7.0148, grad_fn=<AddBackward0>)\n",
      "tensor(6.7546, grad_fn=<AddBackward0>)\n",
      "tensor(6.6756, grad_fn=<AddBackward0>)\n",
      "tensor(6.7715, grad_fn=<AddBackward0>)\n",
      "tensor(6.5788, grad_fn=<AddBackward0>)\n",
      "tensor(6.3692, grad_fn=<AddBackward0>)\n",
      "tensor(6.1862, grad_fn=<AddBackward0>)\n",
      "tensor(6.2560, grad_fn=<AddBackward0>)\n",
      "tensor(6.2180, grad_fn=<AddBackward0>)\n",
      "tensor(5.5821, grad_fn=<AddBackward0>)\n",
      "tensor(6.0089, grad_fn=<AddBackward0>)\n",
      "tensor(6.2956, grad_fn=<AddBackward0>)\n",
      "tensor(6.0355, grad_fn=<AddBackward0>)\n",
      "tensor(6.3123, grad_fn=<AddBackward0>)\n",
      "tensor(5.5553, grad_fn=<AddBackward0>)\n",
      "tensor(5.5863, grad_fn=<AddBackward0>)\n",
      "tensor(5.6801, grad_fn=<AddBackward0>)\n",
      "tensor(5.9284, grad_fn=<AddBackward0>)\n",
      "tensor(5.4362, grad_fn=<AddBackward0>)\n",
      "tensor(5.1188, grad_fn=<AddBackward0>)\n",
      "tensor(5.6535, grad_fn=<AddBackward0>)\n",
      "tensor(4.9500, grad_fn=<AddBackward0>)\n",
      "tensor(5.1462, grad_fn=<AddBackward0>)\n",
      "tensor(4.8054, grad_fn=<AddBackward0>)\n",
      "tensor(4.6851, grad_fn=<AddBackward0>)\n",
      "tensor(4.7545, grad_fn=<AddBackward0>)\n",
      "tensor(4.7306, grad_fn=<AddBackward0>)\n",
      "tensor(4.5485, grad_fn=<AddBackward0>)\n",
      "tensor(4.9952, grad_fn=<AddBackward0>)\n",
      "tensor(4.4413, grad_fn=<AddBackward0>)\n",
      "tensor(4.5301, grad_fn=<AddBackward0>)\n",
      "tensor(4.5697, grad_fn=<AddBackward0>)\n",
      "tensor(4.3245, grad_fn=<AddBackward0>)\n",
      "tensor(4.0852, grad_fn=<AddBackward0>)\n",
      "tensor(4.2651, grad_fn=<AddBackward0>)\n",
      "tensor(4.1365, grad_fn=<AddBackward0>)\n",
      "tensor(4.9752, grad_fn=<AddBackward0>)\n",
      "tensor(4.2551, grad_fn=<AddBackward0>)\n",
      "tensor(4.3220, grad_fn=<AddBackward0>)\n",
      "tensor(5.2639, grad_fn=<AddBackward0>)\n",
      "tensor(4.3669, grad_fn=<AddBackward0>)\n",
      "tensor(3.8189, grad_fn=<AddBackward0>)\n",
      "tensor(3.3798, grad_fn=<AddBackward0>)\n",
      "tensor(3.8909, grad_fn=<AddBackward0>)\n",
      "tensor(3.7253, grad_fn=<AddBackward0>)\n",
      "tensor(4.7946, grad_fn=<AddBackward0>)\n",
      "tensor(3.2294, grad_fn=<AddBackward0>)\n",
      "tensor(3.3763, grad_fn=<AddBackward0>)\n",
      "tensor(3.3446, grad_fn=<AddBackward0>)\n",
      "tensor(3.3492, grad_fn=<AddBackward0>)\n",
      "tensor(3.0091, grad_fn=<AddBackward0>)\n",
      "tensor(3.9839, grad_fn=<AddBackward0>)\n",
      "tensor(4.0823, grad_fn=<AddBackward0>)\n",
      "tensor(3.4406, grad_fn=<AddBackward0>)\n",
      "tensor(3.7415, grad_fn=<AddBackward0>)\n",
      "tensor(4.2287, grad_fn=<AddBackward0>)\n",
      "tensor(4.6934, grad_fn=<AddBackward0>)\n",
      "tensor(4.4623, grad_fn=<AddBackward0>)\n",
      "tensor(4.4164, grad_fn=<AddBackward0>)\n",
      "tensor(3.8477, grad_fn=<AddBackward0>)\n",
      "tensor(4.5148, grad_fn=<AddBackward0>)\n",
      "tensor(4.2647, grad_fn=<AddBackward0>)\n",
      "tensor(3.0103, grad_fn=<AddBackward0>)\n",
      "tensor(2.9202, grad_fn=<AddBackward0>)\n",
      "tensor(3.6015, grad_fn=<AddBackward0>)\n",
      "tensor(2.6427, grad_fn=<AddBackward0>)\n",
      "tensor(2.6968, grad_fn=<AddBackward0>)\n",
      "tensor(2.3253, grad_fn=<AddBackward0>)\n",
      "tensor(2.0563, grad_fn=<AddBackward0>)\n",
      "tensor(2.8022, grad_fn=<AddBackward0>)\n",
      "tensor(2.6601, grad_fn=<AddBackward0>)\n",
      "tensor(1.9918, grad_fn=<AddBackward0>)\n",
      "tensor(2.8672, grad_fn=<AddBackward0>)\n",
      "tensor(2.9919, grad_fn=<AddBackward0>)\n",
      "tensor(2.1337, grad_fn=<AddBackward0>)\n",
      "tensor(2.6678, grad_fn=<AddBackward0>)\n",
      "tensor(3.1562, grad_fn=<AddBackward0>)\n",
      "tensor(1.6096, grad_fn=<AddBackward0>)\n",
      "tensor(1.6296, grad_fn=<AddBackward0>)\n",
      "tensor(1.7211, grad_fn=<AddBackward0>)\n",
      "tensor(1.6318, grad_fn=<AddBackward0>)\n",
      "tensor(1.2498, grad_fn=<AddBackward0>)\n",
      "tensor(1.2414, grad_fn=<AddBackward0>)\n",
      "tensor(1.0949, grad_fn=<AddBackward0>)\n",
      "tensor(1.0630, grad_fn=<AddBackward0>)\n",
      "tensor(1.7110, grad_fn=<AddBackward0>)\n",
      "tensor(2.2675, grad_fn=<AddBackward0>)\n",
      "tensor(0.9866, grad_fn=<AddBackward0>)\n",
      "tensor(0.9390, grad_fn=<AddBackward0>)\n",
      "tensor(0.9775, grad_fn=<AddBackward0>)\n",
      "tensor(2.4175, grad_fn=<AddBackward0>)\n",
      "tensor(1.5564, grad_fn=<AddBackward0>)\n",
      "tensor(0.8492, grad_fn=<AddBackward0>)\n",
      "tensor(1.4710, grad_fn=<AddBackward0>)\n",
      "tensor(3.5376, grad_fn=<AddBackward0>)\n",
      "tensor(1.5938, grad_fn=<AddBackward0>)\n",
      "tensor(2.0841, grad_fn=<AddBackward0>)\n",
      "tensor(1.4436, grad_fn=<AddBackward0>)\n",
      "tensor(2.0130, grad_fn=<AddBackward0>)\n",
      "tensor(2.0142, grad_fn=<AddBackward0>)\n",
      "tensor(1.7404, grad_fn=<AddBackward0>)\n",
      "tensor(1.8237, grad_fn=<AddBackward0>)\n",
      "tensor(1.5442, grad_fn=<AddBackward0>)\n",
      "tensor(1.2050, grad_fn=<AddBackward0>)\n",
      "tensor(1.4903, grad_fn=<AddBackward0>)\n",
      "tensor(1.3399, grad_fn=<AddBackward0>)\n",
      "tensor(0.6630, grad_fn=<AddBackward0>)\n",
      "tensor(0.8340, grad_fn=<AddBackward0>)\n",
      "tensor(0.9244, grad_fn=<AddBackward0>)\n",
      "tensor(0.7921, grad_fn=<AddBackward0>)\n",
      "tensor(1.9189, grad_fn=<AddBackward0>)\n",
      "tensor(0.5077, grad_fn=<AddBackward0>)\n",
      "tensor(0.6653, grad_fn=<AddBackward0>)\n",
      "tensor(0.5776, grad_fn=<AddBackward0>)\n",
      "tensor(0.5981, grad_fn=<AddBackward0>)\n",
      "tensor(0.6541, grad_fn=<AddBackward0>)\n",
      "tensor(0.2216, grad_fn=<AddBackward0>)\n",
      "tensor(0.4382, grad_fn=<AddBackward0>)\n",
      "tensor(0.2206, grad_fn=<AddBackward0>)\n",
      "tensor(0.2873, grad_fn=<AddBackward0>)\n",
      "tensor(0.1881, grad_fn=<AddBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "tensor(0.2500, grad_fn=<AddBackward0>)\n",
      "tensor(1.1676, grad_fn=<AddBackward0>)\n",
      "tensor(0.4180, grad_fn=<AddBackward0>)\n",
      "tensor(0.3841, grad_fn=<AddBackward0>)\n",
      "tensor(1.3942, grad_fn=<AddBackward0>)\n",
      "tensor(0.5507, grad_fn=<AddBackward0>)\n",
      "tensor(3.2813, grad_fn=<AddBackward0>)\n",
      "tensor(0.7083, grad_fn=<AddBackward0>)\n",
      "tensor(3.3329, grad_fn=<AddBackward0>)\n",
      "tensor(0.6389, grad_fn=<AddBackward0>)\n",
      "tensor(4.3503, grad_fn=<AddBackward0>)\n",
      "tensor(1.4490, grad_fn=<AddBackward0>)\n",
      "tensor(0.3762, grad_fn=<AddBackward0>)\n",
      "tensor(1.9821, grad_fn=<AddBackward0>)\n",
      "tensor(1.5461, grad_fn=<AddBackward0>)\n",
      "tensor(1.2248, grad_fn=<AddBackward0>)\n",
      "tensor(1.2479, grad_fn=<AddBackward0>)\n",
      "tensor(0.5563, grad_fn=<AddBackward0>)\n",
      "tensor(1.2159, grad_fn=<AddBackward0>)\n",
      "tensor(1.1613, grad_fn=<AddBackward0>)\n",
      "tensor(0.4543, grad_fn=<AddBackward0>)\n",
      "tensor(0.5636, grad_fn=<AddBackward0>)\n",
      "tensor(0.5810, grad_fn=<AddBackward0>)\n",
      "tensor(0.5506, grad_fn=<AddBackward0>)\n",
      "tensor(0.3705, grad_fn=<AddBackward0>)\n",
      "tensor(4.6872, grad_fn=<AddBackward0>)\n",
      "tensor(0.6506, grad_fn=<AddBackward0>)\n",
      "tensor(0.5080, grad_fn=<AddBackward0>)\n",
      "tensor(0.8878, grad_fn=<AddBackward0>)\n",
      "tensor(3.0198, grad_fn=<AddBackward0>)\n",
      "tensor(0.8405, grad_fn=<AddBackward0>)\n",
      "tensor(0.7411, grad_fn=<AddBackward0>)\n",
      "tensor(0.8103, grad_fn=<AddBackward0>)\n",
      "tensor(0.6606, grad_fn=<AddBackward0>)\n",
      "tensor(0.6224, grad_fn=<AddBackward0>)\n",
      "tensor(0.3580, grad_fn=<AddBackward0>)\n",
      "tensor(0.3623, grad_fn=<AddBackward0>)\n",
      "tensor(0.6050, grad_fn=<AddBackward0>)\n",
      "tensor(0.9039, grad_fn=<AddBackward0>)\n",
      "tensor(0.3400, grad_fn=<AddBackward0>)\n",
      "tensor(0.4116, grad_fn=<AddBackward0>)\n",
      "tensor(0.1615, grad_fn=<AddBackward0>)\n",
      "tensor(0.2016, grad_fn=<AddBackward0>)\n",
      "tensor(0.3591, grad_fn=<AddBackward0>)\n",
      "tensor(0.0762, grad_fn=<AddBackward0>)\n",
      "tensor(0.3849, grad_fn=<AddBackward0>)\n",
      "tensor(1.0880, grad_fn=<AddBackward0>)\n",
      "tensor(0.2901, grad_fn=<AddBackward0>)\n",
      "tensor(0.3843, grad_fn=<AddBackward0>)\n",
      "tensor(0.2412, grad_fn=<AddBackward0>)\n",
      "tensor(0.4122, grad_fn=<AddBackward0>)\n",
      "tensor(0.6425, grad_fn=<AddBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "tensor(0.0654, grad_fn=<AddBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "tensor(0.2060, grad_fn=<AddBackward0>)\n",
      "tensor(0.7955, grad_fn=<AddBackward0>)\n",
      "tensor(0.2922, grad_fn=<AddBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "tensor(0.4515, grad_fn=<AddBackward0>)\n",
      "tensor(0.4964, grad_fn=<AddBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "tensor(0.0662, grad_fn=<AddBackward0>)\n",
      "tensor(1.0663, grad_fn=<AddBackward0>)\n",
      "tensor(0.0338, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1394, grad_fn=<AddBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "tensor(0.0246, grad_fn=<AddBackward0>)\n",
      "tensor(0.4736, grad_fn=<AddBackward0>)\n",
      "tensor(0.1304, grad_fn=<AddBackward0>)\n",
      "tensor(0.1634, grad_fn=<AddBackward0>)\n",
      "tensor(0.1823, grad_fn=<AddBackward0>)\n",
      "tensor(0.7106, grad_fn=<AddBackward0>)\n",
      "tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(0.0415, grad_fn=<AddBackward0>)\n",
      "tensor(1.8078, grad_fn=<AddBackward0>)\n",
      "tensor(0.9299, grad_fn=<AddBackward0>)\n",
      "tensor(0.3593, grad_fn=<AddBackward0>)\n",
      "tensor(0.2644, grad_fn=<AddBackward0>)\n",
      "tensor(0.3232, grad_fn=<AddBackward0>)\n",
      "tensor(0.2660, grad_fn=<AddBackward0>)\n",
      "tensor(0.8886, grad_fn=<AddBackward0>)\n",
      "tensor(1.7445, grad_fn=<AddBackward0>)\n",
      "tensor(0.1726, grad_fn=<AddBackward0>)\n",
      "tensor(0.4069, grad_fn=<AddBackward0>)\n",
      "tensor(0.2657, grad_fn=<AddBackward0>)\n",
      "tensor(3.5218, grad_fn=<AddBackward0>)\n",
      "tensor(1.5644, grad_fn=<AddBackward0>)\n",
      "tensor(0.7932, grad_fn=<AddBackward0>)\n",
      "tensor(0.3712, grad_fn=<AddBackward0>)\n",
      "tensor(0.9135, grad_fn=<AddBackward0>)\n",
      "tensor(0.2951, grad_fn=<AddBackward0>)\n",
      "tensor(0.3078, grad_fn=<AddBackward0>)\n",
      "tensor(0.7850, grad_fn=<AddBackward0>)\n",
      "tensor(0.1804, grad_fn=<AddBackward0>)\n",
      "tensor(0.5317, grad_fn=<AddBackward0>)\n",
      "tensor(0.5434, grad_fn=<AddBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "tensor(0.3684, grad_fn=<AddBackward0>)\n",
      "tensor(0.2204, grad_fn=<AddBackward0>)\n",
      "tensor(0.5839, grad_fn=<AddBackward0>)\n",
      "tensor(1.1177, grad_fn=<AddBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "tensor(0.3096, grad_fn=<AddBackward0>)\n",
      "tensor(0.1951, grad_fn=<AddBackward0>)\n",
      "tensor(0.2047, grad_fn=<AddBackward0>)\n",
      "tensor(0.1937, grad_fn=<AddBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "tensor(0.1245, grad_fn=<AddBackward0>)\n",
      "tensor(0.3339, grad_fn=<AddBackward0>)\n",
      "tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(0.0199, grad_fn=<AddBackward0>)\n",
      "tensor(0.0247, grad_fn=<AddBackward0>)\n",
      "tensor(0.0452, grad_fn=<AddBackward0>)\n",
      "tensor(0.0334, grad_fn=<AddBackward0>)\n",
      "tensor(0.0261, grad_fn=<AddBackward0>)\n",
      "tensor(0.0433, grad_fn=<AddBackward0>)\n",
      "tensor(0.1059, grad_fn=<AddBackward0>)\n",
      "tensor(0.1480, grad_fn=<AddBackward0>)\n",
      "tensor(0.0174, grad_fn=<AddBackward0>)\n",
      "tensor(0.0260, grad_fn=<AddBackward0>)\n",
      "tensor(0.2556, grad_fn=<AddBackward0>)\n",
      "tensor(0.0308, grad_fn=<AddBackward0>)\n",
      "tensor(0.0289, grad_fn=<AddBackward0>)\n",
      "tensor(0.6932, grad_fn=<AddBackward0>)\n",
      "tensor(0.0235, grad_fn=<AddBackward0>)\n",
      "tensor(0.0815, grad_fn=<AddBackward0>)\n",
      "tensor(0.1406, grad_fn=<AddBackward0>)\n",
      "tensor(0.1630, grad_fn=<AddBackward0>)\n",
      "tensor(0.0339, grad_fn=<AddBackward0>)\n",
      "tensor(0.2624, grad_fn=<AddBackward0>)\n",
      "tensor(1.3520, grad_fn=<AddBackward0>)\n",
      "tensor(0.1704, grad_fn=<AddBackward0>)\n",
      "tensor(0.4515, grad_fn=<AddBackward0>)\n",
      "tensor(0.2980, grad_fn=<AddBackward0>)\n",
      "tensor(1.1044, grad_fn=<AddBackward0>)\n",
      "tensor(0.0438, grad_fn=<AddBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "tensor(0.2314, grad_fn=<AddBackward0>)\n",
      "tensor(0.2417, grad_fn=<AddBackward0>)\n",
      "tensor(0.0543, grad_fn=<AddBackward0>)\n",
      "tensor(0.0662, grad_fn=<AddBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "tensor(0.0267, grad_fn=<AddBackward0>)\n",
      "tensor(0.0724, grad_fn=<AddBackward0>)\n",
      "tensor(0.0394, grad_fn=<AddBackward0>)\n",
      "tensor(0.0210, grad_fn=<AddBackward0>)\n",
      "tensor(0.0309, grad_fn=<AddBackward0>)\n",
      "tensor(0.0668, grad_fn=<AddBackward0>)\n",
      "tensor(0.7401, grad_fn=<AddBackward0>)\n",
      "tensor(0.0567, grad_fn=<AddBackward0>)\n",
      "tensor(0.0290, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epoch = 300\n",
    "#ECG_LSTM(hidden_layers, embedding_dim, num_words)\n",
    "model = FNETModel(expansion = 2, dropout = 0, d_model = train_x.shape[2], num_layers = 6, decoder = decoder)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for i in range(epoch):\n",
    "    losses = 0\n",
    "    for j, k in zip(train_x, train_y):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(j.unsqueeze(0)).squeeze(0)\n",
    "        loss = loss_fn(outputs, k)\n",
    "        losses += loss\n",
    "    losses.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNETModel(\n",
      "  (decoder): Sequential(\n",
      "    (0): Transpose()\n",
      "    (1): Linear(in_features=8, out_features=15, bias=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Linear(in_features=15, out_features=20, bias=True)\n",
      "    (4): ELU(alpha=1.0)\n",
      "    (5): Transpose()\n",
      "    (6): Linear(in_features=2500, out_features=2000, bias=True)\n",
      "    (7): ELU(alpha=1.0)\n",
      "    (8): Linear(in_features=2000, out_features=1500, bias=True)\n",
      "    (9): ELU(alpha=1.0)\n",
      "    (10): Linear(in_features=1500, out_features=1000, bias=True)\n",
      "    (11): ELU(alpha=1.0)\n",
      "    (12): Linear(in_features=1000, out_features=500, bias=True)\n",
      "    (13): ELU(alpha=1.0)\n",
      "    (14): Linear(in_features=500, out_features=100, bias=True)\n",
      "    (15): ELU(alpha=1.0)\n",
      "    (16): Linear(in_features=100, out_features=30, bias=True)\n",
      "  )\n",
      "  (pos_enb): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): FNETEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): FNETLayer(\n",
      "        (feed_forward): FeedForwardNet(\n",
      "          (linear_1): Linear(in_features=2500, out_features=5000, bias=True)\n",
      "          (linear_2): Linear(in_features=5000, out_features=2500, bias=True)\n",
      "          (dropout_1): Dropout(p=0, inplace=False)\n",
      "          (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): FNETLayer(\n",
      "        (feed_forward): FeedForwardNet(\n",
      "          (linear_1): Linear(in_features=2500, out_features=5000, bias=True)\n",
      "          (linear_2): Linear(in_features=5000, out_features=2500, bias=True)\n",
      "          (dropout_1): Dropout(p=0, inplace=False)\n",
      "          (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): FNETLayer(\n",
      "        (feed_forward): FeedForwardNet(\n",
      "          (linear_1): Linear(in_features=2500, out_features=5000, bias=True)\n",
      "          (linear_2): Linear(in_features=5000, out_features=2500, bias=True)\n",
      "          (dropout_1): Dropout(p=0, inplace=False)\n",
      "          (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): FNETLayer(\n",
      "        (feed_forward): FeedForwardNet(\n",
      "          (linear_1): Linear(in_features=2500, out_features=5000, bias=True)\n",
      "          (linear_2): Linear(in_features=5000, out_features=2500, bias=True)\n",
      "          (dropout_1): Dropout(p=0, inplace=False)\n",
      "          (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): FNETLayer(\n",
      "        (feed_forward): FeedForwardNet(\n",
      "          (linear_1): Linear(in_features=2500, out_features=5000, bias=True)\n",
      "          (linear_2): Linear(in_features=5000, out_features=2500, bias=True)\n",
      "          (dropout_1): Dropout(p=0, inplace=False)\n",
      "          (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): FNETLayer(\n",
      "        (feed_forward): FeedForwardNet(\n",
      "          (linear_1): Linear(in_features=2500, out_features=5000, bias=True)\n",
      "          (linear_2): Linear(in_features=5000, out_features=2500, bias=True)\n",
      "          (dropout_1): Dropout(p=0, inplace=False)\n",
      "          (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (norm_1): LayerNorm((2500,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "(20, 30)\n",
      "[ 3 14 15  2 27 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29]\n",
      "tensor([ 3, 14, 15,  2, 27, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29])\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model/fnet_2.pt')\n",
    "model = FNETModel(expansion = 2, dropout = 0, d_model = train_x.shape[2], num_layers = 6, decoder = decoder)\n",
    "model.load_state_dict(torch.load('model/fnet_2.pt'))\n",
    "print(model)\n",
    "out = (model(train_x[3].unsqueeze(0)))\n",
    "print(out.squeeze(0).detach().numpy().shape)\n",
    "out = np.argmax(out.squeeze(0).detach().numpy(), axis = 1)\n",
    "print(out)\n",
    "print(train_y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "#encoded_inputs = tokenizer(list(waveform_lead_rhythm_diag['diagnosis'])[0], return_tensors='pt')\n",
    "\n",
    "encoded_inputs = tokenizer('Hello my name is Daniel', return_tensors = 'pt')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 - FNET/Basic Mixup Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0729,  5.9056]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "logits = outputs.logits\n",
    "#assert logits[0, 0] < logits[0, 1] # next sentence was random\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n",
      "Keyword arguments {'pad_token': 1} not recognized.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'backwards'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-22ebe355ae43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'backwards'"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "# define tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# preprocess training labels and tokenize\n",
    "train_labels = list(waveform_lead_rhythm_diag['diagnosis'])\n",
    "inputs = tokenizer(train_labels, padding = True, pad_token = tokenizer.add_special_tokens({'pad_token': '[PAD]'}), verbose = False, return_tensors=\"pt\")\n",
    "\n",
    "# adjust model parameters to account for padding token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for i in range(epochs):\n",
    "#model_gpt2DoubleHeadsModel.resize_token_embeddings(len(gpt2_tokenizer))\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels = inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backwards()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "logits = model.logits\n",
    "print(np.argmax(logits[0].detach().numpy(), axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
