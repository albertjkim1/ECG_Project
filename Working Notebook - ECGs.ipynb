{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from base64 import b64decode as decode\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.fft as fft\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/d_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/d_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/d_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/d_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead dataz\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "\n",
    "\n",
    "# adding the diagnosis and labels\n",
    "waveform_and_diag = pd.merge(waveform_lead[['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']], diagnosis_data[[\"exam_id\", \"Full_text\", \"Original_Diag\"]], left_on= \"exam_id\", right_on=\"exam_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: MinMaxScaler().fit_transform(np.vstack(x)))\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "waveform_lead_median = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Median\"]\n",
    "\n",
    "waveform_lead_rhythm['decoded_waveform'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the labels/sentences\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "# Let's look over this tomorrow\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "#\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if row[\"statement_order\"] == 1 and curr_string != \"\":\n",
    "        curr_string = curr_string.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        val = [curr_id, curr_string[1:]]\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    curr_string += \" \" + row[\"Full_text\"]\n",
    "\n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "#waveform_lead_rhythm_diag\n",
    "waveform_lead_rhythm_diag\n",
    "\n",
    "# define full_x\n",
    "full_x = torch.tensor(list(waveform_lead_rhythm_diag['decoded_waveform'])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abnormality', 'normal', 'lvh', 'sinus', 'variant', 'tachycardia', 'ecg', 'low', 'may', 'otherwise', 'be', 'for', 'arrhythmia', 't', 'minimal', 'criteria', 'qrs', 'atrial', 'rhythm', 'wave', 'abnormal', 'bradycardia', 'voltage', 'fibrillation', 'with', 'inferior', 'ischemia', 'borderline', 'consider'}\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for num, sentence in diagnoses:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word)\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv_0): Conv1d(8, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_0): ReLU()\n",
      "  (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_0): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_0): ReLU()\n",
      "  (conv_1): Conv1d(16, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_1): ReLU()\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_1): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_1): ReLU()\n",
      "  (conv_2): Conv1d(32, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_2): ReLU()\n",
      "  (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_2): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_2): ReLU()\n",
      "  (conv_3): Conv1d(64, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_3): ReLU()\n",
      "  (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_3): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_3): ReLU()\n",
      "  (conv_4): Conv1d(128, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_4): ReLU()\n",
      "  (batch_norm_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_4): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_4): ReLU()\n",
      "  (conv_fin): Conv1d(256, 768, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_fin): ReLU()\n",
      "  (batch_fin): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "KER_SIZE = 11\n",
    "PADDING = 5\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(5):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 768, kernel_size = KER_SIZE, padding = PADDING))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(768))\n",
    "print(conv_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 8, 2500])\n",
      "torch.Size([7, 768, 2500])\n",
      "torch.Size([7, 8, 2500])\n"
     ]
    }
   ],
   "source": [
    "deconv_model = nn.Sequential()\n",
    "init_channels = 768\n",
    "for i in range(5):\n",
    "    next_channels = init_channels // 2\n",
    "    deconv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    deconv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    deconv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    deconv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    deconv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "deconv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = KER_SIZE, padding = PADDING))\n",
    "deconv_model.add_module('act_fin', nn.ReLU())\n",
    "deconv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "\n",
    "print(full_x.shape)\n",
    "print(conv_model(full_x).shape)\n",
    "print(deconv_model(conv_model(full_x)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def make_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def make_decoder(self):\n",
    "        return self.decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3016, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2688, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1736, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1416, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0946, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0625, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0360, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9886, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9671, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9323, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9133, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8988, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8857, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8753, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8668, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8571, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8494, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8425, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8355, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8292, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8223, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8157, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8091, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8028, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7972, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7916, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7860, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7811, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7762, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7710, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7659, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7611, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7569, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7524, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7435, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7392, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7353, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7312, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7268, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7230, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7194, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7158, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7120, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7083, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7045, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7013, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6980, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6947, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6915, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6881, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6845, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6814, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6786, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6763, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6728, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6694, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6667, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6638, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6608, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6576, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6548, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6521, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6488, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6458, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6428, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6369, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6341, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6310, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6275, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6244, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6208, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6134, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6098, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6070, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6035, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5958, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5914, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5851, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5820, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5780, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5737, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5709, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5683, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5651, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5616, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5590, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5560, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5532, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5504, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5454, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5429, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5405, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5379, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5353, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5329, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5305, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5282, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5261, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5233, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5210, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5188, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5166, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5145, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5122, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5102, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5085, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5072, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5061, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5044, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5000, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4979, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4963, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4943, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4928, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4889, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4861, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4844, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4818, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4800, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4779, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4757, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4740, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4718, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4700, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4681, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4660, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4641, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4624, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4603, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4586, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4567, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4549, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4536, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4529, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4535, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4553, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4550, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4533, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4517, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4486, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4447, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4432, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4399, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4395, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4361, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4350, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4324, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4310, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4287, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4273, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4251, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4235, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4216, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4199, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4182, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4165, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4149, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4131, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4114, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4099, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4082, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4066, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4051, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4034, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3988, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3973, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3960, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3949, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3942, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3925, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3909, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model to set to\n",
    "auto_model = ConvAutoEncoder(conv_model, deconv_model)\n",
    "auto_optimizer = torch.optim.Adam(auto_model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Training params\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#print(train_x[0])\n",
    "for i in range(180):\n",
    "    #print(train_x.shape)\n",
    "    auto_optimizer.zero_grad()\n",
    "    outputs = auto_model(full_x)\n",
    "    #print(outputs.shape)\n",
    "    losses = loss_function(outputs, full_x)\n",
    "    losses.backward(retain_graph=True)\n",
    "    auto_optimizer.step()\n",
    "    print(losses)\n",
    "    if losses < .001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(auto_model.state_dict(), 'model/autoencoder.pt')\n",
    "\n",
    "conv_embedder = auto_model.make_encoder()\n",
    "\n",
    "torch.save(conv_embedder.state_dict(), \"model/embedder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 1: ResNet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv_0): Conv1d(8, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_0): ReLU()\n",
      "  (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_0): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_0): ReLU()\n",
      "  (conv_1): Conv1d(16, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_1): ReLU()\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_1): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_1): ReLU()\n",
      "  (conv_2): Conv1d(32, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_2): ReLU()\n",
      "  (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_2): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_2): ReLU()\n",
      "  (conv_3): Conv1d(64, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_3): ReLU()\n",
      "  (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_3): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_3): ReLU()\n",
      "  (conv_4): Conv1d(128, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_4): ReLU()\n",
      "  (batch_norm_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_4): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_4): ReLU()\n",
      "  (conv_fin): Conv1d(256, 8, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_fin): ReLU()\n",
      "  (batch_fin): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "torch.Size([6, 8, 2500])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "# build resent model and display the shape of feed through\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(5):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = 249, padding = 124, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = 249, padding = 124))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "    \n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = 249, padding = 124))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "print(conv_model)\n",
    "print(conv_model(train_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 2 - LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 250\n",
    "embedding_dim = 8\n",
    "num_words = len(dict_words)\n",
    "\n",
    "class LSTM_EncoderDecoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim, word_list_length):\n",
    "        super(ECG_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim, num_layers = 4, bidirectional = True)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq_embedded = seq.view(len(seq), -1, embedding_dim)\n",
    "        final_hidd, _ = self.lstm(seq_embedded)\n",
    "        dec_seq = self.linear(final_hidd)\n",
    "        return F.log_softmax(dec_seq, dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 3 - Basic Transformer Architecture with Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ECGTransformerEncoder(nn.Module):\n",
    "    # Takes the ECG discrete signals sequence and maps into a probability distribution of diagnosis\n",
    "    # For working/verification purposes\n",
    "    def __init__(self, vector_size, embed_dim, n_heads, hidden_linear_dim, n_layers, dropout):\n",
    "        super(ECGTransformerEncoder, self).__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.positional_encoder = PositionalEncoder(embed_dim, dropout)\n",
    "    \n",
    "        #Since our data is already discrete numbers, might need some tweaking for this\n",
    "        self.embedder = conv_embedder\n",
    "                        #64 31              #39        64\n",
    "        \n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(embed_dim, n_heads, hidden_linear_dim, dropout),\n",
    "            n_layers)\n",
    "        \n",
    "        self.n_inputs = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Simple linear decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(768, 17),\n",
    "                        Transpose(17, 2500),\n",
    "                        nn.Linear(2500, 30),\n",
    "                        nn.LogSoftmax()\n",
    "                        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #self.embedder.weight.data.uniform_(-.1, .1)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        #self.decoder.weight.data.uniform_(-.1, .1)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.embedder(x) # * math.sqrt(self.n_inputs)\n",
    "        x = x.squeeze(0)\n",
    "        #x = x.view(2500, 8)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1) \n",
    "        #x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the number of the last batch sample in the data set is smaller than the defined batch_batch size, mismatch problems will occur. You can modify it yourself, for example, just pass in the shape behind, and then enter it through x.szie(0).\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class SignalEmbedder(nn.Module):\n",
    "    # Necessary to convert the signal into \"word\" vectors for transformer processing.\n",
    "    # Currently a simple group and slice method, but will modify later for multi-channel inputs\n",
    "    \n",
    "    def __init__(self, num_slices, size_of_slice):\n",
    "        super(SignalEmbedder, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.size_of_slice = size_of_slice\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[: self.num_slices * self.size_of_slice]\n",
    "        x = x.reshape((self.num_slices, self.size_of_slice))\n",
    "        return x  \n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    # Necessary to store positional data about the input data\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=2500, batch_size = 1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, 1, embed_dim)\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        divisor = torch.exp(torch.arange(0, embed_dim, 2).float() * (- math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pos_encoding[:, 0, 0::2] = torch.sin(position * divisor)\n",
    "        pos_encoding[:, 0, 1::2] = torch.cos(position * divisor)\n",
    "        pos_encoding = pos_encoding.repeat(1, batch_size, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoding[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(features, features * expansion)\n",
    "        self.linear_2 = nn.Linear(features * expansion, features)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        #self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.norm_1(x + res)\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "class FNETLayer(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FNETLayer, self).__init__()\n",
    "        self.feed_forward = FeedForwardNet(features, expansion, dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = fft.fftn(x, dim = (-2, -1)).real\n",
    "        x = self.norm_1(x + res)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "    \n",
    "class FNETEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self, features, expansion=2, dropout=0.5, num_layers=6):\n",
    "        encoder_layer = FNETLayer(features, expansion, dropout)\n",
    "        super().__init__(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    " \n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x.transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 1 - Huggingface GPT2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.8.crossattention.c_attn.weight', 'h.7.crossattention.bias', 'h.6.crossattention.masked_bias', 'h.4.crossattention.c_proj.bias', 'h.0.crossattention.bias', 'h.0.crossattention.masked_bias', 'h.3.crossattention.bias', 'h.5.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.7.ln_cross_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.3.ln_cross_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.9.ln_cross_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.0.ln_cross_attn.weight', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.bias', 'h.1.crossattention.bias', 'h.8.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.8.crossattention.masked_bias', 'h.10.crossattention.c_proj.weight', 'h.5.ln_cross_attn.weight', 'h.9.crossattention.masked_bias', 'h.6.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.2.ln_cross_attn.weight', 'h.11.ln_cross_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.8.crossattention.bias', 'h.3.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.4.crossattention.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.weight', 'h.10.crossattention.bias', 'h.7.crossattention.masked_bias', 'h.6.ln_cross_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.masked_bias', 'h.2.crossattention.c_proj.bias', 'h.9.crossattention.bias', 'h.8.ln_cross_attn.weight', 'h.10.crossattention.masked_bias', 'h.8.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.5.crossattention.q_attn.weight', 'h.2.crossattention.bias', 'h.2.crossattention.masked_bias', 'h.8.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.1.ln_cross_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.1.crossattention.masked_bias', 'h.4.crossattention.masked_bias', 'h.5.crossattention.bias', 'h.11.crossattention.masked_bias', 'h.3.crossattention.masked_bias', 'h.6.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.6.crossattention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "    token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "    # only last token for inputs_ids if past is defined in kwargs\n",
    "    if past:\n",
    "        input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "    attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "    position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "    if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        if past:\n",
    "            position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "    else:\n",
    "        position_ids = None\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"past_key_values\": past,\n",
    "        \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "        \"encoder_hidden_states\": kwargs.get(\"encoder_hidden_states\", None),\n",
    "        \"position_ids\": position_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "    }\n",
    "\n",
    "# define tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', config = GPT2Config(add_cross_attention = True, is_encoder_decoder = True))\n",
    "model.prepare_inputs_for_generation = prepare_inputs_for_generation.__get__(model, GPT2LMHeadModel)\n",
    "\n",
    "# preprocess training labels and tokenize\n",
    "train_labels = list(waveform_lead_rhythm_diag['diagnosis'])\n",
    "inputs = tokenizer(train_labels, padding = True, verbose = False, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0579, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5628, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0102, grad_fn=<NllLossBackward>)\n",
      "tensor(2.9243, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1574, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6910, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4633, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3435, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1420, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9907, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8403, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7053, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5677, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4541, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3781, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3731, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2472, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1931, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-29755b92caad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    963\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m         \u001b[0mlm_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mformat_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;34m\"\"\"Shorthand for 'format_list(extract_stack(f, limit))'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pretrain decoder\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels = inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "torch.save(model.state_dict(), 'model/gpt2.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder - FNET Encoder Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-df700713c78a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#enc_dec_model((full_x, inputs))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0menc_dec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-df700713c78a>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_enb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreturn_enc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    957\u001b[0m         )\n\u001b[0;32m    958\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m             \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# create encoder decoder model with GPT2 \n",
    "class CustEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder):\n",
    "        super(CustEncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pos_enb = PositionalEncoding(d_model = 768)\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ecgs, labels = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(**labels, labels = labels[\"input_ids\"], encoder_hidden_states = x.contiguous())\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ecgs = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(input_ids = torch.tensor(self.decoder.config.bos_token_id), encoder_hidden_states = x.contiguous())\n",
    "    \n",
    "    def return_enc(self):\n",
    "        return self.encoder\n",
    "\n",
    "# define component models\n",
    "conv_embedder = nn.Sequential(nn.Conv1d(in_channels = 8, out_channels = 350, kernel_size = 15, padding = 7, stride = 1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm1d(350),\n",
    "                              nn.Conv1d(in_channels = 350, out_channels = 768, kernel_size = 15, padding = 7, stride = 1),\n",
    "                              nn.ReLU())\n",
    "model.load_state_dict(torch.load('model/gpt2.pt'))\n",
    "encoder = FNETEncoder(768, expansion = 2, dropout=0.1, num_layers = 6)\n",
    "enc_dec_model = CustEncoderDecoder(encoder, model, conv_embedder)\n",
    "\n",
    "# define all x, feed through to see if all is good\n",
    "full_x = torch.tensor(list(waveform_lead_rhythm_diag['decoded_waveform'])).float()\n",
    "#enc_dec_model((full_x, inputs))\n",
    "\n",
    "enc_dec_model.predict(full_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-38e2297584be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_dec_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train encoder decoder model!\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-5)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = enc_dec_model((full_x, inputs))\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "torch.save(enc_dec_model.state_dict(), 'model/gpt2_enc_dec.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustEncoderDecoder:\n\tMissing key(s) in state_dict: \"encoder.layers.0.feed_forward.linear_1.weight\", \"encoder.layers.0.feed_forward.linear_1.bias\", \"encoder.layers.0.feed_forward.linear_2.weight\", \"encoder.layers.0.feed_forward.linear_2.bias\", \"encoder.layers.0.feed_forward.norm_1.weight\", \"encoder.layers.0.feed_forward.norm_1.bias\", \"encoder.layers.0.norm_1.weight\", \"encoder.layers.0.norm_1.bias\", \"encoder.layers.1.feed_forward.linear_1.weight\", \"encoder.layers.1.feed_forward.linear_1.bias\", \"encoder.layers.1.feed_forward.linear_2.weight\", \"encoder.layers.1.feed_forward.linear_2.bias\", \"encoder.layers.1.feed_forward.norm_1.weight\", \"encoder.layers.1.feed_forward.norm_1.bias\", \"encoder.layers.1.norm_1.weight\", \"encoder.layers.1.norm_1.bias\", \"encoder.layers.2.feed_forward.linear_1.weight\", \"encoder.layers.2.feed_forward.linear_1.bias\", \"encoder.layers.2.feed_forward.linear_2.weight\", \"encoder.layers.2.feed_forward.linear_2.bias\", \"encoder.layers.2.feed_forward.norm_1.weight\", \"encoder.layers.2.feed_forward.norm_1.bias\", \"encoder.layers.2.norm_1.weight\", \"encoder.layers.2.norm_1.bias\", \"encoder.layers.3.feed_forward.linear_1.weight\", \"encoder.layers.3.feed_forward.linear_1.bias\", \"encoder.layers.3.feed_forward.linear_2.weight\", \"encoder.layers.3.feed_forward.linear_2.bias\", \"encoder.layers.3.feed_forward.norm_1.weight\", \"encoder.layers.3.feed_forward.norm_1.bias\", \"encoder.layers.3.norm_1.weight\", \"encoder.layers.3.norm_1.bias\", \"encoder.layers.4.feed_forward.linear_1.weight\", \"encoder.layers.4.feed_forward.linear_1.bias\", \"encoder.layers.4.feed_forward.linear_2.weight\", \"encoder.layers.4.feed_forward.linear_2.bias\", \"encoder.layers.4.feed_forward.norm_1.weight\", \"encoder.layers.4.feed_forward.norm_1.bias\", \"encoder.layers.4.norm_1.weight\", \"encoder.layers.4.norm_1.bias\", \"encoder.layers.5.feed_forward.linear_1.weight\", \"encoder.layers.5.feed_forward.linear_1.bias\", \"encoder.layers.5.feed_forward.linear_2.weight\", \"encoder.layers.5.feed_forward.linear_2.bias\", \"encoder.layers.5.feed_forward.norm_1.weight\", \"encoder.layers.5.feed_forward.norm_1.bias\", \"encoder.layers.5.norm_1.weight\", \"encoder.layers.5.norm_1.bias\", \"decoder.transformer.wte.weight\", \"decoder.transformer.wpe.weight\", \"decoder.transformer.h.0.ln_1.weight\", \"decoder.transformer.h.0.ln_1.bias\", \"decoder.transformer.h.0.attn.bias\", \"decoder.transformer.h.0.attn.masked_bias\", \"decoder.transformer.h.0.attn.c_attn.weight\", \"decoder.transformer.h.0.attn.c_attn.bias\", \"decoder.transformer.h.0.attn.c_proj.weight\", \"decoder.transformer.h.0.attn.c_proj.bias\", \"decoder.transformer.h.0.ln_2.weight\", \"decoder.transformer.h.0.ln_2.bias\", \"decoder.transformer.h.0.crossattention.bias\", \"decoder.transformer.h.0.crossattention.masked_bias\", \"decoder.transformer.h.0.crossattention.c_attn.weight\", \"decoder.transformer.h.0.crossattention.c_attn.bias\", \"decoder.transformer.h.0.crossattention.q_attn.weight\", \"decoder.transformer.h.0.crossattention.q_attn.bias\", \"decoder.transformer.h.0.crossattention.c_proj.weight\", \"decoder.transformer.h.0.crossattention.c_proj.bias\", \"decoder.transformer.h.0.ln_cross_attn.weight\", \"decoder.transformer.h.0.ln_cross_attn.bias\", \"decoder.transformer.h.0.mlp.c_fc.weight\", \"decoder.transformer.h.0.mlp.c_fc.bias\", \"decoder.transformer.h.0.mlp.c_proj.weight\", \"decoder.transformer.h.0.mlp.c_proj.bias\", \"decoder.transformer.h.1.ln_1.weight\", \"decoder.transformer.h.1.ln_1.bias\", \"decoder.transformer.h.1.attn.bias\", \"decoder.transformer.h.1.attn.masked_bias\", \"decoder.transformer.h.1.attn.c_attn.weight\", \"decoder.transformer.h.1.attn.c_attn.bias\", \"decoder.transformer.h.1.attn.c_proj.weight\", \"decoder.transformer.h.1.attn.c_proj.bias\", \"decoder.transformer.h.1.ln_2.weight\", \"decoder.transformer.h.1.ln_2.bias\", \"decoder.transformer.h.1.crossattention.bias\", \"decoder.transformer.h.1.crossattention.masked_bias\", \"decoder.transformer.h.1.crossattention.c_attn.weight\", \"decoder.transformer.h.1.crossattention.c_attn.bias\", \"decoder.transformer.h.1.crossattention.q_attn.weight\", \"decoder.transformer.h.1.crossattention.q_attn.bias\", \"decoder.transformer.h.1.crossattention.c_proj.weight\", \"decoder.transformer.h.1.crossattention.c_proj.bias\", \"decoder.transformer.h.1.ln_cross_attn.weight\", \"decoder.transformer.h.1.ln_cross_attn.bias\", \"decoder.transformer.h.1.mlp.c_fc.weight\", \"decoder.transformer.h.1.mlp.c_fc.bias\", \"decoder.transformer.h.1.mlp.c_proj.weight\", \"decoder.transformer.h.1.mlp.c_proj.bias\", \"decoder.transformer.h.2.ln_1.weight\", \"decoder.transformer.h.2.ln_1.bias\", \"decoder.transformer.h.2.attn.bias\", \"decoder.transformer.h.2.attn.masked_bias\", \"decoder.transformer.h.2.attn.c_attn.weight\", \"decoder.transformer.h.2.attn.c_attn.bias\", \"decoder.transformer.h.2.attn.c_proj.weight\", \"decoder.transformer.h.2.attn.c_proj.bias\", \"decoder.transformer.h.2.ln_2.weight\", \"decoder.transformer.h.2.ln_2.bias\", \"decoder.transformer.h.2.crossattention.bias\", \"decoder.transformer.h.2.crossattention.masked_bias\", \"decoder.transformer.h.2.crossattention.c_attn.weight\", \"decoder.transformer.h.2.crossattention.c_attn.bias\", \"decoder.transformer.h.2.crossattention.q_attn.weight\", \"decoder.transformer.h.2.crossattention.q_attn.bias\", \"decoder.transformer.h.2.crossattention.c_proj.weight\", \"decoder.transformer.h.2.crossattention.c_proj.bias\", \"decoder.transformer.h.2.ln_cross_attn.weight\", \"decoder.transformer.h.2.ln_cross_attn.bias\", \"decoder.transformer.h.2.mlp.c_fc.weight\", \"decoder.transformer.h.2.mlp.c_fc.bias\", \"decoder.transformer.h.2.mlp.c_proj.weight\", \"decoder.transformer.h.2.mlp.c_proj.bias\", \"decoder.transformer.h.3.ln_1.weight\", \"decoder.transformer.h.3.ln_1.bias\", \"decoder.transformer.h.3.attn.bias\", \"decoder.transformer.h.3.attn.masked_bias\", \"decoder.transformer.h.3.attn.c_attn.weight\", \"decoder.transformer.h.3.attn.c_attn.bias\", \"decoder.transformer.h.3.attn.c_proj.weight\", \"decoder.transformer.h.3.attn.c_proj.bias\", \"decoder.transformer.h.3.ln_2.weight\", \"decoder.transformer.h.3.ln_2.bias\", \"decoder.transformer.h.3.crossattention.bias\", \"decoder.transformer.h.3.crossattention.masked_bias\", \"decoder.transformer.h.3.crossattention.c_attn.weight\", \"decoder.transformer.h.3.crossattention.c_attn.bias\", \"decoder.transformer.h.3.crossattention.q_attn.weight\", \"decoder.transformer.h.3.crossattention.q_attn.bias\", \"decoder.transformer.h.3.crossattention.c_proj.weight\", \"decoder.transformer.h.3.crossattention.c_proj.bias\", \"decoder.transformer.h.3.ln_cross_attn.weight\", \"decoder.transformer.h.3.ln_cross_attn.bias\", \"decoder.transformer.h.3.mlp.c_fc.weight\", \"decoder.transformer.h.3.mlp.c_fc.bias\", \"decoder.transformer.h.3.mlp.c_proj.weight\", \"decoder.transformer.h.3.mlp.c_proj.bias\", \"decoder.transformer.h.4.ln_1.weight\", \"decoder.transformer.h.4.ln_1.bias\", \"decoder.transformer.h.4.attn.bias\", \"decoder.transformer.h.4.attn.masked_bias\", \"decoder.transformer.h.4.attn.c_attn.weight\", \"decoder.transformer.h.4.attn.c_attn.bias\", \"decoder.transformer.h.4.attn.c_proj.weight\", \"decoder.transformer.h.4.attn.c_proj.bias\", \"decoder.transformer.h.4.ln_2.weight\", \"decoder.transformer.h.4.ln_2.bias\", \"decoder.transformer.h.4.crossattention.bias\", \"decoder.transformer.h.4.crossattention.masked_bias\", \"decoder.transformer.h.4.crossattention.c_attn.weight\", \"decoder.transformer.h.4.crossattention.c_attn.bias\", \"decoder.transformer.h.4.crossattention.q_attn.weight\", \"decoder.transformer.h.4.crossattention.q_attn.bias\", \"decoder.transformer.h.4.crossattention.c_proj.weight\", \"decoder.transformer.h.4.crossattention.c_proj.bias\", \"decoder.transformer.h.4.ln_cross_attn.weight\", \"decoder.transformer.h.4.ln_cross_attn.bias\", \"decoder.transformer.h.4.mlp.c_fc.weight\", \"decoder.transformer.h.4.mlp.c_fc.bias\", \"decoder.transformer.h.4.mlp.c_proj.weight\", \"decoder.transformer.h.4.mlp.c_proj.bias\", \"decoder.transformer.h.5.ln_1.weight\", \"decoder.transformer.h.5.ln_1.bias\", \"decoder.transformer.h.5.attn.bias\", \"decoder.transformer.h.5.attn.masked_bias\", \"decoder.transformer.h.5.attn.c_attn.weight\", \"decoder.transformer.h.5.attn.c_attn.bias\", \"decoder.transformer.h.5.attn.c_proj.weight\", \"decoder.transformer.h.5.attn.c_proj.bias\", \"decoder.transformer.h.5.ln_2.weight\", \"decoder.transformer.h.5.ln_2.bias\", \"decoder.transformer.h.5.crossattention.bias\", \"decoder.transformer.h.5.crossattention.masked_bias\", \"decoder.transformer.h.5.crossattention.c_attn.weight\", \"decoder.transformer.h.5.crossattention.c_attn.bias\", \"decoder.transformer.h.5.crossattention.q_attn.weight\", \"decoder.transformer.h.5.crossattention.q_attn.bias\", \"decoder.transformer.h.5.crossattention.c_proj.weight\", \"decoder.transformer.h.5.crossattention.c_proj.bias\", \"decoder.transformer.h.5.ln_cross_attn.weight\", \"decoder.transformer.h.5.ln_cross_attn.bias\", \"decoder.transformer.h.5.mlp.c_fc.weight\", \"decoder.transformer.h.5.mlp.c_fc.bias\", \"decoder.transformer.h.5.mlp.c_proj.weight\", \"decoder.transformer.h.5.mlp.c_proj.bias\", \"decoder.transformer.h.6.ln_1.weight\", \"decoder.transformer.h.6.ln_1.bias\", \"decoder.transformer.h.6.attn.bias\", \"decoder.transformer.h.6.attn.masked_bias\", \"decoder.transformer.h.6.attn.c_attn.weight\", \"decoder.transformer.h.6.attn.c_attn.bias\", \"decoder.transformer.h.6.attn.c_proj.weight\", \"decoder.transformer.h.6.attn.c_proj.bias\", \"decoder.transformer.h.6.ln_2.weight\", \"decoder.transformer.h.6.ln_2.bias\", \"decoder.transformer.h.6.crossattention.bias\", \"decoder.transformer.h.6.crossattention.masked_bias\", \"decoder.transformer.h.6.crossattention.c_attn.weight\", \"decoder.transformer.h.6.crossattention.c_attn.bias\", \"decoder.transformer.h.6.crossattention.q_attn.weight\", \"decoder.transformer.h.6.crossattention.q_attn.bias\", \"decoder.transformer.h.6.crossattention.c_proj.weight\", \"decoder.transformer.h.6.crossattention.c_proj.bias\", \"decoder.transformer.h.6.ln_cross_attn.weight\", \"decoder.transformer.h.6.ln_cross_attn.bias\", \"decoder.transformer.h.6.mlp.c_fc.weight\", \"decoder.transformer.h.6.mlp.c_fc.bias\", \"decoder.transformer.h.6.mlp.c_proj.weight\", \"decoder.transformer.h.6.mlp.c_proj.bias\", \"decoder.transformer.h.7.ln_1.weight\", \"decoder.transformer.h.7.ln_1.bias\", \"decoder.transformer.h.7.attn.bias\", \"decoder.transformer.h.7.attn.masked_bias\", \"decoder.transformer.h.7.attn.c_attn.weight\", \"decoder.transformer.h.7.attn.c_attn.bias\", \"decoder.transformer.h.7.attn.c_proj.weight\", \"decoder.transformer.h.7.attn.c_proj.bias\", \"decoder.transformer.h.7.ln_2.weight\", \"decoder.transformer.h.7.ln_2.bias\", \"decoder.transformer.h.7.crossattention.bias\", \"decoder.transformer.h.7.crossattention.masked_bias\", \"decoder.transformer.h.7.crossattention.c_attn.weight\", \"decoder.transformer.h.7.crossattention.c_attn.bias\", \"decoder.transformer.h.7.crossattention.q_attn.weight\", \"decoder.transformer.h.7.crossattention.q_attn.bias\", \"decoder.transformer.h.7.crossattention.c_proj.weight\", \"decoder.transformer.h.7.crossattention.c_proj.bias\", \"decoder.transformer.h.7.ln_cross_attn.weight\", \"decoder.transformer.h.7.ln_cross_attn.bias\", \"decoder.transformer.h.7.mlp.c_fc.weight\", \"decoder.transformer.h.7.mlp.c_fc.bias\", \"decoder.transformer.h.7.mlp.c_proj.weight\", \"decoder.transformer.h.7.mlp.c_proj.bias\", \"decoder.transformer.h.8.ln_1.weight\", \"decoder.transformer.h.8.ln_1.bias\", \"decoder.transformer.h.8.attn.bias\", \"decoder.transformer.h.8.attn.masked_bias\", \"decoder.transformer.h.8.attn.c_attn.weight\", \"decoder.transformer.h.8.attn.c_attn.bias\", \"decoder.transformer.h.8.attn.c_proj.weight\", \"decoder.transformer.h.8.attn.c_proj.bias\", \"decoder.transformer.h.8.ln_2.weight\", \"decoder.transformer.h.8.ln_2.bias\", \"decoder.transformer.h.8.crossattention.bias\", \"decoder.transformer.h.8.crossattention.masked_bias\", \"decoder.transformer.h.8.crossattention.c_attn.weight\", \"decoder.transformer.h.8.crossattention.c_attn.bias\", \"decoder.transformer.h.8.crossattention.q_attn.weight\", \"decoder.transformer.h.8.crossattention.q_attn.bias\", \"decoder.transformer.h.8.crossattention.c_proj.weight\", \"decoder.transformer.h.8.crossattention.c_proj.bias\", \"decoder.transformer.h.8.ln_cross_attn.weight\", \"decoder.transformer.h.8.ln_cross_attn.bias\", \"decoder.transformer.h.8.mlp.c_fc.weight\", \"decoder.transformer.h.8.mlp.c_fc.bias\", \"decoder.transformer.h.8.mlp.c_proj.weight\", \"decoder.transformer.h.8.mlp.c_proj.bias\", \"decoder.transformer.h.9.ln_1.weight\", \"decoder.transformer.h.9.ln_1.bias\", \"decoder.transformer.h.9.attn.bias\", \"decoder.transformer.h.9.attn.masked_bias\", \"decoder.transformer.h.9.attn.c_attn.weight\", \"decoder.transformer.h.9.attn.c_attn.bias\", \"decoder.transformer.h.9.attn.c_proj.weight\", \"decoder.transformer.h.9.attn.c_proj.bias\", \"decoder.transformer.h.9.ln_2.weight\", \"decoder.transformer.h.9.ln_2.bias\", \"decoder.transformer.h.9.crossattention.bias\", \"decoder.transformer.h.9.crossattention.masked_bias\", \"decoder.transformer.h.9.crossattention.c_attn.weight\", \"decoder.transformer.h.9.crossattention.c_attn.bias\", \"decoder.transformer.h.9.crossattention.q_attn.weight\", \"decoder.transformer.h.9.crossattention.q_attn.bias\", \"decoder.transformer.h.9.crossattention.c_proj.weight\", \"decoder.transformer.h.9.crossattention.c_proj.bias\", \"decoder.transformer.h.9.ln_cross_attn.weight\", \"decoder.transformer.h.9.ln_cross_attn.bias\", \"decoder.transformer.h.9.mlp.c_fc.weight\", \"decoder.transformer.h.9.mlp.c_fc.bias\", \"decoder.transformer.h.9.mlp.c_proj.weight\", \"decoder.transformer.h.9.mlp.c_proj.bias\", \"decoder.transformer.h.10.ln_1.weight\", \"decoder.transformer.h.10.ln_1.bias\", \"decoder.transformer.h.10.attn.bias\", \"decoder.transformer.h.10.attn.masked_bias\", \"decoder.transformer.h.10.attn.c_attn.weight\", \"decoder.transformer.h.10.attn.c_attn.bias\", \"decoder.transformer.h.10.attn.c_proj.weight\", \"decoder.transformer.h.10.attn.c_proj.bias\", \"decoder.transformer.h.10.ln_2.weight\", \"decoder.transformer.h.10.ln_2.bias\", \"decoder.transformer.h.10.crossattention.bias\", \"decoder.transformer.h.10.crossattention.masked_bias\", \"decoder.transformer.h.10.crossattention.c_attn.weight\", \"decoder.transformer.h.10.crossattention.c_attn.bias\", \"decoder.transformer.h.10.crossattention.q_attn.weight\", \"decoder.transformer.h.10.crossattention.q_attn.bias\", \"decoder.transformer.h.10.crossattention.c_proj.weight\", \"decoder.transformer.h.10.crossattention.c_proj.bias\", \"decoder.transformer.h.10.ln_cross_attn.weight\", \"decoder.transformer.h.10.ln_cross_attn.bias\", \"decoder.transformer.h.10.mlp.c_fc.weight\", \"decoder.transformer.h.10.mlp.c_fc.bias\", \"decoder.transformer.h.10.mlp.c_proj.weight\", \"decoder.transformer.h.10.mlp.c_proj.bias\", \"decoder.transformer.h.11.ln_1.weight\", \"decoder.transformer.h.11.ln_1.bias\", \"decoder.transformer.h.11.attn.bias\", \"decoder.transformer.h.11.attn.masked_bias\", \"decoder.transformer.h.11.attn.c_attn.weight\", \"decoder.transformer.h.11.attn.c_attn.bias\", \"decoder.transformer.h.11.attn.c_proj.weight\", \"decoder.transformer.h.11.attn.c_proj.bias\", \"decoder.transformer.h.11.ln_2.weight\", \"decoder.transformer.h.11.ln_2.bias\", \"decoder.transformer.h.11.crossattention.bias\", \"decoder.transformer.h.11.crossattention.masked_bias\", \"decoder.transformer.h.11.crossattention.c_attn.weight\", \"decoder.transformer.h.11.crossattention.c_attn.bias\", \"decoder.transformer.h.11.crossattention.q_attn.weight\", \"decoder.transformer.h.11.crossattention.q_attn.bias\", \"decoder.transformer.h.11.crossattention.c_proj.weight\", \"decoder.transformer.h.11.crossattention.c_proj.bias\", \"decoder.transformer.h.11.ln_cross_attn.weight\", \"decoder.transformer.h.11.ln_cross_attn.bias\", \"decoder.transformer.h.11.mlp.c_fc.weight\", \"decoder.transformer.h.11.mlp.c_fc.bias\", \"decoder.transformer.h.11.mlp.c_proj.weight\", \"decoder.transformer.h.11.mlp.c_proj.bias\", \"decoder.transformer.ln_f.weight\", \"decoder.transformer.ln_f.bias\", \"decoder.lm_head.weight\", \"pos_enb.pe\", \"embedder.0.weight\", \"embedder.0.bias\", \"embedder.2.weight\", \"embedder.2.bias\", \"embedder.2.running_mean\", \"embedder.2.running_var\", \"embedder.3.weight\", \"embedder.3.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.wte.weight\", \"transformer.wpe.weight\", \"transformer.h.0.ln_1.weight\", \"transformer.h.0.ln_1.bias\", \"transformer.h.0.attn.bias\", \"transformer.h.0.attn.masked_bias\", \"transformer.h.0.attn.c_attn.weight\", \"transformer.h.0.attn.c_attn.bias\", \"transformer.h.0.attn.c_proj.weight\", \"transformer.h.0.attn.c_proj.bias\", \"transformer.h.0.ln_2.weight\", \"transformer.h.0.ln_2.bias\", \"transformer.h.0.crossattention.bias\", \"transformer.h.0.crossattention.masked_bias\", \"transformer.h.0.crossattention.c_attn.weight\", \"transformer.h.0.crossattention.c_attn.bias\", \"transformer.h.0.crossattention.q_attn.weight\", \"transformer.h.0.crossattention.q_attn.bias\", \"transformer.h.0.crossattention.c_proj.weight\", \"transformer.h.0.crossattention.c_proj.bias\", \"transformer.h.0.ln_cross_attn.weight\", \"transformer.h.0.ln_cross_attn.bias\", \"transformer.h.0.mlp.c_fc.weight\", \"transformer.h.0.mlp.c_fc.bias\", \"transformer.h.0.mlp.c_proj.weight\", \"transformer.h.0.mlp.c_proj.bias\", \"transformer.h.1.ln_1.weight\", \"transformer.h.1.ln_1.bias\", \"transformer.h.1.attn.bias\", \"transformer.h.1.attn.masked_bias\", \"transformer.h.1.attn.c_attn.weight\", \"transformer.h.1.attn.c_attn.bias\", \"transformer.h.1.attn.c_proj.weight\", \"transformer.h.1.attn.c_proj.bias\", \"transformer.h.1.ln_2.weight\", \"transformer.h.1.ln_2.bias\", \"transformer.h.1.crossattention.bias\", \"transformer.h.1.crossattention.masked_bias\", \"transformer.h.1.crossattention.c_attn.weight\", \"transformer.h.1.crossattention.c_attn.bias\", \"transformer.h.1.crossattention.q_attn.weight\", \"transformer.h.1.crossattention.q_attn.bias\", \"transformer.h.1.crossattention.c_proj.weight\", \"transformer.h.1.crossattention.c_proj.bias\", \"transformer.h.1.ln_cross_attn.weight\", \"transformer.h.1.ln_cross_attn.bias\", \"transformer.h.1.mlp.c_fc.weight\", \"transformer.h.1.mlp.c_fc.bias\", \"transformer.h.1.mlp.c_proj.weight\", \"transformer.h.1.mlp.c_proj.bias\", \"transformer.h.2.ln_1.weight\", \"transformer.h.2.ln_1.bias\", \"transformer.h.2.attn.bias\", \"transformer.h.2.attn.masked_bias\", \"transformer.h.2.attn.c_attn.weight\", \"transformer.h.2.attn.c_attn.bias\", \"transformer.h.2.attn.c_proj.weight\", \"transformer.h.2.attn.c_proj.bias\", \"transformer.h.2.ln_2.weight\", \"transformer.h.2.ln_2.bias\", \"transformer.h.2.crossattention.bias\", \"transformer.h.2.crossattention.masked_bias\", \"transformer.h.2.crossattention.c_attn.weight\", \"transformer.h.2.crossattention.c_attn.bias\", \"transformer.h.2.crossattention.q_attn.weight\", \"transformer.h.2.crossattention.q_attn.bias\", \"transformer.h.2.crossattention.c_proj.weight\", \"transformer.h.2.crossattention.c_proj.bias\", \"transformer.h.2.ln_cross_attn.weight\", \"transformer.h.2.ln_cross_attn.bias\", \"transformer.h.2.mlp.c_fc.weight\", \"transformer.h.2.mlp.c_fc.bias\", \"transformer.h.2.mlp.c_proj.weight\", \"transformer.h.2.mlp.c_proj.bias\", \"transformer.h.3.ln_1.weight\", \"transformer.h.3.ln_1.bias\", \"transformer.h.3.attn.bias\", \"transformer.h.3.attn.masked_bias\", \"transformer.h.3.attn.c_attn.weight\", \"transformer.h.3.attn.c_attn.bias\", \"transformer.h.3.attn.c_proj.weight\", \"transformer.h.3.attn.c_proj.bias\", \"transformer.h.3.ln_2.weight\", \"transformer.h.3.ln_2.bias\", \"transformer.h.3.crossattention.bias\", \"transformer.h.3.crossattention.masked_bias\", \"transformer.h.3.crossattention.c_attn.weight\", \"transformer.h.3.crossattention.c_attn.bias\", \"transformer.h.3.crossattention.q_attn.weight\", \"transformer.h.3.crossattention.q_attn.bias\", \"transformer.h.3.crossattention.c_proj.weight\", \"transformer.h.3.crossattention.c_proj.bias\", \"transformer.h.3.ln_cross_attn.weight\", \"transformer.h.3.ln_cross_attn.bias\", \"transformer.h.3.mlp.c_fc.weight\", \"transformer.h.3.mlp.c_fc.bias\", \"transformer.h.3.mlp.c_proj.weight\", \"transformer.h.3.mlp.c_proj.bias\", \"transformer.h.4.ln_1.weight\", \"transformer.h.4.ln_1.bias\", \"transformer.h.4.attn.bias\", \"transformer.h.4.attn.masked_bias\", \"transformer.h.4.attn.c_attn.weight\", \"transformer.h.4.attn.c_attn.bias\", \"transformer.h.4.attn.c_proj.weight\", \"transformer.h.4.attn.c_proj.bias\", \"transformer.h.4.ln_2.weight\", \"transformer.h.4.ln_2.bias\", \"transformer.h.4.crossattention.bias\", \"transformer.h.4.crossattention.masked_bias\", \"transformer.h.4.crossattention.c_attn.weight\", \"transformer.h.4.crossattention.c_attn.bias\", \"transformer.h.4.crossattention.q_attn.weight\", \"transformer.h.4.crossattention.q_attn.bias\", \"transformer.h.4.crossattention.c_proj.weight\", \"transformer.h.4.crossattention.c_proj.bias\", \"transformer.h.4.ln_cross_attn.weight\", \"transformer.h.4.ln_cross_attn.bias\", \"transformer.h.4.mlp.c_fc.weight\", \"transformer.h.4.mlp.c_fc.bias\", \"transformer.h.4.mlp.c_proj.weight\", \"transformer.h.4.mlp.c_proj.bias\", \"transformer.h.5.ln_1.weight\", \"transformer.h.5.ln_1.bias\", \"transformer.h.5.attn.bias\", \"transformer.h.5.attn.masked_bias\", \"transformer.h.5.attn.c_attn.weight\", \"transformer.h.5.attn.c_attn.bias\", \"transformer.h.5.attn.c_proj.weight\", \"transformer.h.5.attn.c_proj.bias\", \"transformer.h.5.ln_2.weight\", \"transformer.h.5.ln_2.bias\", \"transformer.h.5.crossattention.bias\", \"transformer.h.5.crossattention.masked_bias\", \"transformer.h.5.crossattention.c_attn.weight\", \"transformer.h.5.crossattention.c_attn.bias\", \"transformer.h.5.crossattention.q_attn.weight\", \"transformer.h.5.crossattention.q_attn.bias\", \"transformer.h.5.crossattention.c_proj.weight\", \"transformer.h.5.crossattention.c_proj.bias\", \"transformer.h.5.ln_cross_attn.weight\", \"transformer.h.5.ln_cross_attn.bias\", \"transformer.h.5.mlp.c_fc.weight\", \"transformer.h.5.mlp.c_fc.bias\", \"transformer.h.5.mlp.c_proj.weight\", \"transformer.h.5.mlp.c_proj.bias\", \"transformer.h.6.ln_1.weight\", \"transformer.h.6.ln_1.bias\", \"transformer.h.6.attn.bias\", \"transformer.h.6.attn.masked_bias\", \"transformer.h.6.attn.c_attn.weight\", \"transformer.h.6.attn.c_attn.bias\", \"transformer.h.6.attn.c_proj.weight\", \"transformer.h.6.attn.c_proj.bias\", \"transformer.h.6.ln_2.weight\", \"transformer.h.6.ln_2.bias\", \"transformer.h.6.crossattention.bias\", \"transformer.h.6.crossattention.masked_bias\", \"transformer.h.6.crossattention.c_attn.weight\", \"transformer.h.6.crossattention.c_attn.bias\", \"transformer.h.6.crossattention.q_attn.weight\", \"transformer.h.6.crossattention.q_attn.bias\", \"transformer.h.6.crossattention.c_proj.weight\", \"transformer.h.6.crossattention.c_proj.bias\", \"transformer.h.6.ln_cross_attn.weight\", \"transformer.h.6.ln_cross_attn.bias\", \"transformer.h.6.mlp.c_fc.weight\", \"transformer.h.6.mlp.c_fc.bias\", \"transformer.h.6.mlp.c_proj.weight\", \"transformer.h.6.mlp.c_proj.bias\", \"transformer.h.7.ln_1.weight\", \"transformer.h.7.ln_1.bias\", \"transformer.h.7.attn.bias\", \"transformer.h.7.attn.masked_bias\", \"transformer.h.7.attn.c_attn.weight\", \"transformer.h.7.attn.c_attn.bias\", \"transformer.h.7.attn.c_proj.weight\", \"transformer.h.7.attn.c_proj.bias\", \"transformer.h.7.ln_2.weight\", \"transformer.h.7.ln_2.bias\", \"transformer.h.7.crossattention.bias\", \"transformer.h.7.crossattention.masked_bias\", \"transformer.h.7.crossattention.c_attn.weight\", \"transformer.h.7.crossattention.c_attn.bias\", \"transformer.h.7.crossattention.q_attn.weight\", \"transformer.h.7.crossattention.q_attn.bias\", \"transformer.h.7.crossattention.c_proj.weight\", \"transformer.h.7.crossattention.c_proj.bias\", \"transformer.h.7.ln_cross_attn.weight\", \"transformer.h.7.ln_cross_attn.bias\", \"transformer.h.7.mlp.c_fc.weight\", \"transformer.h.7.mlp.c_fc.bias\", \"transformer.h.7.mlp.c_proj.weight\", \"transformer.h.7.mlp.c_proj.bias\", \"transformer.h.8.ln_1.weight\", \"transformer.h.8.ln_1.bias\", \"transformer.h.8.attn.bias\", \"transformer.h.8.attn.masked_bias\", \"transformer.h.8.attn.c_attn.weight\", \"transformer.h.8.attn.c_attn.bias\", \"transformer.h.8.attn.c_proj.weight\", \"transformer.h.8.attn.c_proj.bias\", \"transformer.h.8.ln_2.weight\", \"transformer.h.8.ln_2.bias\", \"transformer.h.8.crossattention.bias\", \"transformer.h.8.crossattention.masked_bias\", \"transformer.h.8.crossattention.c_attn.weight\", \"transformer.h.8.crossattention.c_attn.bias\", \"transformer.h.8.crossattention.q_attn.weight\", \"transformer.h.8.crossattention.q_attn.bias\", \"transformer.h.8.crossattention.c_proj.weight\", \"transformer.h.8.crossattention.c_proj.bias\", \"transformer.h.8.ln_cross_attn.weight\", \"transformer.h.8.ln_cross_attn.bias\", \"transformer.h.8.mlp.c_fc.weight\", \"transformer.h.8.mlp.c_fc.bias\", \"transformer.h.8.mlp.c_proj.weight\", \"transformer.h.8.mlp.c_proj.bias\", \"transformer.h.9.ln_1.weight\", \"transformer.h.9.ln_1.bias\", \"transformer.h.9.attn.bias\", \"transformer.h.9.attn.masked_bias\", \"transformer.h.9.attn.c_attn.weight\", \"transformer.h.9.attn.c_attn.bias\", \"transformer.h.9.attn.c_proj.weight\", \"transformer.h.9.attn.c_proj.bias\", \"transformer.h.9.ln_2.weight\", \"transformer.h.9.ln_2.bias\", \"transformer.h.9.crossattention.bias\", \"transformer.h.9.crossattention.masked_bias\", \"transformer.h.9.crossattention.c_attn.weight\", \"transformer.h.9.crossattention.c_attn.bias\", \"transformer.h.9.crossattention.q_attn.weight\", \"transformer.h.9.crossattention.q_attn.bias\", \"transformer.h.9.crossattention.c_proj.weight\", \"transformer.h.9.crossattention.c_proj.bias\", \"transformer.h.9.ln_cross_attn.weight\", \"transformer.h.9.ln_cross_attn.bias\", \"transformer.h.9.mlp.c_fc.weight\", \"transformer.h.9.mlp.c_fc.bias\", \"transformer.h.9.mlp.c_proj.weight\", \"transformer.h.9.mlp.c_proj.bias\", \"transformer.h.10.ln_1.weight\", \"transformer.h.10.ln_1.bias\", \"transformer.h.10.attn.bias\", \"transformer.h.10.attn.masked_bias\", \"transformer.h.10.attn.c_attn.weight\", \"transformer.h.10.attn.c_attn.bias\", \"transformer.h.10.attn.c_proj.weight\", \"transformer.h.10.attn.c_proj.bias\", \"transformer.h.10.ln_2.weight\", \"transformer.h.10.ln_2.bias\", \"transformer.h.10.crossattention.bias\", \"transformer.h.10.crossattention.masked_bias\", \"transformer.h.10.crossattention.c_attn.weight\", \"transformer.h.10.crossattention.c_attn.bias\", \"transformer.h.10.crossattention.q_attn.weight\", \"transformer.h.10.crossattention.q_attn.bias\", \"transformer.h.10.crossattention.c_proj.weight\", \"transformer.h.10.crossattention.c_proj.bias\", \"transformer.h.10.ln_cross_attn.weight\", \"transformer.h.10.ln_cross_attn.bias\", \"transformer.h.10.mlp.c_fc.weight\", \"transformer.h.10.mlp.c_fc.bias\", \"transformer.h.10.mlp.c_proj.weight\", \"transformer.h.10.mlp.c_proj.bias\", \"transformer.h.11.ln_1.weight\", \"transformer.h.11.ln_1.bias\", \"transformer.h.11.attn.bias\", \"transformer.h.11.attn.masked_bias\", \"transformer.h.11.attn.c_attn.weight\", \"transformer.h.11.attn.c_attn.bias\", \"transformer.h.11.attn.c_proj.weight\", \"transformer.h.11.attn.c_proj.bias\", \"transformer.h.11.ln_2.weight\", \"transformer.h.11.ln_2.bias\", \"transformer.h.11.crossattention.bias\", \"transformer.h.11.crossattention.masked_bias\", \"transformer.h.11.crossattention.c_attn.weight\", \"transformer.h.11.crossattention.c_attn.bias\", \"transformer.h.11.crossattention.q_attn.weight\", \"transformer.h.11.crossattention.q_attn.bias\", \"transformer.h.11.crossattention.c_proj.weight\", \"transformer.h.11.crossattention.c_proj.bias\", \"transformer.h.11.ln_cross_attn.weight\", \"transformer.h.11.ln_cross_attn.bias\", \"transformer.h.11.mlp.c_fc.weight\", \"transformer.h.11.mlp.c_fc.bias\", \"transformer.h.11.mlp.c_proj.weight\", \"transformer.h.11.mlp.c_proj.bias\", \"transformer.ln_f.weight\", \"transformer.ln_f.bias\", \"lm_head.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-b4ddf86144a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menc_dec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/gpt2_enc_dec.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1052\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustEncoderDecoder:\n\tMissing key(s) in state_dict: \"encoder.layers.0.feed_forward.linear_1.weight\", \"encoder.layers.0.feed_forward.linear_1.bias\", \"encoder.layers.0.feed_forward.linear_2.weight\", \"encoder.layers.0.feed_forward.linear_2.bias\", \"encoder.layers.0.feed_forward.norm_1.weight\", \"encoder.layers.0.feed_forward.norm_1.bias\", \"encoder.layers.0.norm_1.weight\", \"encoder.layers.0.norm_1.bias\", \"encoder.layers.1.feed_forward.linear_1.weight\", \"encoder.layers.1.feed_forward.linear_1.bias\", \"encoder.layers.1.feed_forward.linear_2.weight\", \"encoder.layers.1.feed_forward.linear_2.bias\", \"encoder.layers.1.feed_forward.norm_1.weight\", \"encoder.layers.1.feed_forward.norm_1.bias\", \"encoder.layers.1.norm_1.weight\", \"encoder.layers.1.norm_1.bias\", \"encoder.layers.2.feed_forward.linear_1.weight\", \"encoder.layers.2.feed_forward.linear_1.bias\", \"encoder.layers.2.feed_forward.linear_2.weight\", \"encoder.layers.2.feed_forward.linear_2.bias\", \"encoder.layers.2.feed_forward.norm_1.weight\", \"encoder.layers.2.feed_forward.norm_1.bias\", \"encoder.layers.2.norm_1.weight\", \"encoder.layers.2.norm_1.bias\", \"encoder.layers.3.feed_forward.linear_1.weight\", \"encoder.layers.3.feed_forward.linear_1.bias\", \"encoder.layers.3.feed_forward.linear_2.weight\", \"encoder.layers.3.feed_forward.linear_2.bias\", \"encoder.layers.3.feed_forward.norm_1.weight\", \"encoder.layers.3.feed_forward.norm_1.bias\", \"encoder.layers.3.norm_1.weight\", \"encoder.layers.3.norm_1.bias\", \"encoder.layers.4.feed_forward.linear_1.weight\", \"encoder.layers.4.feed_forward.linear_1.bias\", \"encoder.layers.4.feed_forward.linear_2.weight\", \"encoder.layers.4.feed_forward.linear_2.bias\", \"encoder.layers.4.feed_forward.norm_1.weight\", \"encoder.layers.4.feed_forward.norm_1.bias\", \"encoder.layers.4.norm_1.weight\", \"encoder.layers.4.norm_1.bias\", \"encoder.layers.5.feed_forward.linear_1.weight\", \"encoder.layers.5.feed_forward.linear_1.bias\", \"encoder.layers.5.feed_forward.linear_2.weight\", \"encoder.layers.5.feed_forward.linear_2.bias\", \"encoder.layers.5.feed_forward.norm_1.weight\", \"encoder.layers.5.feed_forward.norm_1.bias\", \"encoder.layers.5.norm_1.weight\", \"encoder.layers.5.norm_1.bias\", \"decoder.transformer.wte.weight\", \"decoder.transformer.wpe.weight\", \"decoder.transformer.h.0.ln_1.weight\", \"decoder.transformer.h.0.ln_1.bias\", \"decoder.transformer.h.0.attn.bias\", \"decoder.transformer.h.0.attn.masked_bias\", \"decoder.transformer.h.0.attn.c_attn.weight\", \"decoder.transformer.h.0.attn.c_attn.bias\", \"decoder.transformer.h.0.attn.c_proj.weight\", \"decoder.transformer.h.0.attn.c_proj.bias\", \"decoder.transformer.h.0.ln_2.weight\", \"decoder.transformer.h.0.ln_2.bias\", \"decoder.transformer.h.0.crossattention.bias\", \"decoder.transformer.h.0.crossattention.masked_bias\", \"decoder.transformer.h.0.crossattention.c_attn.weight\", \"decoder.transformer.h.0.crossattention.c_attn.bias\", \"decoder.transformer.h.0.crossattention.q_attn.weight\", \"decoder.transformer.h.0.crossattention.q_attn.bias\", \"decoder.transformer.h.0.crossattention.c_proj.weight\", \"decoder.transformer.h.0.crossattention.c_proj.bias\", \"decoder.transformer.h.0.ln_cross_attn.weight\", \"decoder.transformer.h.0.ln_cross_attn.bias\", \"decoder.transformer.h.0.mlp.c_fc.weight\", \"decoder.transformer.h.0.mlp.c_fc.bias\", \"decoder.transformer.h.0.mlp.c_proj.weight\", \"decoder.transformer.h.0.mlp.c_proj.bias\", \"decoder.transformer.h.1.ln_1.weight\", \"decoder.transformer.h.1.ln_1.bias\", \"decoder.transformer.h.1.attn.bias\", \"decoder.transformer.h.1.attn.masked_bias\", \"decoder.transformer.h.1.attn.c_attn.weight\", \"decoder.transformer.h.1.attn.c_attn.bias\", \"decoder.transformer.h.1.attn.c_proj.weight\", \"decoder.transformer.h.1.attn.c_proj.bias\", \"decoder.transformer.h.1.ln_2.weight\", \"decoder.transformer.h.1.ln_2.bias\", \"decoder.transformer.h.1.crossattention.bias\", \"decoder.transformer.h.1.crossattention.masked_bias\", \"decoder.transformer.h.1.crossattention.c_attn.weight\", \"decoder.transformer.h.1.crossattention.c_attn.bias\", \"decoder.transformer.h.1.crossattention.q_attn.weight\", \"decoder.transformer.h.1.crossattention.q_attn.bias\", \"decoder.transformer.h.1.crossattention.c_proj.weight\", \"decoder.transformer.h.1.crossattention.c_proj.bias\", \"decoder.transformer.h.1.ln_cross_attn.weight\", \"decoder.transformer.h.1.ln_cross_attn.bias\", \"decoder.transformer.h.1.mlp.c_fc.weight\", \"decoder.transformer.h.1.mlp.c_fc.bias\", \"decoder.transformer.h.1.mlp.c_proj.weight\", \"decoder.transformer.h.1.mlp.c_proj.bias\", \"decoder.transformer.h.2.ln_1.weight\", \"decoder.transformer.h.2.ln_1.bias\", \"decoder.transformer.h.2.attn.bias\", \"decoder.transformer.h.2.attn.masked_bias\", \"decoder.transformer.h.2.attn.c_attn.weight\", \"decoder.transformer.h.2.attn.c_attn.bias\", \"decoder.transformer.h.2.attn.c_proj.weight\", \"decoder.transformer.h.2.attn.c_proj.bias\", \"decoder.transformer.h.2.ln_2.weight\", \"decoder.transformer.h.2.ln_2.bias\", \"decoder.transformer.h.2.crossattention.bias\", \"decoder.transformer.h.2.crossattention.masked_bias\", \"decoder.transformer.h.2.crossattention.c_attn.weight\", \"decoder.transformer.h.2.crossattention.c_attn.bias\", \"decoder.transformer.h.2.crossattention.q_attn.weight\", \"decoder.transformer.h.2.crossattention.q_attn.bias\", \"decoder.transformer.h.2.crossattention.c_proj.weight\", \"decoder.transformer.h.2.crossattention.c_proj.bias\", \"decoder.transformer.h.2.ln_cross_attn.weight\", \"decoder.transformer.h.2.ln_cross_attn.bias\", \"decoder.transformer.h.2.mlp.c_fc.weight\", \"decoder.transformer.h.2.mlp.c_fc.bias\", \"decoder.transformer.h.2.mlp.c_proj.weight\", \"decoder.transformer.h.2.mlp.c_proj.bias\", \"decoder.transformer.h.3.ln_1.weight\", \"decoder.transformer.h.3.ln_1.bias\", \"decoder.transformer.h.3.attn.bias\", \"decoder.transformer.h.3.attn.masked_bias\", \"decoder.transformer.h.3.attn.c_attn.weight\", \"decoder.transformer.h.3.attn.c_attn.bias\", \"decoder.transformer.h.3.attn.c_proj.weight\", \"decoder.transformer.h.3.attn.c_proj.bias\", \"decoder.transformer.h.3.ln_2.weight\", \"decoder.transformer.h.3.ln_2.bias\", \"decoder.transformer.h.3.crossattention.bias\", \"decoder.transformer.h.3.crossattention.masked_bias\", \"decoder.transformer.h.3.crossattention.c_attn.weight\", \"decoder.transformer.h.3.crossattention.c_attn.bias\", \"decoder.transformer.h.3.crossattention.q_attn.weight\", \"decoder.transformer.h.3.crossattention.q_attn.bias\", \"decoder.transformer.h.3.crossattention.c_proj.weight\", \"decoder.transformer.h.3.crossattention.c_proj.bias\", \"decoder.transformer.h.3.ln_cross_attn.weight\", \"decoder.transformer.h.3.ln_cross_attn.bias\", \"decoder.transformer.h.3.mlp.c_fc.weight\", \"decoder.transformer.h.3.mlp.c_fc.bias\", \"decoder.transformer.h.3.mlp.c_proj.weight\", \"decoder.transformer.h.3.mlp.c_proj.bias\", \"decoder.transformer.h.4.ln_1.weight\", \"decoder.transformer.h.4.ln_1.bias\", \"decoder.transformer.h.4.attn.bias\", \"decoder.transformer.h.4.attn.masked_bias\", \"decoder.transformer.h.4.attn.c_attn.weight\", \"decoder.transformer.h.4.attn.c_attn.bias\", \"decoder.transformer.h.4.attn.c_proj.weight\", \"decoder.transformer.h.4.attn.c_proj.bias\", \"decoder.transformer.h.4.ln_2.weight\", \"decoder.transformer.h.4.ln_2.bias\", \"decoder.transformer.h.4.crossattention.bias\", \"decoder.transformer.h.4.crossattention.masked_bias\", \"decoder.transformer.h.4.crossattention.c_attn.weight\", \"decoder.transformer.h.4.crossattention.c_attn.bias\", \"decoder.transformer.h.4.crossattention.q_attn.weight\", \"decoder.transformer.h.4.crossattention.q_attn.bias\", \"decoder.transformer.h.4.crossattention.c_proj.weight\", \"decoder.transformer.h.4.crossattention.c_proj.bias\", \"decoder.transformer.h.4.ln_cross_attn.weight\", \"decoder.transformer.h.4.ln_cross_attn.bias\", \"decoder.transformer.h.4.mlp.c_fc.weight\", \"decoder.transformer.h.4.mlp.c_fc.bias\", \"decoder.transformer.h.4.mlp.c_proj.weight\", \"decoder.transformer.h.4.mlp.c_proj.bias\", \"decoder.transformer.h.5.ln_1.weight\", \"decoder.transformer.h.5.ln_1.bias\", \"decoder.transformer.h.5.attn.bias\", \"decoder.transformer.h.5.attn.masked_bias\", \"decoder.transformer.h.5.attn.c_attn.weight\", \"decoder.transformer.h.5.attn.c_attn.bias\", \"decoder.transformer.h.5.attn.c_proj.weight\", \"decoder.transformer.h.5.attn.c_proj.bias\", \"decoder.transformer.h.5.ln_2.weight\", \"decoder.transformer.h.5.ln_2.bias\", \"decoder.transformer.h.5.crossattention.bias\", \"decoder.transformer.h.5.crossattention.masked_bias\", \"decoder.transformer.h.5.crossattention.c_attn.weight\", \"decoder.transformer.h.5.crossattention.c_attn.bias\", \"decoder.transformer.h.5.crossattention.q_attn.weight\", \"decoder.transformer.h.5.crossattention.q_attn.bias\", \"decoder.transformer.h.5.crossattention.c_proj.weight\", \"decoder.transformer.h.5.crossattention.c_proj.bias\", \"decoder.transformer.h.5.ln_cross_attn.weight\", \"decoder.transformer.h.5.ln_cross_attn.bias\", \"decoder.transformer.h.5.mlp.c_fc.weight\", \"decoder.transformer.h.5.mlp.c_fc.bias\", \"decoder.transformer.h.5.mlp.c_proj.weight\", \"decoder.transformer.h.5.mlp.c_proj.bias\", \"decoder.transformer.h.6.ln_1.weight\", \"decoder.transformer.h.6.ln_1.bias\", \"decoder.transformer.h.6.attn.bias\", \"decoder.transformer.h.6.attn.masked_bias\", \"decoder.transformer.h.6.attn.c_attn.weight\", \"decoder.transformer.h.6.attn.c_attn.bias\", \"decoder.transformer.h.6.attn.c_proj.weight\", \"decoder.transformer.h.6.attn.c_proj.bias\", \"decoder.transformer.h.6.ln_2.weight\", \"decoder.transformer.h.6.ln_2.bias\", \"decoder.transformer.h.6.crossattention.bias\", \"decoder.transformer.h.6.crossattention.masked_bias\", \"decoder.transformer.h.6.crossattention.c_attn.weight\", \"decoder.transformer.h.6.crossattention.c_attn.bias\", \"decoder.transformer.h.6.crossattention.q_attn.weight\", \"decoder.transformer.h.6.crossattention.q_attn.bias\", \"decoder.transformer.h.6.crossattention.c_proj.weight\", \"decoder.transformer.h.6.crossattention.c_proj.bias\", \"decoder.transformer.h.6.ln_cross_attn.weight\", \"decoder.transformer.h.6.ln_cross_attn.bias\", \"decoder.transformer.h.6.mlp.c_fc.weight\", \"decoder.transformer.h.6.mlp.c_fc.bias\", \"decoder.transformer.h.6.mlp.c_proj.weight\", \"decoder.transformer.h.6.mlp.c_proj.bias\", \"decoder.transformer.h.7.ln_1.weight\", \"decoder.transformer.h.7.ln_1.bias\", \"decoder.transformer.h.7.attn.bias\", \"decoder.transformer.h.7.attn.masked_bias\", \"decoder.transformer.h.7.attn.c_attn.weight\", \"decoder.transformer.h.7.attn.c_attn.bias\", \"decoder.transformer.h.7.attn.c_proj.weight\", \"decoder.transformer.h.7.attn.c_proj.bias\", \"decoder.transformer.h.7.ln_2.weight\", \"decoder.transformer.h.7.ln_2.bias\", \"decoder.transformer.h.7.crossattention.bias\", \"decoder.transformer.h.7.crossattention.masked_bias\", \"decoder.transformer.h.7.crossattention.c_attn.weight\", \"decoder.transformer.h.7.crossattention.c_attn.bias\", \"decoder.transformer.h.7.crossattention.q_attn.weight\", \"decoder.transformer.h.7.crossattention.q_attn.bias\", \"decoder.transformer.h.7.crossattention.c_proj.weight\", \"decoder.transformer.h.7.crossattention.c_proj.bias\", \"decoder.transformer.h.7.ln_cross_attn.weight\", \"decoder.transformer.h.7.ln_cross_attn.bias\", \"decoder.transformer.h.7.mlp.c_fc.weight\", \"decoder.transformer.h.7.mlp.c_fc.bias\", \"decoder.transformer.h.7.mlp.c_proj.weight\", \"decoder.transformer.h.7.mlp.c_proj.bias\", \"decoder.transformer.h.8.ln_1.weight\", \"decoder.transformer.h.8.ln_1.bias\", \"decoder.transformer.h.8.attn.bias\", \"decoder.transformer.h.8.attn.masked_bias\", \"decoder.transformer.h.8.attn.c_attn.weight\", \"decoder.transformer.h.8.attn.c_attn.bias\", \"decoder.transformer.h.8.attn.c_proj.weight\", \"decoder.transformer.h.8.attn.c_proj.bias\", \"decoder.transformer.h.8.ln_2.weight\", \"decoder.transformer.h.8.ln_2.bias\", \"decoder.transformer.h.8.crossattention.bias\", \"decoder.transformer.h.8.crossattention.masked_bias\", \"decoder.transformer.h.8.crossattention.c_attn.weight\", \"decoder.transformer.h.8.crossattention.c_attn.bias\", \"decoder.transformer.h.8.crossattention.q_attn.weight\", \"decoder.transformer.h.8.crossattention.q_attn.bias\", \"decoder.transformer.h.8.crossattention.c_proj.weight\", \"decoder.transformer.h.8.crossattention.c_proj.bias\", \"decoder.transformer.h.8.ln_cross_attn.weight\", \"decoder.transformer.h.8.ln_cross_attn.bias\", \"decoder.transformer.h.8.mlp.c_fc.weight\", \"decoder.transformer.h.8.mlp.c_fc.bias\", \"decoder.transformer.h.8.mlp.c_proj.weight\", \"decoder.transformer.h.8.mlp.c_proj.bias\", \"decoder.transformer.h.9.ln_1.weight\", \"decoder.transformer.h.9.ln_1.bias\", \"decoder.transformer.h.9.attn.bias\", \"decoder.transformer.h.9.attn.masked_bias\", \"decoder.transformer.h.9.attn.c_attn.weight\", \"decoder.transformer.h.9.attn.c_attn.bias\", \"decoder.transformer.h.9.attn.c_proj.weight\", \"decoder.transformer.h.9.attn.c_proj.bias\", \"decoder.transformer.h.9.ln_2.weight\", \"decoder.transformer.h.9.ln_2.bias\", \"decoder.transformer.h.9.crossattention.bias\", \"decoder.transformer.h.9.crossattention.masked_bias\", \"decoder.transformer.h.9.crossattention.c_attn.weight\", \"decoder.transformer.h.9.crossattention.c_attn.bias\", \"decoder.transformer.h.9.crossattention.q_attn.weight\", \"decoder.transformer.h.9.crossattention.q_attn.bias\", \"decoder.transformer.h.9.crossattention.c_proj.weight\", \"decoder.transformer.h.9.crossattention.c_proj.bias\", \"decoder.transformer.h.9.ln_cross_attn.weight\", \"decoder.transformer.h.9.ln_cross_attn.bias\", \"decoder.transformer.h.9.mlp.c_fc.weight\", \"decoder.transformer.h.9.mlp.c_fc.bias\", \"decoder.transformer.h.9.mlp.c_proj.weight\", \"decoder.transformer.h.9.mlp.c_proj.bias\", \"decoder.transformer.h.10.ln_1.weight\", \"decoder.transformer.h.10.ln_1.bias\", \"decoder.transformer.h.10.attn.bias\", \"decoder.transformer.h.10.attn.masked_bias\", \"decoder.transformer.h.10.attn.c_attn.weight\", \"decoder.transformer.h.10.attn.c_attn.bias\", \"decoder.transformer.h.10.attn.c_proj.weight\", \"decoder.transformer.h.10.attn.c_proj.bias\", \"decoder.transformer.h.10.ln_2.weight\", \"decoder.transformer.h.10.ln_2.bias\", \"decoder.transformer.h.10.crossattention.bias\", \"decoder.transformer.h.10.crossattention.masked_bias\", \"decoder.transformer.h.10.crossattention.c_attn.weight\", \"decoder.transformer.h.10.crossattention.c_attn.bias\", \"decoder.transformer.h.10.crossattention.q_attn.weight\", \"decoder.transformer.h.10.crossattention.q_attn.bias\", \"decoder.transformer.h.10.crossattention.c_proj.weight\", \"decoder.transformer.h.10.crossattention.c_proj.bias\", \"decoder.transformer.h.10.ln_cross_attn.weight\", \"decoder.transformer.h.10.ln_cross_attn.bias\", \"decoder.transformer.h.10.mlp.c_fc.weight\", \"decoder.transformer.h.10.mlp.c_fc.bias\", \"decoder.transformer.h.10.mlp.c_proj.weight\", \"decoder.transformer.h.10.mlp.c_proj.bias\", \"decoder.transformer.h.11.ln_1.weight\", \"decoder.transformer.h.11.ln_1.bias\", \"decoder.transformer.h.11.attn.bias\", \"decoder.transformer.h.11.attn.masked_bias\", \"decoder.transformer.h.11.attn.c_attn.weight\", \"decoder.transformer.h.11.attn.c_attn.bias\", \"decoder.transformer.h.11.attn.c_proj.weight\", \"decoder.transformer.h.11.attn.c_proj.bias\", \"decoder.transformer.h.11.ln_2.weight\", \"decoder.transformer.h.11.ln_2.bias\", \"decoder.transformer.h.11.crossattention.bias\", \"decoder.transformer.h.11.crossattention.masked_bias\", \"decoder.transformer.h.11.crossattention.c_attn.weight\", \"decoder.transformer.h.11.crossattention.c_attn.bias\", \"decoder.transformer.h.11.crossattention.q_attn.weight\", \"decoder.transformer.h.11.crossattention.q_attn.bias\", \"decoder.transformer.h.11.crossattention.c_proj.weight\", \"decoder.transformer.h.11.crossattention.c_proj.bias\", \"decoder.transformer.h.11.ln_cross_attn.weight\", \"decoder.transformer.h.11.ln_cross_attn.bias\", \"decoder.transformer.h.11.mlp.c_fc.weight\", \"decoder.transformer.h.11.mlp.c_fc.bias\", \"decoder.transformer.h.11.mlp.c_proj.weight\", \"decoder.transformer.h.11.mlp.c_proj.bias\", \"decoder.transformer.ln_f.weight\", \"decoder.transformer.ln_f.bias\", \"decoder.lm_head.weight\", \"pos_enb.pe\", \"embedder.0.weight\", \"embedder.0.bias\", \"embedder.2.weight\", \"embedder.2.bias\", \"embedder.2.running_mean\", \"embedder.2.running_var\", \"embedder.3.weight\", \"embedder.3.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.wte.weight\", \"transformer.wpe.weight\", \"transformer.h.0.ln_1.weight\", \"transformer.h.0.ln_1.bias\", \"transformer.h.0.attn.bias\", \"transformer.h.0.attn.masked_bias\", \"transformer.h.0.attn.c_attn.weight\", \"transformer.h.0.attn.c_attn.bias\", \"transformer.h.0.attn.c_proj.weight\", \"transformer.h.0.attn.c_proj.bias\", \"transformer.h.0.ln_2.weight\", \"transformer.h.0.ln_2.bias\", \"transformer.h.0.crossattention.bias\", \"transformer.h.0.crossattention.masked_bias\", \"transformer.h.0.crossattention.c_attn.weight\", \"transformer.h.0.crossattention.c_attn.bias\", \"transformer.h.0.crossattention.q_attn.weight\", \"transformer.h.0.crossattention.q_attn.bias\", \"transformer.h.0.crossattention.c_proj.weight\", \"transformer.h.0.crossattention.c_proj.bias\", \"transformer.h.0.ln_cross_attn.weight\", \"transformer.h.0.ln_cross_attn.bias\", \"transformer.h.0.mlp.c_fc.weight\", \"transformer.h.0.mlp.c_fc.bias\", \"transformer.h.0.mlp.c_proj.weight\", \"transformer.h.0.mlp.c_proj.bias\", \"transformer.h.1.ln_1.weight\", \"transformer.h.1.ln_1.bias\", \"transformer.h.1.attn.bias\", \"transformer.h.1.attn.masked_bias\", \"transformer.h.1.attn.c_attn.weight\", \"transformer.h.1.attn.c_attn.bias\", \"transformer.h.1.attn.c_proj.weight\", \"transformer.h.1.attn.c_proj.bias\", \"transformer.h.1.ln_2.weight\", \"transformer.h.1.ln_2.bias\", \"transformer.h.1.crossattention.bias\", \"transformer.h.1.crossattention.masked_bias\", \"transformer.h.1.crossattention.c_attn.weight\", \"transformer.h.1.crossattention.c_attn.bias\", \"transformer.h.1.crossattention.q_attn.weight\", \"transformer.h.1.crossattention.q_attn.bias\", \"transformer.h.1.crossattention.c_proj.weight\", \"transformer.h.1.crossattention.c_proj.bias\", \"transformer.h.1.ln_cross_attn.weight\", \"transformer.h.1.ln_cross_attn.bias\", \"transformer.h.1.mlp.c_fc.weight\", \"transformer.h.1.mlp.c_fc.bias\", \"transformer.h.1.mlp.c_proj.weight\", \"transformer.h.1.mlp.c_proj.bias\", \"transformer.h.2.ln_1.weight\", \"transformer.h.2.ln_1.bias\", \"transformer.h.2.attn.bias\", \"transformer.h.2.attn.masked_bias\", \"transformer.h.2.attn.c_attn.weight\", \"transformer.h.2.attn.c_attn.bias\", \"transformer.h.2.attn.c_proj.weight\", \"transformer.h.2.attn.c_proj.bias\", \"transformer.h.2.ln_2.weight\", \"transformer.h.2.ln_2.bias\", \"transformer.h.2.crossattention.bias\", \"transformer.h.2.crossattention.masked_bias\", \"transformer.h.2.crossattention.c_attn.weight\", \"transformer.h.2.crossattention.c_attn.bias\", \"transformer.h.2.crossattention.q_attn.weight\", \"transformer.h.2.crossattention.q_attn.bias\", \"transformer.h.2.crossattention.c_proj.weight\", \"transformer.h.2.crossattention.c_proj.bias\", \"transformer.h.2.ln_cross_attn.weight\", \"transformer.h.2.ln_cross_attn.bias\", \"transformer.h.2.mlp.c_fc.weight\", \"transformer.h.2.mlp.c_fc.bias\", \"transformer.h.2.mlp.c_proj.weight\", \"transformer.h.2.mlp.c_proj.bias\", \"transformer.h.3.ln_1.weight\", \"transformer.h.3.ln_1.bias\", \"transformer.h.3.attn.bias\", \"transformer.h.3.attn.masked_bias\", \"transformer.h.3.attn.c_attn.weight\", \"transformer.h.3.attn.c_attn.bias\", \"transformer.h.3.attn.c_proj.weight\", \"transformer.h.3.attn.c_proj.bias\", \"transformer.h.3.ln_2.weight\", \"transformer.h.3.ln_2.bias\", \"transformer.h.3.crossattention.bias\", \"transformer.h.3.crossattention.masked_bias\", \"transformer.h.3.crossattention.c_attn.weight\", \"transformer.h.3.crossattention.c_attn.bias\", \"transformer.h.3.crossattention.q_attn.weight\", \"transformer.h.3.crossattention.q_attn.bias\", \"transformer.h.3.crossattention.c_proj.weight\", \"transformer.h.3.crossattention.c_proj.bias\", \"transformer.h.3.ln_cross_attn.weight\", \"transformer.h.3.ln_cross_attn.bias\", \"transformer.h.3.mlp.c_fc.weight\", \"transformer.h.3.mlp.c_fc.bias\", \"transformer.h.3.mlp.c_proj.weight\", \"transformer.h.3.mlp.c_proj.bias\", \"transformer.h.4.ln_1.weight\", \"transformer.h.4.ln_1.bias\", \"transformer.h.4.attn.bias\", \"transformer.h.4.attn.masked_bias\", \"transformer.h.4.attn.c_attn.weight\", \"transformer.h.4.attn.c_attn.bias\", \"transformer.h.4.attn.c_proj.weight\", \"transformer.h.4.attn.c_proj.bias\", \"transformer.h.4.ln_2.weight\", \"transformer.h.4.ln_2.bias\", \"transformer.h.4.crossattention.bias\", \"transformer.h.4.crossattention.masked_bias\", \"transformer.h.4.crossattention.c_attn.weight\", \"transformer.h.4.crossattention.c_attn.bias\", \"transformer.h.4.crossattention.q_attn.weight\", \"transformer.h.4.crossattention.q_attn.bias\", \"transformer.h.4.crossattention.c_proj.weight\", \"transformer.h.4.crossattention.c_proj.bias\", \"transformer.h.4.ln_cross_attn.weight\", \"transformer.h.4.ln_cross_attn.bias\", \"transformer.h.4.mlp.c_fc.weight\", \"transformer.h.4.mlp.c_fc.bias\", \"transformer.h.4.mlp.c_proj.weight\", \"transformer.h.4.mlp.c_proj.bias\", \"transformer.h.5.ln_1.weight\", \"transformer.h.5.ln_1.bias\", \"transformer.h.5.attn.bias\", \"transformer.h.5.attn.masked_bias\", \"transformer.h.5.attn.c_attn.weight\", \"transformer.h.5.attn.c_attn.bias\", \"transformer.h.5.attn.c_proj.weight\", \"transformer.h.5.attn.c_proj.bias\", \"transformer.h.5.ln_2.weight\", \"transformer.h.5.ln_2.bias\", \"transformer.h.5.crossattention.bias\", \"transformer.h.5.crossattention.masked_bias\", \"transformer.h.5.crossattention.c_attn.weight\", \"transformer.h.5.crossattention.c_attn.bias\", \"transformer.h.5.crossattention.q_attn.weight\", \"transformer.h.5.crossattention.q_attn.bias\", \"transformer.h.5.crossattention.c_proj.weight\", \"transformer.h.5.crossattention.c_proj.bias\", \"transformer.h.5.ln_cross_attn.weight\", \"transformer.h.5.ln_cross_attn.bias\", \"transformer.h.5.mlp.c_fc.weight\", \"transformer.h.5.mlp.c_fc.bias\", \"transformer.h.5.mlp.c_proj.weight\", \"transformer.h.5.mlp.c_proj.bias\", \"transformer.h.6.ln_1.weight\", \"transformer.h.6.ln_1.bias\", \"transformer.h.6.attn.bias\", \"transformer.h.6.attn.masked_bias\", \"transformer.h.6.attn.c_attn.weight\", \"transformer.h.6.attn.c_attn.bias\", \"transformer.h.6.attn.c_proj.weight\", \"transformer.h.6.attn.c_proj.bias\", \"transformer.h.6.ln_2.weight\", \"transformer.h.6.ln_2.bias\", \"transformer.h.6.crossattention.bias\", \"transformer.h.6.crossattention.masked_bias\", \"transformer.h.6.crossattention.c_attn.weight\", \"transformer.h.6.crossattention.c_attn.bias\", \"transformer.h.6.crossattention.q_attn.weight\", \"transformer.h.6.crossattention.q_attn.bias\", \"transformer.h.6.crossattention.c_proj.weight\", \"transformer.h.6.crossattention.c_proj.bias\", \"transformer.h.6.ln_cross_attn.weight\", \"transformer.h.6.ln_cross_attn.bias\", \"transformer.h.6.mlp.c_fc.weight\", \"transformer.h.6.mlp.c_fc.bias\", \"transformer.h.6.mlp.c_proj.weight\", \"transformer.h.6.mlp.c_proj.bias\", \"transformer.h.7.ln_1.weight\", \"transformer.h.7.ln_1.bias\", \"transformer.h.7.attn.bias\", \"transformer.h.7.attn.masked_bias\", \"transformer.h.7.attn.c_attn.weight\", \"transformer.h.7.attn.c_attn.bias\", \"transformer.h.7.attn.c_proj.weight\", \"transformer.h.7.attn.c_proj.bias\", \"transformer.h.7.ln_2.weight\", \"transformer.h.7.ln_2.bias\", \"transformer.h.7.crossattention.bias\", \"transformer.h.7.crossattention.masked_bias\", \"transformer.h.7.crossattention.c_attn.weight\", \"transformer.h.7.crossattention.c_attn.bias\", \"transformer.h.7.crossattention.q_attn.weight\", \"transformer.h.7.crossattention.q_attn.bias\", \"transformer.h.7.crossattention.c_proj.weight\", \"transformer.h.7.crossattention.c_proj.bias\", \"transformer.h.7.ln_cross_attn.weight\", \"transformer.h.7.ln_cross_attn.bias\", \"transformer.h.7.mlp.c_fc.weight\", \"transformer.h.7.mlp.c_fc.bias\", \"transformer.h.7.mlp.c_proj.weight\", \"transformer.h.7.mlp.c_proj.bias\", \"transformer.h.8.ln_1.weight\", \"transformer.h.8.ln_1.bias\", \"transformer.h.8.attn.bias\", \"transformer.h.8.attn.masked_bias\", \"transformer.h.8.attn.c_attn.weight\", \"transformer.h.8.attn.c_attn.bias\", \"transformer.h.8.attn.c_proj.weight\", \"transformer.h.8.attn.c_proj.bias\", \"transformer.h.8.ln_2.weight\", \"transformer.h.8.ln_2.bias\", \"transformer.h.8.crossattention.bias\", \"transformer.h.8.crossattention.masked_bias\", \"transformer.h.8.crossattention.c_attn.weight\", \"transformer.h.8.crossattention.c_attn.bias\", \"transformer.h.8.crossattention.q_attn.weight\", \"transformer.h.8.crossattention.q_attn.bias\", \"transformer.h.8.crossattention.c_proj.weight\", \"transformer.h.8.crossattention.c_proj.bias\", \"transformer.h.8.ln_cross_attn.weight\", \"transformer.h.8.ln_cross_attn.bias\", \"transformer.h.8.mlp.c_fc.weight\", \"transformer.h.8.mlp.c_fc.bias\", \"transformer.h.8.mlp.c_proj.weight\", \"transformer.h.8.mlp.c_proj.bias\", \"transformer.h.9.ln_1.weight\", \"transformer.h.9.ln_1.bias\", \"transformer.h.9.attn.bias\", \"transformer.h.9.attn.masked_bias\", \"transformer.h.9.attn.c_attn.weight\", \"transformer.h.9.attn.c_attn.bias\", \"transformer.h.9.attn.c_proj.weight\", \"transformer.h.9.attn.c_proj.bias\", \"transformer.h.9.ln_2.weight\", \"transformer.h.9.ln_2.bias\", \"transformer.h.9.crossattention.bias\", \"transformer.h.9.crossattention.masked_bias\", \"transformer.h.9.crossattention.c_attn.weight\", \"transformer.h.9.crossattention.c_attn.bias\", \"transformer.h.9.crossattention.q_attn.weight\", \"transformer.h.9.crossattention.q_attn.bias\", \"transformer.h.9.crossattention.c_proj.weight\", \"transformer.h.9.crossattention.c_proj.bias\", \"transformer.h.9.ln_cross_attn.weight\", \"transformer.h.9.ln_cross_attn.bias\", \"transformer.h.9.mlp.c_fc.weight\", \"transformer.h.9.mlp.c_fc.bias\", \"transformer.h.9.mlp.c_proj.weight\", \"transformer.h.9.mlp.c_proj.bias\", \"transformer.h.10.ln_1.weight\", \"transformer.h.10.ln_1.bias\", \"transformer.h.10.attn.bias\", \"transformer.h.10.attn.masked_bias\", \"transformer.h.10.attn.c_attn.weight\", \"transformer.h.10.attn.c_attn.bias\", \"transformer.h.10.attn.c_proj.weight\", \"transformer.h.10.attn.c_proj.bias\", \"transformer.h.10.ln_2.weight\", \"transformer.h.10.ln_2.bias\", \"transformer.h.10.crossattention.bias\", \"transformer.h.10.crossattention.masked_bias\", \"transformer.h.10.crossattention.c_attn.weight\", \"transformer.h.10.crossattention.c_attn.bias\", \"transformer.h.10.crossattention.q_attn.weight\", \"transformer.h.10.crossattention.q_attn.bias\", \"transformer.h.10.crossattention.c_proj.weight\", \"transformer.h.10.crossattention.c_proj.bias\", \"transformer.h.10.ln_cross_attn.weight\", \"transformer.h.10.ln_cross_attn.bias\", \"transformer.h.10.mlp.c_fc.weight\", \"transformer.h.10.mlp.c_fc.bias\", \"transformer.h.10.mlp.c_proj.weight\", \"transformer.h.10.mlp.c_proj.bias\", \"transformer.h.11.ln_1.weight\", \"transformer.h.11.ln_1.bias\", \"transformer.h.11.attn.bias\", \"transformer.h.11.attn.masked_bias\", \"transformer.h.11.attn.c_attn.weight\", \"transformer.h.11.attn.c_attn.bias\", \"transformer.h.11.attn.c_proj.weight\", \"transformer.h.11.attn.c_proj.bias\", \"transformer.h.11.ln_2.weight\", \"transformer.h.11.ln_2.bias\", \"transformer.h.11.crossattention.bias\", \"transformer.h.11.crossattention.masked_bias\", \"transformer.h.11.crossattention.c_attn.weight\", \"transformer.h.11.crossattention.c_attn.bias\", \"transformer.h.11.crossattention.q_attn.weight\", \"transformer.h.11.crossattention.q_attn.bias\", \"transformer.h.11.crossattention.c_proj.weight\", \"transformer.h.11.crossattention.c_proj.bias\", \"transformer.h.11.ln_cross_attn.weight\", \"transformer.h.11.ln_cross_attn.bias\", \"transformer.h.11.mlp.c_fc.weight\", \"transformer.h.11.mlp.c_fc.bias\", \"transformer.h.11.mlp.c_proj.weight\", \"transformer.h.11.mlp.c_proj.bias\", \"transformer.ln_f.weight\", \"transformer.ln_f.bias\", \"lm_head.weight\". "
     ]
    }
   ],
   "source": [
    "enc_dec_model.load_state_dict(torch.load('model/gpt2_enc_dec.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/gpt2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2Config()\n",
    "model.config.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
