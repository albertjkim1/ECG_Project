{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from base64 import b64decode as decode\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.fft as fft\n",
    "import torch.nn.functional as F\n",
    "from transformers import * #GPT2Tokenizer, GPT2Config, GPT2PreTrainedModel, GPT2Model\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "\n",
    "import transformers\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-e23654c93940>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m from ...modeling_outputs import (\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mBaseModelOutputWithPastAndCrossAttentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mCausalLMOutputWithCrossAttentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/d_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/d_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/d_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/d_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead dataz\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "\n",
    "\n",
    "# adding the diagnosis and labels\n",
    "waveform_and_diag = pd.merge(waveform_lead[['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']], diagnosis_data[[\"exam_id\", \"Full_text\", \"Original_Diag\"]], left_on= \"exam_id\", right_on=\"exam_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: MinMaxScaler().fit_transform(np.vstack(x)))\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "waveform_lead_median = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Median\"]\n",
    "\n",
    "waveform_lead_rhythm['decoded_waveform'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the labels/sentences\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "# Let's look over this tomorrow\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "#\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if row[\"statement_order\"] == 1 and curr_string != \"\":\n",
    "        curr_string = curr_string.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        val = [curr_id, curr_string[1:]]\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    curr_string += \" \" + row[\"Full_text\"]\n",
    "\n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "#waveform_lead_rhythm_diag\n",
    "waveform_lead_rhythm_diag\n",
    "\n",
    "# define full_x\n",
    "full_x = torch.tensor(list(waveform_lead_rhythm_diag['decoded_waveform'])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'borderline', 'normal', 't', 'otherwise', 'tachycardia', 'criteria', 'with', 'fibrillation', 'may', 'voltage', 'rhythm', 'consider', 'atrial', 'arrhythmia', 'wave', 'for', 'be', 'qrs', 'minimal', 'abnormal', 'ischemia', 'sinus', 'inferior', 'low', 'variant', 'abnormality', 'ecg', 'bradycardia', 'lvh'}\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for num, sentence in diagnoses:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word)\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv_0): Conv1d(8, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_0): ReLU()\n",
      "  (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_0): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_0): ReLU()\n",
      "  (conv_1): Conv1d(16, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_1): ReLU()\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_1): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_1): ReLU()\n",
      "  (conv_2): Conv1d(32, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_2): ReLU()\n",
      "  (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_2): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_2): ReLU()\n",
      "  (conv_3): Conv1d(64, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_3): ReLU()\n",
      "  (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_3): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_3): ReLU()\n",
      "  (conv_4): Conv1d(128, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_4): ReLU()\n",
      "  (batch_norm_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_4): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (conv1d_2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_4): ReLU()\n",
      "  (conv_fin): Conv1d(256, 768, kernel_size=(11,), stride=(1,), padding=(5,))\n",
      "  (act_fin): ReLU()\n",
      "  (batch_fin): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "KER_SIZE = 11\n",
    "PADDING = 5\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(5):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 768, kernel_size = KER_SIZE, padding = PADDING))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(768))\n",
    "print(conv_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 8, 2500])\n",
      "torch.Size([7, 768, 2500])\n",
      "torch.Size([7, 8, 2500])\n"
     ]
    }
   ],
   "source": [
    "deconv_model = nn.Sequential()\n",
    "init_channels = 768\n",
    "for i in range(5):\n",
    "    next_channels = init_channels // 2\n",
    "    deconv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    deconv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    deconv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    deconv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    deconv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "deconv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = KER_SIZE, padding = PADDING))\n",
    "deconv_model.add_module('act_fin', nn.ReLU())\n",
    "deconv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "\n",
    "print(full_x.shape)\n",
    "print(conv_model(full_x).shape)\n",
    "print(deconv_model(conv_model(full_x)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def make_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def make_decoder(self):\n",
    "        return self.decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3016, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2688, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1736, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1416, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0946, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0625, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0360, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9886, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9671, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9323, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9133, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8988, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8857, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8753, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8668, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8571, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8494, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8425, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8355, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8292, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8223, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8157, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8091, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8028, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7972, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7916, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7860, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7811, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7762, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7710, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7659, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7611, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7569, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7524, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7435, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7392, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7353, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7312, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7268, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7230, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7194, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7158, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7120, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7083, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7045, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7013, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6980, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6947, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6915, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6881, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6845, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6814, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6786, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6763, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6728, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6694, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6667, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6638, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6608, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6576, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6548, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6521, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6488, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6458, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6428, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6400, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6369, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6341, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6310, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6275, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6244, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6208, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6170, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6134, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6098, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6070, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6035, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5958, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5914, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5851, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5820, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5780, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5737, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5709, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5683, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5651, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5616, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5590, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5560, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5532, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5504, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5481, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5454, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5429, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5405, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5379, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5353, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5329, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5305, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5282, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5261, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5233, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5210, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5188, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5166, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5145, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5122, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5102, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5085, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5072, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5061, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5044, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5018, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5000, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4979, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4963, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4943, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4928, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4899, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4889, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4861, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4844, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4818, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4800, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4779, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4757, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4740, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4718, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4700, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4681, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4660, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4641, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4624, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4603, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4586, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4567, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4549, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4536, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4529, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4535, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4553, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4550, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4533, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4517, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4486, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4447, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4432, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4399, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4395, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4361, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4350, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4324, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4310, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4287, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4273, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4251, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4235, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4216, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4199, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4182, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4165, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4149, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4131, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4114, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4099, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4082, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4066, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4051, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4034, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3988, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3973, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3960, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3949, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3942, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3925, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3909, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model to set to\n",
    "auto_model = ConvAutoEncoder(conv_model, deconv_model)\n",
    "auto_optimizer = torch.optim.Adam(auto_model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Training params\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#print(train_x[0])\n",
    "for i in range(180):\n",
    "    #print(train_x.shape)\n",
    "    auto_optimizer.zero_grad()\n",
    "    outputs = auto_model(full_x)\n",
    "    #print(outputs.shape)\n",
    "    losses = loss_function(outputs, full_x)\n",
    "    losses.backward(retain_graph=True)\n",
    "    auto_optimizer.step()\n",
    "    print(losses)\n",
    "    if losses < .001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(auto_model.state_dict(), 'model/autoencoder.pt')\n",
    "\n",
    "conv_embedder = auto_model.make_encoder()\n",
    "\n",
    "torch.save(conv_embedder.state_dict(), \"model/embedder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 1: ResNet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv_0): Conv1d(8, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_0): ReLU()\n",
      "  (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_0): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_0): ReLU()\n",
      "  (conv_1): Conv1d(16, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_1): ReLU()\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_1): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_1): ReLU()\n",
      "  (conv_2): Conv1d(32, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_2): ReLU()\n",
      "  (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_2): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_2): ReLU()\n",
      "  (conv_3): Conv1d(64, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_3): ReLU()\n",
      "  (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_3): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_3): ReLU()\n",
      "  (conv_4): Conv1d(128, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_4): ReLU()\n",
      "  (batch_norm_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_4): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_4): ReLU()\n",
      "  (conv_fin): Conv1d(256, 8, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_fin): ReLU()\n",
      "  (batch_fin): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "torch.Size([6, 8, 2500])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "# build resent model and display the shape of feed through\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(5):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = 249, padding = 124, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = 249, padding = 124))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "    \n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = 249, padding = 124))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "print(conv_model)\n",
    "print(conv_model(train_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 2 - LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 250\n",
    "embedding_dim = 8\n",
    "num_words = len(dict_words)\n",
    "\n",
    "class LSTM_EncoderDecoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim, word_list_length):\n",
    "        super(ECG_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim, num_layers = 4, bidirectional = True)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq_embedded = seq.view(len(seq), -1, embedding_dim)\n",
    "        final_hidd, _ = self.lstm(seq_embedded)\n",
    "        dec_seq = self.linear(final_hidd)\n",
    "        return F.log_softmax(dec_seq, dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 3 - Basic Transformer Architecture with Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ECGTransformerEncoder(nn.Module):\n",
    "    # Takes the ECG discrete signals sequence and maps into a probability distribution of diagnosis\n",
    "    # For working/verification purposes\n",
    "    def __init__(self, vector_size, embed_dim, n_heads, hidden_linear_dim, n_layers, dropout):\n",
    "        super(ECGTransformerEncoder, self).__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.positional_encoder = PositionalEncoder(embed_dim, dropout)\n",
    "    \n",
    "        #Since our data is already discrete numbers, might need some tweaking for this\n",
    "        self.embedder = conv_embedder\n",
    "                        #64 31              #39        64\n",
    "        \n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(embed_dim, n_heads, hidden_linear_dim, dropout),\n",
    "            n_layers)\n",
    "        \n",
    "        self.n_inputs = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Simple linear decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(768, 17),\n",
    "                        Transpose(17, 2500),\n",
    "                        nn.Linear(2500, 30),\n",
    "                        nn.LogSoftmax()\n",
    "                        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #self.embedder.weight.data.uniform_(-.1, .1)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        #self.decoder.weight.data.uniform_(-.1, .1)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.embedder(x) # * math.sqrt(self.n_inputs)\n",
    "        x = x.squeeze(0)\n",
    "        #x = x.view(2500, 8)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1) \n",
    "        #x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the number of the last batch sample in the data set is smaller than the defined batch_batch size, mismatch problems will occur. You can modify it yourself, for example, just pass in the shape behind, and then enter it through x.szie(0).\n",
    "        return x.view(self.shape)\n",
    "\n",
    "class SignalEmbedder(nn.Module):\n",
    "    # Necessary to convert the signal into \"word\" vectors for transformer processing.\n",
    "    # Currently a simple group and slice method, but will modify later for multi-channel inputs\n",
    "    \n",
    "    def __init__(self, num_slices, size_of_slice):\n",
    "        super(SignalEmbedder, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.size_of_slice = size_of_slice\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[: self.num_slices * self.size_of_slice]\n",
    "        x = x.reshape((self.num_slices, self.size_of_slice))\n",
    "        return x  \n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    # Necessary to store positional data about the input data\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=2500, batch_size = 1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, 1, embed_dim)\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        divisor = torch.exp(torch.arange(0, embed_dim, 2).float() * (- math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pos_encoding[:, 0, 0::2] = torch.sin(position * divisor)\n",
    "        pos_encoding[:, 0, 1::2] = torch.cos(position * divisor)\n",
    "        pos_encoding = pos_encoding.repeat(1, batch_size, 1)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoding[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(features, features * expansion)\n",
    "        self.linear_2 = nn.Linear(features * expansion, features)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        #self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.norm_1(x + res)\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "class FNETLayer(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FNETLayer, self).__init__()\n",
    "        self.feed_forward = FeedForwardNet(features, expansion, dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = fft.fftn(x, dim = (-2, -1)).real\n",
    "        x = self.norm_1(x + res)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "    \n",
    "class FNETEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self, features, expansion=2, dropout=0.5, num_layers=6):\n",
    "        encoder_layer = FNETLayer(features, expansion, dropout)\n",
    "        super().__init__(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    " \n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x.transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 1 - Huggingface GPT2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward2(\n",
    "    self,\n",
    "    input_ids=None,\n",
    "    past_key_values=None,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    encoder_hidden_states=None,\n",
    "    encoder_attention_mask=None,\n",
    "    use_cache=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "):\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    if input_ids is not None and inputs_embeds is not None:\n",
    "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "    elif input_ids is not None:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        batch_size = input_ids.shape[0]\n",
    "    elif inputs_embeds is not None:\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "        batch_size = inputs_embeds.shape[0]\n",
    "    else:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
    "    if position_ids is not None:\n",
    "        position_ids = position_ids.view(-1, input_shape[-1])\n",
    "\n",
    "    if past_key_values is None:\n",
    "        past_length = 0\n",
    "        past_key_values = tuple([None] * len(self.h))\n",
    "    else:\n",
    "        past_length = past_key_values[0][0].size(-2)\n",
    "    if position_ids is None:\n",
    "        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "\n",
    "    # GPT2Attention mask.\n",
    "    if attention_mask is not None:\n",
    "        assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
    "        attention_mask = attention_mask.view(batch_size, -1)\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "\n",
    "    # If a 2D ou 3D attention mask is provided for the cross-attention\n",
    "    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "    if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
    "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "        if encoder_attention_mask is None:\n",
    "            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "        encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "    else:\n",
    "        encoder_attention_mask = None\n",
    "\n",
    "    # Prepare head mask if needed\n",
    "    # 1.0 in head_mask indicate we keep the head\n",
    "    # attention_probs has shape bsz x n_heads x N x N\n",
    "    # head_mask has shape n_layer x batch x n_heads x N x N\n",
    "    head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "    position_embeds = self.wpe(position_ids)\n",
    "    hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "    if token_type_ids is not None:\n",
    "        token_type_embeds = self.wte(token_type_ids)\n",
    "        hidden_states = hidden_states + token_type_embeds\n",
    "\n",
    "    hidden_states = self.drop(hidden_states)\n",
    "\n",
    "    output_shape = input_shape + (hidden_states.size(-1),)\n",
    "\n",
    "    presents = () if use_cache else None\n",
    "    all_self_attentions = () if output_attentions else None\n",
    "    all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
    "\n",
    "        # Model parallel\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(hidden_states.device)\n",
    "            # Ensure layer_past is on same device as hidden_states (might not be correct)\n",
    "            if layer_past is not None:\n",
    "                layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n",
    "            # Ensure that attention_mask is always on the same device as hidden_states\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(hidden_states.device)\n",
    "            if isinstance(head_mask, torch.Tensor):\n",
    "                head_mask = head_mask.to(hidden_states.device)\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "            if use_cache:\n",
    "                logger.warning(\n",
    "                    \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
    "                    \"`use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "            def create_custom_forward(module):\n",
    "                def custom_forward(*inputs):\n",
    "                    # None for past_key_value\n",
    "                    return module(*inputs, use_cache, output_attentions)\n",
    "\n",
    "                return custom_forward\n",
    "\n",
    "            outputs = torch.utils.checkpoint.checkpoint(\n",
    "                create_custom_forward(block),\n",
    "                hidden_states,\n",
    "                None,\n",
    "                attention_mask,\n",
    "                head_mask[i],\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "            )\n",
    "        else:\n",
    "            outputs = block(\n",
    "                hidden_states,\n",
    "                layer_past=layer_past,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask[i],\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if use_cache is True:\n",
    "            presents = presents + (outputs[1],)\n",
    "\n",
    "        if output_attentions:\n",
    "            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
    "            if self.config.add_cross_attention:\n",
    "                all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
    "\n",
    "        # Model Parallel: If it's the last layer for that device, put things on the next device\n",
    "        if self.model_parallel:\n",
    "            for k, v in self.device_map.items():\n",
    "                if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
    "                    hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
    "\n",
    "    hidden_states = self.ln_f(hidden_states)\n",
    "    \n",
    "    print(hidden_states.shape, output_shape)\n",
    "    \n",
    "    hidden_states = hidden_states.view(*output_shape)\n",
    "    # Add last hidden state\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    if not return_dict:\n",
    "        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=presents,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_self_attentions,\n",
    "        cross_attentions=all_cross_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2LMHeadModel(GPT2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"attn.masked_bias\", r\"attn.bias\", r\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        #self.transformer.forward = forward2.__get__(self.transformer, GPT2Model)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "    def parallelize(self, device_map=None):\n",
    "        self.device_map = (\n",
    "            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n",
    "            if device_map is None\n",
    "            else device_map\n",
    "        )\n",
    "        assert_device_map(self.device_map, len(self.transformer.h))\n",
    "        self.transformer.parallelize(self.device_map)\n",
    "        self.lm_head = self.lm_head.to(self.transformer.first_device)\n",
    "        self.model_parallel = True\n",
    "        \n",
    "    def deparallelize(self):\n",
    "        self.transformer.deparallelize()\n",
    "        self.transformer = self.transformer.to(\"cpu\")\n",
    "        self.lm_head = self.lm_head.to(\"cpu\")\n",
    "        self.model_parallel = False\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            position_ids = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"encoder_hidden_states\": kwargs.get(\"encoder_hidden_states\", None),\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.transformer.first_device)\n",
    "            hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reorder_cache(past: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the :obj:`past_key_values` cache if\n",
    "        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.ln_cross_attn.weight', 'h.5.crossattention.bias', 'h.11.crossattention.bias', 'h.9.crossattention.q_attn.weight', 'h.2.crossattention.bias', 'h.2.crossattention.c_proj.bias', 'h.5.crossattention.q_attn.weight', 'h.6.crossattention.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.0.crossattention.bias', 'h.3.crossattention.masked_bias', 'h.0.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.11.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.3.crossattention.q_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.9.crossattention.bias', 'h.6.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.9.crossattention.c_proj.bias', 'h.6.ln_cross_attn.weight', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.1.ln_cross_attn.weight', 'h.7.crossattention.masked_bias', 'h.4.crossattention.c_proj.bias', 'h.3.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.10.crossattention.bias', 'h.10.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.weight', 'h.3.crossattention.bias', 'h.5.ln_cross_attn.weight', 'h.8.ln_cross_attn.weight', 'h.9.ln_cross_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.weight', 'h.0.crossattention.masked_bias', 'h.5.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.weight', 'h.7.crossattention.bias', 'h.2.ln_cross_attn.weight', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.weight', 'h.6.crossattention.masked_bias', 'h.6.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.4.crossattention.bias', 'h.9.crossattention.c_proj.weight', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.masked_bias', 'h.1.crossattention.masked_bias', 'h.7.crossattention.c_attn.weight', 'h.8.crossattention.masked_bias', 'h.3.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.8.crossattention.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.bias', 'h.2.crossattention.masked_bias', 'h.9.crossattention.masked_bias', 'h.10.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "    token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "    # only last token for inputs_ids if past is defined in kwargs\n",
    "    if past:\n",
    "        input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "    attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "    position_ids = kwargs.get(\"position_ids\", None)\n",
    "    print(\"poop\")\n",
    "    if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "        if past:\n",
    "            position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "    else:\n",
    "        position_ids = None\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"past_key_values\": past,\n",
    "        \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "        \"encoder_hidden_states\": kwargs.get(\"encoder_hidden_states\", None),\n",
    "        \"position_ids\": position_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "    }\n",
    "\n",
    "# define tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', config = GPT2Config(add_cross_attention = True))\n",
    "\n",
    "# preprocess training labels and tokenize\n",
    "train_labels = list(waveform_lead_rhythm_diag['diagnosis'])\n",
    "inputs = tokenizer(train_labels, padding = True, verbose = False, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(8.0579, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(5.5628, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(3.0102, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(2.9243, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(2.1574, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(1.6910, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(1.4633, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(1.3435, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(1.1420, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.9907, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.8403, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.7053, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.5677, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.4541, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.3781, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.3731, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.3357, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.2472, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.1931, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.1652, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.1417, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.1258, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.1130, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0927, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0694, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0648, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0605, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0523, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0513, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0454, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0401, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0393, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0369, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0365, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0354, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0345, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n",
      "tensor(0.0350, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-29755b92caad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pretrain decoder\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels = inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "torch.save(model.state_dict(), 'model/gpt2.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder - FNET Encoder Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768]) torch.Size([1, 1, 768])\n",
      "tensor([50256,  7813,   385, 18662,  3487,  9940,    70, 50256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|endoftext|> sinus rhythm normal ecg<|endoftext|>'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create encoder decoder model with GPT2 \n",
    "class CustEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder):\n",
    "        super(CustEncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pos_enb = PositionalEncoding(d_model = 768)\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ecgs, labels = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(**labels, labels = labels[\"input_ids\"], encoder_hidden_states = x.contiguous())\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        ecgs = x.unsqueeze(0)\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        sequence = self.decoder.generate(encoder_hidden_states = x.contiguous())[0]\n",
    "        print(sequence)\n",
    "        return tokenizer.decode(sequence)\n",
    "    \n",
    "    def return_enc(self):\n",
    "        return self.encoder\n",
    "\n",
    "# define component models\n",
    "conv_embedder = nn.Sequential(nn.Conv1d(in_channels = 8, out_channels = 350, kernel_size = 15, padding = 7, stride = 1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.BatchNorm1d(350),\n",
    "                              nn.Conv1d(in_channels = 350, out_channels = 768, kernel_size = 15, padding = 7, stride = 1),\n",
    "                              nn.ReLU())\n",
    "model.load_state_dict(torch.load('model/gpt2.pt'))\n",
    "encoder = FNETEncoder(768, expansion = 2, dropout=0.1, num_layers = 6)\n",
    "enc_dec_model = CustEncoderDecoder(encoder, model, conv_embedder)\n",
    "\n",
    "# define all x, feed through to see if all is good\n",
    "full_x = torch.tensor(list(waveform_lead_rhythm_diag['decoded_waveform'])).float()\n",
    "#enc_dec_model((full_x, inputs))\n",
    "\n",
    "enc_dec_model.predict(full_x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:132: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ..\\aten\\src\\ATen\\native\\Copy.cpp:162.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0347, grad_fn=<NllLossBackward>)\n",
      "torch.Size([7, 25, 768]) torch.Size([7, 25, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-38e2297584be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_dec_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train encoder decoder model!\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-5)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = enc_dec_model((full_x, inputs))\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "torch.save(enc_dec_model.state_dict(), 'model/gpt2_enc_dec.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustEncoderDecoder:\n\tMissing key(s) in state_dict: \"encoder.layers.0.feed_forward.linear_1.weight\", \"encoder.layers.0.feed_forward.linear_1.bias\", \"encoder.layers.0.feed_forward.linear_2.weight\", \"encoder.layers.0.feed_forward.linear_2.bias\", \"encoder.layers.0.feed_forward.norm_1.weight\", \"encoder.layers.0.feed_forward.norm_1.bias\", \"encoder.layers.0.norm_1.weight\", \"encoder.layers.0.norm_1.bias\", \"encoder.layers.1.feed_forward.linear_1.weight\", \"encoder.layers.1.feed_forward.linear_1.bias\", \"encoder.layers.1.feed_forward.linear_2.weight\", \"encoder.layers.1.feed_forward.linear_2.bias\", \"encoder.layers.1.feed_forward.norm_1.weight\", \"encoder.layers.1.feed_forward.norm_1.bias\", \"encoder.layers.1.norm_1.weight\", \"encoder.layers.1.norm_1.bias\", \"encoder.layers.2.feed_forward.linear_1.weight\", \"encoder.layers.2.feed_forward.linear_1.bias\", \"encoder.layers.2.feed_forward.linear_2.weight\", \"encoder.layers.2.feed_forward.linear_2.bias\", \"encoder.layers.2.feed_forward.norm_1.weight\", \"encoder.layers.2.feed_forward.norm_1.bias\", \"encoder.layers.2.norm_1.weight\", \"encoder.layers.2.norm_1.bias\", \"encoder.layers.3.feed_forward.linear_1.weight\", \"encoder.layers.3.feed_forward.linear_1.bias\", \"encoder.layers.3.feed_forward.linear_2.weight\", \"encoder.layers.3.feed_forward.linear_2.bias\", \"encoder.layers.3.feed_forward.norm_1.weight\", \"encoder.layers.3.feed_forward.norm_1.bias\", \"encoder.layers.3.norm_1.weight\", \"encoder.layers.3.norm_1.bias\", \"encoder.layers.4.feed_forward.linear_1.weight\", \"encoder.layers.4.feed_forward.linear_1.bias\", \"encoder.layers.4.feed_forward.linear_2.weight\", \"encoder.layers.4.feed_forward.linear_2.bias\", \"encoder.layers.4.feed_forward.norm_1.weight\", \"encoder.layers.4.feed_forward.norm_1.bias\", \"encoder.layers.4.norm_1.weight\", \"encoder.layers.4.norm_1.bias\", \"encoder.layers.5.feed_forward.linear_1.weight\", \"encoder.layers.5.feed_forward.linear_1.bias\", \"encoder.layers.5.feed_forward.linear_2.weight\", \"encoder.layers.5.feed_forward.linear_2.bias\", \"encoder.layers.5.feed_forward.norm_1.weight\", \"encoder.layers.5.feed_forward.norm_1.bias\", \"encoder.layers.5.norm_1.weight\", \"encoder.layers.5.norm_1.bias\", \"decoder.transformer.wte.weight\", \"decoder.transformer.wpe.weight\", \"decoder.transformer.h.0.ln_1.weight\", \"decoder.transformer.h.0.ln_1.bias\", \"decoder.transformer.h.0.attn.bias\", \"decoder.transformer.h.0.attn.masked_bias\", \"decoder.transformer.h.0.attn.c_attn.weight\", \"decoder.transformer.h.0.attn.c_attn.bias\", \"decoder.transformer.h.0.attn.c_proj.weight\", \"decoder.transformer.h.0.attn.c_proj.bias\", \"decoder.transformer.h.0.ln_2.weight\", \"decoder.transformer.h.0.ln_2.bias\", \"decoder.transformer.h.0.crossattention.bias\", \"decoder.transformer.h.0.crossattention.masked_bias\", \"decoder.transformer.h.0.crossattention.c_attn.weight\", \"decoder.transformer.h.0.crossattention.c_attn.bias\", \"decoder.transformer.h.0.crossattention.q_attn.weight\", \"decoder.transformer.h.0.crossattention.q_attn.bias\", \"decoder.transformer.h.0.crossattention.c_proj.weight\", \"decoder.transformer.h.0.crossattention.c_proj.bias\", \"decoder.transformer.h.0.ln_cross_attn.weight\", \"decoder.transformer.h.0.ln_cross_attn.bias\", \"decoder.transformer.h.0.mlp.c_fc.weight\", \"decoder.transformer.h.0.mlp.c_fc.bias\", \"decoder.transformer.h.0.mlp.c_proj.weight\", \"decoder.transformer.h.0.mlp.c_proj.bias\", \"decoder.transformer.h.1.ln_1.weight\", \"decoder.transformer.h.1.ln_1.bias\", \"decoder.transformer.h.1.attn.bias\", \"decoder.transformer.h.1.attn.masked_bias\", \"decoder.transformer.h.1.attn.c_attn.weight\", \"decoder.transformer.h.1.attn.c_attn.bias\", \"decoder.transformer.h.1.attn.c_proj.weight\", \"decoder.transformer.h.1.attn.c_proj.bias\", \"decoder.transformer.h.1.ln_2.weight\", \"decoder.transformer.h.1.ln_2.bias\", \"decoder.transformer.h.1.crossattention.bias\", \"decoder.transformer.h.1.crossattention.masked_bias\", \"decoder.transformer.h.1.crossattention.c_attn.weight\", \"decoder.transformer.h.1.crossattention.c_attn.bias\", \"decoder.transformer.h.1.crossattention.q_attn.weight\", \"decoder.transformer.h.1.crossattention.q_attn.bias\", \"decoder.transformer.h.1.crossattention.c_proj.weight\", \"decoder.transformer.h.1.crossattention.c_proj.bias\", \"decoder.transformer.h.1.ln_cross_attn.weight\", \"decoder.transformer.h.1.ln_cross_attn.bias\", \"decoder.transformer.h.1.mlp.c_fc.weight\", \"decoder.transformer.h.1.mlp.c_fc.bias\", \"decoder.transformer.h.1.mlp.c_proj.weight\", \"decoder.transformer.h.1.mlp.c_proj.bias\", \"decoder.transformer.h.2.ln_1.weight\", \"decoder.transformer.h.2.ln_1.bias\", \"decoder.transformer.h.2.attn.bias\", \"decoder.transformer.h.2.attn.masked_bias\", \"decoder.transformer.h.2.attn.c_attn.weight\", \"decoder.transformer.h.2.attn.c_attn.bias\", \"decoder.transformer.h.2.attn.c_proj.weight\", \"decoder.transformer.h.2.attn.c_proj.bias\", \"decoder.transformer.h.2.ln_2.weight\", \"decoder.transformer.h.2.ln_2.bias\", \"decoder.transformer.h.2.crossattention.bias\", \"decoder.transformer.h.2.crossattention.masked_bias\", \"decoder.transformer.h.2.crossattention.c_attn.weight\", \"decoder.transformer.h.2.crossattention.c_attn.bias\", \"decoder.transformer.h.2.crossattention.q_attn.weight\", \"decoder.transformer.h.2.crossattention.q_attn.bias\", \"decoder.transformer.h.2.crossattention.c_proj.weight\", \"decoder.transformer.h.2.crossattention.c_proj.bias\", \"decoder.transformer.h.2.ln_cross_attn.weight\", \"decoder.transformer.h.2.ln_cross_attn.bias\", \"decoder.transformer.h.2.mlp.c_fc.weight\", \"decoder.transformer.h.2.mlp.c_fc.bias\", \"decoder.transformer.h.2.mlp.c_proj.weight\", \"decoder.transformer.h.2.mlp.c_proj.bias\", \"decoder.transformer.h.3.ln_1.weight\", \"decoder.transformer.h.3.ln_1.bias\", \"decoder.transformer.h.3.attn.bias\", \"decoder.transformer.h.3.attn.masked_bias\", \"decoder.transformer.h.3.attn.c_attn.weight\", \"decoder.transformer.h.3.attn.c_attn.bias\", \"decoder.transformer.h.3.attn.c_proj.weight\", \"decoder.transformer.h.3.attn.c_proj.bias\", \"decoder.transformer.h.3.ln_2.weight\", \"decoder.transformer.h.3.ln_2.bias\", \"decoder.transformer.h.3.crossattention.bias\", \"decoder.transformer.h.3.crossattention.masked_bias\", \"decoder.transformer.h.3.crossattention.c_attn.weight\", \"decoder.transformer.h.3.crossattention.c_attn.bias\", \"decoder.transformer.h.3.crossattention.q_attn.weight\", \"decoder.transformer.h.3.crossattention.q_attn.bias\", \"decoder.transformer.h.3.crossattention.c_proj.weight\", \"decoder.transformer.h.3.crossattention.c_proj.bias\", \"decoder.transformer.h.3.ln_cross_attn.weight\", \"decoder.transformer.h.3.ln_cross_attn.bias\", \"decoder.transformer.h.3.mlp.c_fc.weight\", \"decoder.transformer.h.3.mlp.c_fc.bias\", \"decoder.transformer.h.3.mlp.c_proj.weight\", \"decoder.transformer.h.3.mlp.c_proj.bias\", \"decoder.transformer.h.4.ln_1.weight\", \"decoder.transformer.h.4.ln_1.bias\", \"decoder.transformer.h.4.attn.bias\", \"decoder.transformer.h.4.attn.masked_bias\", \"decoder.transformer.h.4.attn.c_attn.weight\", \"decoder.transformer.h.4.attn.c_attn.bias\", \"decoder.transformer.h.4.attn.c_proj.weight\", \"decoder.transformer.h.4.attn.c_proj.bias\", \"decoder.transformer.h.4.ln_2.weight\", \"decoder.transformer.h.4.ln_2.bias\", \"decoder.transformer.h.4.crossattention.bias\", \"decoder.transformer.h.4.crossattention.masked_bias\", \"decoder.transformer.h.4.crossattention.c_attn.weight\", \"decoder.transformer.h.4.crossattention.c_attn.bias\", \"decoder.transformer.h.4.crossattention.q_attn.weight\", \"decoder.transformer.h.4.crossattention.q_attn.bias\", \"decoder.transformer.h.4.crossattention.c_proj.weight\", \"decoder.transformer.h.4.crossattention.c_proj.bias\", \"decoder.transformer.h.4.ln_cross_attn.weight\", \"decoder.transformer.h.4.ln_cross_attn.bias\", \"decoder.transformer.h.4.mlp.c_fc.weight\", \"decoder.transformer.h.4.mlp.c_fc.bias\", \"decoder.transformer.h.4.mlp.c_proj.weight\", \"decoder.transformer.h.4.mlp.c_proj.bias\", \"decoder.transformer.h.5.ln_1.weight\", \"decoder.transformer.h.5.ln_1.bias\", \"decoder.transformer.h.5.attn.bias\", \"decoder.transformer.h.5.attn.masked_bias\", \"decoder.transformer.h.5.attn.c_attn.weight\", \"decoder.transformer.h.5.attn.c_attn.bias\", \"decoder.transformer.h.5.attn.c_proj.weight\", \"decoder.transformer.h.5.attn.c_proj.bias\", \"decoder.transformer.h.5.ln_2.weight\", \"decoder.transformer.h.5.ln_2.bias\", \"decoder.transformer.h.5.crossattention.bias\", \"decoder.transformer.h.5.crossattention.masked_bias\", \"decoder.transformer.h.5.crossattention.c_attn.weight\", \"decoder.transformer.h.5.crossattention.c_attn.bias\", \"decoder.transformer.h.5.crossattention.q_attn.weight\", \"decoder.transformer.h.5.crossattention.q_attn.bias\", \"decoder.transformer.h.5.crossattention.c_proj.weight\", \"decoder.transformer.h.5.crossattention.c_proj.bias\", \"decoder.transformer.h.5.ln_cross_attn.weight\", \"decoder.transformer.h.5.ln_cross_attn.bias\", \"decoder.transformer.h.5.mlp.c_fc.weight\", \"decoder.transformer.h.5.mlp.c_fc.bias\", \"decoder.transformer.h.5.mlp.c_proj.weight\", \"decoder.transformer.h.5.mlp.c_proj.bias\", \"decoder.transformer.h.6.ln_1.weight\", \"decoder.transformer.h.6.ln_1.bias\", \"decoder.transformer.h.6.attn.bias\", \"decoder.transformer.h.6.attn.masked_bias\", \"decoder.transformer.h.6.attn.c_attn.weight\", \"decoder.transformer.h.6.attn.c_attn.bias\", \"decoder.transformer.h.6.attn.c_proj.weight\", \"decoder.transformer.h.6.attn.c_proj.bias\", \"decoder.transformer.h.6.ln_2.weight\", \"decoder.transformer.h.6.ln_2.bias\", \"decoder.transformer.h.6.crossattention.bias\", \"decoder.transformer.h.6.crossattention.masked_bias\", \"decoder.transformer.h.6.crossattention.c_attn.weight\", \"decoder.transformer.h.6.crossattention.c_attn.bias\", \"decoder.transformer.h.6.crossattention.q_attn.weight\", \"decoder.transformer.h.6.crossattention.q_attn.bias\", \"decoder.transformer.h.6.crossattention.c_proj.weight\", \"decoder.transformer.h.6.crossattention.c_proj.bias\", \"decoder.transformer.h.6.ln_cross_attn.weight\", \"decoder.transformer.h.6.ln_cross_attn.bias\", \"decoder.transformer.h.6.mlp.c_fc.weight\", \"decoder.transformer.h.6.mlp.c_fc.bias\", \"decoder.transformer.h.6.mlp.c_proj.weight\", \"decoder.transformer.h.6.mlp.c_proj.bias\", \"decoder.transformer.h.7.ln_1.weight\", \"decoder.transformer.h.7.ln_1.bias\", \"decoder.transformer.h.7.attn.bias\", \"decoder.transformer.h.7.attn.masked_bias\", \"decoder.transformer.h.7.attn.c_attn.weight\", \"decoder.transformer.h.7.attn.c_attn.bias\", \"decoder.transformer.h.7.attn.c_proj.weight\", \"decoder.transformer.h.7.attn.c_proj.bias\", \"decoder.transformer.h.7.ln_2.weight\", \"decoder.transformer.h.7.ln_2.bias\", \"decoder.transformer.h.7.crossattention.bias\", \"decoder.transformer.h.7.crossattention.masked_bias\", \"decoder.transformer.h.7.crossattention.c_attn.weight\", \"decoder.transformer.h.7.crossattention.c_attn.bias\", \"decoder.transformer.h.7.crossattention.q_attn.weight\", \"decoder.transformer.h.7.crossattention.q_attn.bias\", \"decoder.transformer.h.7.crossattention.c_proj.weight\", \"decoder.transformer.h.7.crossattention.c_proj.bias\", \"decoder.transformer.h.7.ln_cross_attn.weight\", \"decoder.transformer.h.7.ln_cross_attn.bias\", \"decoder.transformer.h.7.mlp.c_fc.weight\", \"decoder.transformer.h.7.mlp.c_fc.bias\", \"decoder.transformer.h.7.mlp.c_proj.weight\", \"decoder.transformer.h.7.mlp.c_proj.bias\", \"decoder.transformer.h.8.ln_1.weight\", \"decoder.transformer.h.8.ln_1.bias\", \"decoder.transformer.h.8.attn.bias\", \"decoder.transformer.h.8.attn.masked_bias\", \"decoder.transformer.h.8.attn.c_attn.weight\", \"decoder.transformer.h.8.attn.c_attn.bias\", \"decoder.transformer.h.8.attn.c_proj.weight\", \"decoder.transformer.h.8.attn.c_proj.bias\", \"decoder.transformer.h.8.ln_2.weight\", \"decoder.transformer.h.8.ln_2.bias\", \"decoder.transformer.h.8.crossattention.bias\", \"decoder.transformer.h.8.crossattention.masked_bias\", \"decoder.transformer.h.8.crossattention.c_attn.weight\", \"decoder.transformer.h.8.crossattention.c_attn.bias\", \"decoder.transformer.h.8.crossattention.q_attn.weight\", \"decoder.transformer.h.8.crossattention.q_attn.bias\", \"decoder.transformer.h.8.crossattention.c_proj.weight\", \"decoder.transformer.h.8.crossattention.c_proj.bias\", \"decoder.transformer.h.8.ln_cross_attn.weight\", \"decoder.transformer.h.8.ln_cross_attn.bias\", \"decoder.transformer.h.8.mlp.c_fc.weight\", \"decoder.transformer.h.8.mlp.c_fc.bias\", \"decoder.transformer.h.8.mlp.c_proj.weight\", \"decoder.transformer.h.8.mlp.c_proj.bias\", \"decoder.transformer.h.9.ln_1.weight\", \"decoder.transformer.h.9.ln_1.bias\", \"decoder.transformer.h.9.attn.bias\", \"decoder.transformer.h.9.attn.masked_bias\", \"decoder.transformer.h.9.attn.c_attn.weight\", \"decoder.transformer.h.9.attn.c_attn.bias\", \"decoder.transformer.h.9.attn.c_proj.weight\", \"decoder.transformer.h.9.attn.c_proj.bias\", \"decoder.transformer.h.9.ln_2.weight\", \"decoder.transformer.h.9.ln_2.bias\", \"decoder.transformer.h.9.crossattention.bias\", \"decoder.transformer.h.9.crossattention.masked_bias\", \"decoder.transformer.h.9.crossattention.c_attn.weight\", \"decoder.transformer.h.9.crossattention.c_attn.bias\", \"decoder.transformer.h.9.crossattention.q_attn.weight\", \"decoder.transformer.h.9.crossattention.q_attn.bias\", \"decoder.transformer.h.9.crossattention.c_proj.weight\", \"decoder.transformer.h.9.crossattention.c_proj.bias\", \"decoder.transformer.h.9.ln_cross_attn.weight\", \"decoder.transformer.h.9.ln_cross_attn.bias\", \"decoder.transformer.h.9.mlp.c_fc.weight\", \"decoder.transformer.h.9.mlp.c_fc.bias\", \"decoder.transformer.h.9.mlp.c_proj.weight\", \"decoder.transformer.h.9.mlp.c_proj.bias\", \"decoder.transformer.h.10.ln_1.weight\", \"decoder.transformer.h.10.ln_1.bias\", \"decoder.transformer.h.10.attn.bias\", \"decoder.transformer.h.10.attn.masked_bias\", \"decoder.transformer.h.10.attn.c_attn.weight\", \"decoder.transformer.h.10.attn.c_attn.bias\", \"decoder.transformer.h.10.attn.c_proj.weight\", \"decoder.transformer.h.10.attn.c_proj.bias\", \"decoder.transformer.h.10.ln_2.weight\", \"decoder.transformer.h.10.ln_2.bias\", \"decoder.transformer.h.10.crossattention.bias\", \"decoder.transformer.h.10.crossattention.masked_bias\", \"decoder.transformer.h.10.crossattention.c_attn.weight\", \"decoder.transformer.h.10.crossattention.c_attn.bias\", \"decoder.transformer.h.10.crossattention.q_attn.weight\", \"decoder.transformer.h.10.crossattention.q_attn.bias\", \"decoder.transformer.h.10.crossattention.c_proj.weight\", \"decoder.transformer.h.10.crossattention.c_proj.bias\", \"decoder.transformer.h.10.ln_cross_attn.weight\", \"decoder.transformer.h.10.ln_cross_attn.bias\", \"decoder.transformer.h.10.mlp.c_fc.weight\", \"decoder.transformer.h.10.mlp.c_fc.bias\", \"decoder.transformer.h.10.mlp.c_proj.weight\", \"decoder.transformer.h.10.mlp.c_proj.bias\", \"decoder.transformer.h.11.ln_1.weight\", \"decoder.transformer.h.11.ln_1.bias\", \"decoder.transformer.h.11.attn.bias\", \"decoder.transformer.h.11.attn.masked_bias\", \"decoder.transformer.h.11.attn.c_attn.weight\", \"decoder.transformer.h.11.attn.c_attn.bias\", \"decoder.transformer.h.11.attn.c_proj.weight\", \"decoder.transformer.h.11.attn.c_proj.bias\", \"decoder.transformer.h.11.ln_2.weight\", \"decoder.transformer.h.11.ln_2.bias\", \"decoder.transformer.h.11.crossattention.bias\", \"decoder.transformer.h.11.crossattention.masked_bias\", \"decoder.transformer.h.11.crossattention.c_attn.weight\", \"decoder.transformer.h.11.crossattention.c_attn.bias\", \"decoder.transformer.h.11.crossattention.q_attn.weight\", \"decoder.transformer.h.11.crossattention.q_attn.bias\", \"decoder.transformer.h.11.crossattention.c_proj.weight\", \"decoder.transformer.h.11.crossattention.c_proj.bias\", \"decoder.transformer.h.11.ln_cross_attn.weight\", \"decoder.transformer.h.11.ln_cross_attn.bias\", \"decoder.transformer.h.11.mlp.c_fc.weight\", \"decoder.transformer.h.11.mlp.c_fc.bias\", \"decoder.transformer.h.11.mlp.c_proj.weight\", \"decoder.transformer.h.11.mlp.c_proj.bias\", \"decoder.transformer.ln_f.weight\", \"decoder.transformer.ln_f.bias\", \"decoder.lm_head.weight\", \"pos_enb.pe\", \"embedder.0.weight\", \"embedder.0.bias\", \"embedder.2.weight\", \"embedder.2.bias\", \"embedder.2.running_mean\", \"embedder.2.running_var\", \"embedder.3.weight\", \"embedder.3.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.wte.weight\", \"transformer.wpe.weight\", \"transformer.h.0.ln_1.weight\", \"transformer.h.0.ln_1.bias\", \"transformer.h.0.attn.bias\", \"transformer.h.0.attn.masked_bias\", \"transformer.h.0.attn.c_attn.weight\", \"transformer.h.0.attn.c_attn.bias\", \"transformer.h.0.attn.c_proj.weight\", \"transformer.h.0.attn.c_proj.bias\", \"transformer.h.0.ln_2.weight\", \"transformer.h.0.ln_2.bias\", \"transformer.h.0.crossattention.bias\", \"transformer.h.0.crossattention.masked_bias\", \"transformer.h.0.crossattention.c_attn.weight\", \"transformer.h.0.crossattention.c_attn.bias\", \"transformer.h.0.crossattention.q_attn.weight\", \"transformer.h.0.crossattention.q_attn.bias\", \"transformer.h.0.crossattention.c_proj.weight\", \"transformer.h.0.crossattention.c_proj.bias\", \"transformer.h.0.ln_cross_attn.weight\", \"transformer.h.0.ln_cross_attn.bias\", \"transformer.h.0.mlp.c_fc.weight\", \"transformer.h.0.mlp.c_fc.bias\", \"transformer.h.0.mlp.c_proj.weight\", \"transformer.h.0.mlp.c_proj.bias\", \"transformer.h.1.ln_1.weight\", \"transformer.h.1.ln_1.bias\", \"transformer.h.1.attn.bias\", \"transformer.h.1.attn.masked_bias\", \"transformer.h.1.attn.c_attn.weight\", \"transformer.h.1.attn.c_attn.bias\", \"transformer.h.1.attn.c_proj.weight\", \"transformer.h.1.attn.c_proj.bias\", \"transformer.h.1.ln_2.weight\", \"transformer.h.1.ln_2.bias\", \"transformer.h.1.crossattention.bias\", \"transformer.h.1.crossattention.masked_bias\", \"transformer.h.1.crossattention.c_attn.weight\", \"transformer.h.1.crossattention.c_attn.bias\", \"transformer.h.1.crossattention.q_attn.weight\", \"transformer.h.1.crossattention.q_attn.bias\", \"transformer.h.1.crossattention.c_proj.weight\", \"transformer.h.1.crossattention.c_proj.bias\", \"transformer.h.1.ln_cross_attn.weight\", \"transformer.h.1.ln_cross_attn.bias\", \"transformer.h.1.mlp.c_fc.weight\", \"transformer.h.1.mlp.c_fc.bias\", \"transformer.h.1.mlp.c_proj.weight\", \"transformer.h.1.mlp.c_proj.bias\", \"transformer.h.2.ln_1.weight\", \"transformer.h.2.ln_1.bias\", \"transformer.h.2.attn.bias\", \"transformer.h.2.attn.masked_bias\", \"transformer.h.2.attn.c_attn.weight\", \"transformer.h.2.attn.c_attn.bias\", \"transformer.h.2.attn.c_proj.weight\", \"transformer.h.2.attn.c_proj.bias\", \"transformer.h.2.ln_2.weight\", \"transformer.h.2.ln_2.bias\", \"transformer.h.2.crossattention.bias\", \"transformer.h.2.crossattention.masked_bias\", \"transformer.h.2.crossattention.c_attn.weight\", \"transformer.h.2.crossattention.c_attn.bias\", \"transformer.h.2.crossattention.q_attn.weight\", \"transformer.h.2.crossattention.q_attn.bias\", \"transformer.h.2.crossattention.c_proj.weight\", \"transformer.h.2.crossattention.c_proj.bias\", \"transformer.h.2.ln_cross_attn.weight\", \"transformer.h.2.ln_cross_attn.bias\", \"transformer.h.2.mlp.c_fc.weight\", \"transformer.h.2.mlp.c_fc.bias\", \"transformer.h.2.mlp.c_proj.weight\", \"transformer.h.2.mlp.c_proj.bias\", \"transformer.h.3.ln_1.weight\", \"transformer.h.3.ln_1.bias\", \"transformer.h.3.attn.bias\", \"transformer.h.3.attn.masked_bias\", \"transformer.h.3.attn.c_attn.weight\", \"transformer.h.3.attn.c_attn.bias\", \"transformer.h.3.attn.c_proj.weight\", \"transformer.h.3.attn.c_proj.bias\", \"transformer.h.3.ln_2.weight\", \"transformer.h.3.ln_2.bias\", \"transformer.h.3.crossattention.bias\", \"transformer.h.3.crossattention.masked_bias\", \"transformer.h.3.crossattention.c_attn.weight\", \"transformer.h.3.crossattention.c_attn.bias\", \"transformer.h.3.crossattention.q_attn.weight\", \"transformer.h.3.crossattention.q_attn.bias\", \"transformer.h.3.crossattention.c_proj.weight\", \"transformer.h.3.crossattention.c_proj.bias\", \"transformer.h.3.ln_cross_attn.weight\", \"transformer.h.3.ln_cross_attn.bias\", \"transformer.h.3.mlp.c_fc.weight\", \"transformer.h.3.mlp.c_fc.bias\", \"transformer.h.3.mlp.c_proj.weight\", \"transformer.h.3.mlp.c_proj.bias\", \"transformer.h.4.ln_1.weight\", \"transformer.h.4.ln_1.bias\", \"transformer.h.4.attn.bias\", \"transformer.h.4.attn.masked_bias\", \"transformer.h.4.attn.c_attn.weight\", \"transformer.h.4.attn.c_attn.bias\", \"transformer.h.4.attn.c_proj.weight\", \"transformer.h.4.attn.c_proj.bias\", \"transformer.h.4.ln_2.weight\", \"transformer.h.4.ln_2.bias\", \"transformer.h.4.crossattention.bias\", \"transformer.h.4.crossattention.masked_bias\", \"transformer.h.4.crossattention.c_attn.weight\", \"transformer.h.4.crossattention.c_attn.bias\", \"transformer.h.4.crossattention.q_attn.weight\", \"transformer.h.4.crossattention.q_attn.bias\", \"transformer.h.4.crossattention.c_proj.weight\", \"transformer.h.4.crossattention.c_proj.bias\", \"transformer.h.4.ln_cross_attn.weight\", \"transformer.h.4.ln_cross_attn.bias\", \"transformer.h.4.mlp.c_fc.weight\", \"transformer.h.4.mlp.c_fc.bias\", \"transformer.h.4.mlp.c_proj.weight\", \"transformer.h.4.mlp.c_proj.bias\", \"transformer.h.5.ln_1.weight\", \"transformer.h.5.ln_1.bias\", \"transformer.h.5.attn.bias\", \"transformer.h.5.attn.masked_bias\", \"transformer.h.5.attn.c_attn.weight\", \"transformer.h.5.attn.c_attn.bias\", \"transformer.h.5.attn.c_proj.weight\", \"transformer.h.5.attn.c_proj.bias\", \"transformer.h.5.ln_2.weight\", \"transformer.h.5.ln_2.bias\", \"transformer.h.5.crossattention.bias\", \"transformer.h.5.crossattention.masked_bias\", \"transformer.h.5.crossattention.c_attn.weight\", \"transformer.h.5.crossattention.c_attn.bias\", \"transformer.h.5.crossattention.q_attn.weight\", \"transformer.h.5.crossattention.q_attn.bias\", \"transformer.h.5.crossattention.c_proj.weight\", \"transformer.h.5.crossattention.c_proj.bias\", \"transformer.h.5.ln_cross_attn.weight\", \"transformer.h.5.ln_cross_attn.bias\", \"transformer.h.5.mlp.c_fc.weight\", \"transformer.h.5.mlp.c_fc.bias\", \"transformer.h.5.mlp.c_proj.weight\", \"transformer.h.5.mlp.c_proj.bias\", \"transformer.h.6.ln_1.weight\", \"transformer.h.6.ln_1.bias\", \"transformer.h.6.attn.bias\", \"transformer.h.6.attn.masked_bias\", \"transformer.h.6.attn.c_attn.weight\", \"transformer.h.6.attn.c_attn.bias\", \"transformer.h.6.attn.c_proj.weight\", \"transformer.h.6.attn.c_proj.bias\", \"transformer.h.6.ln_2.weight\", \"transformer.h.6.ln_2.bias\", \"transformer.h.6.crossattention.bias\", \"transformer.h.6.crossattention.masked_bias\", \"transformer.h.6.crossattention.c_attn.weight\", \"transformer.h.6.crossattention.c_attn.bias\", \"transformer.h.6.crossattention.q_attn.weight\", \"transformer.h.6.crossattention.q_attn.bias\", \"transformer.h.6.crossattention.c_proj.weight\", \"transformer.h.6.crossattention.c_proj.bias\", \"transformer.h.6.ln_cross_attn.weight\", \"transformer.h.6.ln_cross_attn.bias\", \"transformer.h.6.mlp.c_fc.weight\", \"transformer.h.6.mlp.c_fc.bias\", \"transformer.h.6.mlp.c_proj.weight\", \"transformer.h.6.mlp.c_proj.bias\", \"transformer.h.7.ln_1.weight\", \"transformer.h.7.ln_1.bias\", \"transformer.h.7.attn.bias\", \"transformer.h.7.attn.masked_bias\", \"transformer.h.7.attn.c_attn.weight\", \"transformer.h.7.attn.c_attn.bias\", \"transformer.h.7.attn.c_proj.weight\", \"transformer.h.7.attn.c_proj.bias\", \"transformer.h.7.ln_2.weight\", \"transformer.h.7.ln_2.bias\", \"transformer.h.7.crossattention.bias\", \"transformer.h.7.crossattention.masked_bias\", \"transformer.h.7.crossattention.c_attn.weight\", \"transformer.h.7.crossattention.c_attn.bias\", \"transformer.h.7.crossattention.q_attn.weight\", \"transformer.h.7.crossattention.q_attn.bias\", \"transformer.h.7.crossattention.c_proj.weight\", \"transformer.h.7.crossattention.c_proj.bias\", \"transformer.h.7.ln_cross_attn.weight\", \"transformer.h.7.ln_cross_attn.bias\", \"transformer.h.7.mlp.c_fc.weight\", \"transformer.h.7.mlp.c_fc.bias\", \"transformer.h.7.mlp.c_proj.weight\", \"transformer.h.7.mlp.c_proj.bias\", \"transformer.h.8.ln_1.weight\", \"transformer.h.8.ln_1.bias\", \"transformer.h.8.attn.bias\", \"transformer.h.8.attn.masked_bias\", \"transformer.h.8.attn.c_attn.weight\", \"transformer.h.8.attn.c_attn.bias\", \"transformer.h.8.attn.c_proj.weight\", \"transformer.h.8.attn.c_proj.bias\", \"transformer.h.8.ln_2.weight\", \"transformer.h.8.ln_2.bias\", \"transformer.h.8.crossattention.bias\", \"transformer.h.8.crossattention.masked_bias\", \"transformer.h.8.crossattention.c_attn.weight\", \"transformer.h.8.crossattention.c_attn.bias\", \"transformer.h.8.crossattention.q_attn.weight\", \"transformer.h.8.crossattention.q_attn.bias\", \"transformer.h.8.crossattention.c_proj.weight\", \"transformer.h.8.crossattention.c_proj.bias\", \"transformer.h.8.ln_cross_attn.weight\", \"transformer.h.8.ln_cross_attn.bias\", \"transformer.h.8.mlp.c_fc.weight\", \"transformer.h.8.mlp.c_fc.bias\", \"transformer.h.8.mlp.c_proj.weight\", \"transformer.h.8.mlp.c_proj.bias\", \"transformer.h.9.ln_1.weight\", \"transformer.h.9.ln_1.bias\", \"transformer.h.9.attn.bias\", \"transformer.h.9.attn.masked_bias\", \"transformer.h.9.attn.c_attn.weight\", \"transformer.h.9.attn.c_attn.bias\", \"transformer.h.9.attn.c_proj.weight\", \"transformer.h.9.attn.c_proj.bias\", \"transformer.h.9.ln_2.weight\", \"transformer.h.9.ln_2.bias\", \"transformer.h.9.crossattention.bias\", \"transformer.h.9.crossattention.masked_bias\", \"transformer.h.9.crossattention.c_attn.weight\", \"transformer.h.9.crossattention.c_attn.bias\", \"transformer.h.9.crossattention.q_attn.weight\", \"transformer.h.9.crossattention.q_attn.bias\", \"transformer.h.9.crossattention.c_proj.weight\", \"transformer.h.9.crossattention.c_proj.bias\", \"transformer.h.9.ln_cross_attn.weight\", \"transformer.h.9.ln_cross_attn.bias\", \"transformer.h.9.mlp.c_fc.weight\", \"transformer.h.9.mlp.c_fc.bias\", \"transformer.h.9.mlp.c_proj.weight\", \"transformer.h.9.mlp.c_proj.bias\", \"transformer.h.10.ln_1.weight\", \"transformer.h.10.ln_1.bias\", \"transformer.h.10.attn.bias\", \"transformer.h.10.attn.masked_bias\", \"transformer.h.10.attn.c_attn.weight\", \"transformer.h.10.attn.c_attn.bias\", \"transformer.h.10.attn.c_proj.weight\", \"transformer.h.10.attn.c_proj.bias\", \"transformer.h.10.ln_2.weight\", \"transformer.h.10.ln_2.bias\", \"transformer.h.10.crossattention.bias\", \"transformer.h.10.crossattention.masked_bias\", \"transformer.h.10.crossattention.c_attn.weight\", \"transformer.h.10.crossattention.c_attn.bias\", \"transformer.h.10.crossattention.q_attn.weight\", \"transformer.h.10.crossattention.q_attn.bias\", \"transformer.h.10.crossattention.c_proj.weight\", \"transformer.h.10.crossattention.c_proj.bias\", \"transformer.h.10.ln_cross_attn.weight\", \"transformer.h.10.ln_cross_attn.bias\", \"transformer.h.10.mlp.c_fc.weight\", \"transformer.h.10.mlp.c_fc.bias\", \"transformer.h.10.mlp.c_proj.weight\", \"transformer.h.10.mlp.c_proj.bias\", \"transformer.h.11.ln_1.weight\", \"transformer.h.11.ln_1.bias\", \"transformer.h.11.attn.bias\", \"transformer.h.11.attn.masked_bias\", \"transformer.h.11.attn.c_attn.weight\", \"transformer.h.11.attn.c_attn.bias\", \"transformer.h.11.attn.c_proj.weight\", \"transformer.h.11.attn.c_proj.bias\", \"transformer.h.11.ln_2.weight\", \"transformer.h.11.ln_2.bias\", \"transformer.h.11.crossattention.bias\", \"transformer.h.11.crossattention.masked_bias\", \"transformer.h.11.crossattention.c_attn.weight\", \"transformer.h.11.crossattention.c_attn.bias\", \"transformer.h.11.crossattention.q_attn.weight\", \"transformer.h.11.crossattention.q_attn.bias\", \"transformer.h.11.crossattention.c_proj.weight\", \"transformer.h.11.crossattention.c_proj.bias\", \"transformer.h.11.ln_cross_attn.weight\", \"transformer.h.11.ln_cross_attn.bias\", \"transformer.h.11.mlp.c_fc.weight\", \"transformer.h.11.mlp.c_fc.bias\", \"transformer.h.11.mlp.c_proj.weight\", \"transformer.h.11.mlp.c_proj.bias\", \"transformer.ln_f.weight\", \"transformer.ln_f.bias\", \"lm_head.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-b4ddf86144a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menc_dec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/gpt2_enc_dec.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1052\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustEncoderDecoder:\n\tMissing key(s) in state_dict: \"encoder.layers.0.feed_forward.linear_1.weight\", \"encoder.layers.0.feed_forward.linear_1.bias\", \"encoder.layers.0.feed_forward.linear_2.weight\", \"encoder.layers.0.feed_forward.linear_2.bias\", \"encoder.layers.0.feed_forward.norm_1.weight\", \"encoder.layers.0.feed_forward.norm_1.bias\", \"encoder.layers.0.norm_1.weight\", \"encoder.layers.0.norm_1.bias\", \"encoder.layers.1.feed_forward.linear_1.weight\", \"encoder.layers.1.feed_forward.linear_1.bias\", \"encoder.layers.1.feed_forward.linear_2.weight\", \"encoder.layers.1.feed_forward.linear_2.bias\", \"encoder.layers.1.feed_forward.norm_1.weight\", \"encoder.layers.1.feed_forward.norm_1.bias\", \"encoder.layers.1.norm_1.weight\", \"encoder.layers.1.norm_1.bias\", \"encoder.layers.2.feed_forward.linear_1.weight\", \"encoder.layers.2.feed_forward.linear_1.bias\", \"encoder.layers.2.feed_forward.linear_2.weight\", \"encoder.layers.2.feed_forward.linear_2.bias\", \"encoder.layers.2.feed_forward.norm_1.weight\", \"encoder.layers.2.feed_forward.norm_1.bias\", \"encoder.layers.2.norm_1.weight\", \"encoder.layers.2.norm_1.bias\", \"encoder.layers.3.feed_forward.linear_1.weight\", \"encoder.layers.3.feed_forward.linear_1.bias\", \"encoder.layers.3.feed_forward.linear_2.weight\", \"encoder.layers.3.feed_forward.linear_2.bias\", \"encoder.layers.3.feed_forward.norm_1.weight\", \"encoder.layers.3.feed_forward.norm_1.bias\", \"encoder.layers.3.norm_1.weight\", \"encoder.layers.3.norm_1.bias\", \"encoder.layers.4.feed_forward.linear_1.weight\", \"encoder.layers.4.feed_forward.linear_1.bias\", \"encoder.layers.4.feed_forward.linear_2.weight\", \"encoder.layers.4.feed_forward.linear_2.bias\", \"encoder.layers.4.feed_forward.norm_1.weight\", \"encoder.layers.4.feed_forward.norm_1.bias\", \"encoder.layers.4.norm_1.weight\", \"encoder.layers.4.norm_1.bias\", \"encoder.layers.5.feed_forward.linear_1.weight\", \"encoder.layers.5.feed_forward.linear_1.bias\", \"encoder.layers.5.feed_forward.linear_2.weight\", \"encoder.layers.5.feed_forward.linear_2.bias\", \"encoder.layers.5.feed_forward.norm_1.weight\", \"encoder.layers.5.feed_forward.norm_1.bias\", \"encoder.layers.5.norm_1.weight\", \"encoder.layers.5.norm_1.bias\", \"decoder.transformer.wte.weight\", \"decoder.transformer.wpe.weight\", \"decoder.transformer.h.0.ln_1.weight\", \"decoder.transformer.h.0.ln_1.bias\", \"decoder.transformer.h.0.attn.bias\", \"decoder.transformer.h.0.attn.masked_bias\", \"decoder.transformer.h.0.attn.c_attn.weight\", \"decoder.transformer.h.0.attn.c_attn.bias\", \"decoder.transformer.h.0.attn.c_proj.weight\", \"decoder.transformer.h.0.attn.c_proj.bias\", \"decoder.transformer.h.0.ln_2.weight\", \"decoder.transformer.h.0.ln_2.bias\", \"decoder.transformer.h.0.crossattention.bias\", \"decoder.transformer.h.0.crossattention.masked_bias\", \"decoder.transformer.h.0.crossattention.c_attn.weight\", \"decoder.transformer.h.0.crossattention.c_attn.bias\", \"decoder.transformer.h.0.crossattention.q_attn.weight\", \"decoder.transformer.h.0.crossattention.q_attn.bias\", \"decoder.transformer.h.0.crossattention.c_proj.weight\", \"decoder.transformer.h.0.crossattention.c_proj.bias\", \"decoder.transformer.h.0.ln_cross_attn.weight\", \"decoder.transformer.h.0.ln_cross_attn.bias\", \"decoder.transformer.h.0.mlp.c_fc.weight\", \"decoder.transformer.h.0.mlp.c_fc.bias\", \"decoder.transformer.h.0.mlp.c_proj.weight\", \"decoder.transformer.h.0.mlp.c_proj.bias\", \"decoder.transformer.h.1.ln_1.weight\", \"decoder.transformer.h.1.ln_1.bias\", \"decoder.transformer.h.1.attn.bias\", \"decoder.transformer.h.1.attn.masked_bias\", \"decoder.transformer.h.1.attn.c_attn.weight\", \"decoder.transformer.h.1.attn.c_attn.bias\", \"decoder.transformer.h.1.attn.c_proj.weight\", \"decoder.transformer.h.1.attn.c_proj.bias\", \"decoder.transformer.h.1.ln_2.weight\", \"decoder.transformer.h.1.ln_2.bias\", \"decoder.transformer.h.1.crossattention.bias\", \"decoder.transformer.h.1.crossattention.masked_bias\", \"decoder.transformer.h.1.crossattention.c_attn.weight\", \"decoder.transformer.h.1.crossattention.c_attn.bias\", \"decoder.transformer.h.1.crossattention.q_attn.weight\", \"decoder.transformer.h.1.crossattention.q_attn.bias\", \"decoder.transformer.h.1.crossattention.c_proj.weight\", \"decoder.transformer.h.1.crossattention.c_proj.bias\", \"decoder.transformer.h.1.ln_cross_attn.weight\", \"decoder.transformer.h.1.ln_cross_attn.bias\", \"decoder.transformer.h.1.mlp.c_fc.weight\", \"decoder.transformer.h.1.mlp.c_fc.bias\", \"decoder.transformer.h.1.mlp.c_proj.weight\", \"decoder.transformer.h.1.mlp.c_proj.bias\", \"decoder.transformer.h.2.ln_1.weight\", \"decoder.transformer.h.2.ln_1.bias\", \"decoder.transformer.h.2.attn.bias\", \"decoder.transformer.h.2.attn.masked_bias\", \"decoder.transformer.h.2.attn.c_attn.weight\", \"decoder.transformer.h.2.attn.c_attn.bias\", \"decoder.transformer.h.2.attn.c_proj.weight\", \"decoder.transformer.h.2.attn.c_proj.bias\", \"decoder.transformer.h.2.ln_2.weight\", \"decoder.transformer.h.2.ln_2.bias\", \"decoder.transformer.h.2.crossattention.bias\", \"decoder.transformer.h.2.crossattention.masked_bias\", \"decoder.transformer.h.2.crossattention.c_attn.weight\", \"decoder.transformer.h.2.crossattention.c_attn.bias\", \"decoder.transformer.h.2.crossattention.q_attn.weight\", \"decoder.transformer.h.2.crossattention.q_attn.bias\", \"decoder.transformer.h.2.crossattention.c_proj.weight\", \"decoder.transformer.h.2.crossattention.c_proj.bias\", \"decoder.transformer.h.2.ln_cross_attn.weight\", \"decoder.transformer.h.2.ln_cross_attn.bias\", \"decoder.transformer.h.2.mlp.c_fc.weight\", \"decoder.transformer.h.2.mlp.c_fc.bias\", \"decoder.transformer.h.2.mlp.c_proj.weight\", \"decoder.transformer.h.2.mlp.c_proj.bias\", \"decoder.transformer.h.3.ln_1.weight\", \"decoder.transformer.h.3.ln_1.bias\", \"decoder.transformer.h.3.attn.bias\", \"decoder.transformer.h.3.attn.masked_bias\", \"decoder.transformer.h.3.attn.c_attn.weight\", \"decoder.transformer.h.3.attn.c_attn.bias\", \"decoder.transformer.h.3.attn.c_proj.weight\", \"decoder.transformer.h.3.attn.c_proj.bias\", \"decoder.transformer.h.3.ln_2.weight\", \"decoder.transformer.h.3.ln_2.bias\", \"decoder.transformer.h.3.crossattention.bias\", \"decoder.transformer.h.3.crossattention.masked_bias\", \"decoder.transformer.h.3.crossattention.c_attn.weight\", \"decoder.transformer.h.3.crossattention.c_attn.bias\", \"decoder.transformer.h.3.crossattention.q_attn.weight\", \"decoder.transformer.h.3.crossattention.q_attn.bias\", \"decoder.transformer.h.3.crossattention.c_proj.weight\", \"decoder.transformer.h.3.crossattention.c_proj.bias\", \"decoder.transformer.h.3.ln_cross_attn.weight\", \"decoder.transformer.h.3.ln_cross_attn.bias\", \"decoder.transformer.h.3.mlp.c_fc.weight\", \"decoder.transformer.h.3.mlp.c_fc.bias\", \"decoder.transformer.h.3.mlp.c_proj.weight\", \"decoder.transformer.h.3.mlp.c_proj.bias\", \"decoder.transformer.h.4.ln_1.weight\", \"decoder.transformer.h.4.ln_1.bias\", \"decoder.transformer.h.4.attn.bias\", \"decoder.transformer.h.4.attn.masked_bias\", \"decoder.transformer.h.4.attn.c_attn.weight\", \"decoder.transformer.h.4.attn.c_attn.bias\", \"decoder.transformer.h.4.attn.c_proj.weight\", \"decoder.transformer.h.4.attn.c_proj.bias\", \"decoder.transformer.h.4.ln_2.weight\", \"decoder.transformer.h.4.ln_2.bias\", \"decoder.transformer.h.4.crossattention.bias\", \"decoder.transformer.h.4.crossattention.masked_bias\", \"decoder.transformer.h.4.crossattention.c_attn.weight\", \"decoder.transformer.h.4.crossattention.c_attn.bias\", \"decoder.transformer.h.4.crossattention.q_attn.weight\", \"decoder.transformer.h.4.crossattention.q_attn.bias\", \"decoder.transformer.h.4.crossattention.c_proj.weight\", \"decoder.transformer.h.4.crossattention.c_proj.bias\", \"decoder.transformer.h.4.ln_cross_attn.weight\", \"decoder.transformer.h.4.ln_cross_attn.bias\", \"decoder.transformer.h.4.mlp.c_fc.weight\", \"decoder.transformer.h.4.mlp.c_fc.bias\", \"decoder.transformer.h.4.mlp.c_proj.weight\", \"decoder.transformer.h.4.mlp.c_proj.bias\", \"decoder.transformer.h.5.ln_1.weight\", \"decoder.transformer.h.5.ln_1.bias\", \"decoder.transformer.h.5.attn.bias\", \"decoder.transformer.h.5.attn.masked_bias\", \"decoder.transformer.h.5.attn.c_attn.weight\", \"decoder.transformer.h.5.attn.c_attn.bias\", \"decoder.transformer.h.5.attn.c_proj.weight\", \"decoder.transformer.h.5.attn.c_proj.bias\", \"decoder.transformer.h.5.ln_2.weight\", \"decoder.transformer.h.5.ln_2.bias\", \"decoder.transformer.h.5.crossattention.bias\", \"decoder.transformer.h.5.crossattention.masked_bias\", \"decoder.transformer.h.5.crossattention.c_attn.weight\", \"decoder.transformer.h.5.crossattention.c_attn.bias\", \"decoder.transformer.h.5.crossattention.q_attn.weight\", \"decoder.transformer.h.5.crossattention.q_attn.bias\", \"decoder.transformer.h.5.crossattention.c_proj.weight\", \"decoder.transformer.h.5.crossattention.c_proj.bias\", \"decoder.transformer.h.5.ln_cross_attn.weight\", \"decoder.transformer.h.5.ln_cross_attn.bias\", \"decoder.transformer.h.5.mlp.c_fc.weight\", \"decoder.transformer.h.5.mlp.c_fc.bias\", \"decoder.transformer.h.5.mlp.c_proj.weight\", \"decoder.transformer.h.5.mlp.c_proj.bias\", \"decoder.transformer.h.6.ln_1.weight\", \"decoder.transformer.h.6.ln_1.bias\", \"decoder.transformer.h.6.attn.bias\", \"decoder.transformer.h.6.attn.masked_bias\", \"decoder.transformer.h.6.attn.c_attn.weight\", \"decoder.transformer.h.6.attn.c_attn.bias\", \"decoder.transformer.h.6.attn.c_proj.weight\", \"decoder.transformer.h.6.attn.c_proj.bias\", \"decoder.transformer.h.6.ln_2.weight\", \"decoder.transformer.h.6.ln_2.bias\", \"decoder.transformer.h.6.crossattention.bias\", \"decoder.transformer.h.6.crossattention.masked_bias\", \"decoder.transformer.h.6.crossattention.c_attn.weight\", \"decoder.transformer.h.6.crossattention.c_attn.bias\", \"decoder.transformer.h.6.crossattention.q_attn.weight\", \"decoder.transformer.h.6.crossattention.q_attn.bias\", \"decoder.transformer.h.6.crossattention.c_proj.weight\", \"decoder.transformer.h.6.crossattention.c_proj.bias\", \"decoder.transformer.h.6.ln_cross_attn.weight\", \"decoder.transformer.h.6.ln_cross_attn.bias\", \"decoder.transformer.h.6.mlp.c_fc.weight\", \"decoder.transformer.h.6.mlp.c_fc.bias\", \"decoder.transformer.h.6.mlp.c_proj.weight\", \"decoder.transformer.h.6.mlp.c_proj.bias\", \"decoder.transformer.h.7.ln_1.weight\", \"decoder.transformer.h.7.ln_1.bias\", \"decoder.transformer.h.7.attn.bias\", \"decoder.transformer.h.7.attn.masked_bias\", \"decoder.transformer.h.7.attn.c_attn.weight\", \"decoder.transformer.h.7.attn.c_attn.bias\", \"decoder.transformer.h.7.attn.c_proj.weight\", \"decoder.transformer.h.7.attn.c_proj.bias\", \"decoder.transformer.h.7.ln_2.weight\", \"decoder.transformer.h.7.ln_2.bias\", \"decoder.transformer.h.7.crossattention.bias\", \"decoder.transformer.h.7.crossattention.masked_bias\", \"decoder.transformer.h.7.crossattention.c_attn.weight\", \"decoder.transformer.h.7.crossattention.c_attn.bias\", \"decoder.transformer.h.7.crossattention.q_attn.weight\", \"decoder.transformer.h.7.crossattention.q_attn.bias\", \"decoder.transformer.h.7.crossattention.c_proj.weight\", \"decoder.transformer.h.7.crossattention.c_proj.bias\", \"decoder.transformer.h.7.ln_cross_attn.weight\", \"decoder.transformer.h.7.ln_cross_attn.bias\", \"decoder.transformer.h.7.mlp.c_fc.weight\", \"decoder.transformer.h.7.mlp.c_fc.bias\", \"decoder.transformer.h.7.mlp.c_proj.weight\", \"decoder.transformer.h.7.mlp.c_proj.bias\", \"decoder.transformer.h.8.ln_1.weight\", \"decoder.transformer.h.8.ln_1.bias\", \"decoder.transformer.h.8.attn.bias\", \"decoder.transformer.h.8.attn.masked_bias\", \"decoder.transformer.h.8.attn.c_attn.weight\", \"decoder.transformer.h.8.attn.c_attn.bias\", \"decoder.transformer.h.8.attn.c_proj.weight\", \"decoder.transformer.h.8.attn.c_proj.bias\", \"decoder.transformer.h.8.ln_2.weight\", \"decoder.transformer.h.8.ln_2.bias\", \"decoder.transformer.h.8.crossattention.bias\", \"decoder.transformer.h.8.crossattention.masked_bias\", \"decoder.transformer.h.8.crossattention.c_attn.weight\", \"decoder.transformer.h.8.crossattention.c_attn.bias\", \"decoder.transformer.h.8.crossattention.q_attn.weight\", \"decoder.transformer.h.8.crossattention.q_attn.bias\", \"decoder.transformer.h.8.crossattention.c_proj.weight\", \"decoder.transformer.h.8.crossattention.c_proj.bias\", \"decoder.transformer.h.8.ln_cross_attn.weight\", \"decoder.transformer.h.8.ln_cross_attn.bias\", \"decoder.transformer.h.8.mlp.c_fc.weight\", \"decoder.transformer.h.8.mlp.c_fc.bias\", \"decoder.transformer.h.8.mlp.c_proj.weight\", \"decoder.transformer.h.8.mlp.c_proj.bias\", \"decoder.transformer.h.9.ln_1.weight\", \"decoder.transformer.h.9.ln_1.bias\", \"decoder.transformer.h.9.attn.bias\", \"decoder.transformer.h.9.attn.masked_bias\", \"decoder.transformer.h.9.attn.c_attn.weight\", \"decoder.transformer.h.9.attn.c_attn.bias\", \"decoder.transformer.h.9.attn.c_proj.weight\", \"decoder.transformer.h.9.attn.c_proj.bias\", \"decoder.transformer.h.9.ln_2.weight\", \"decoder.transformer.h.9.ln_2.bias\", \"decoder.transformer.h.9.crossattention.bias\", \"decoder.transformer.h.9.crossattention.masked_bias\", \"decoder.transformer.h.9.crossattention.c_attn.weight\", \"decoder.transformer.h.9.crossattention.c_attn.bias\", \"decoder.transformer.h.9.crossattention.q_attn.weight\", \"decoder.transformer.h.9.crossattention.q_attn.bias\", \"decoder.transformer.h.9.crossattention.c_proj.weight\", \"decoder.transformer.h.9.crossattention.c_proj.bias\", \"decoder.transformer.h.9.ln_cross_attn.weight\", \"decoder.transformer.h.9.ln_cross_attn.bias\", \"decoder.transformer.h.9.mlp.c_fc.weight\", \"decoder.transformer.h.9.mlp.c_fc.bias\", \"decoder.transformer.h.9.mlp.c_proj.weight\", \"decoder.transformer.h.9.mlp.c_proj.bias\", \"decoder.transformer.h.10.ln_1.weight\", \"decoder.transformer.h.10.ln_1.bias\", \"decoder.transformer.h.10.attn.bias\", \"decoder.transformer.h.10.attn.masked_bias\", \"decoder.transformer.h.10.attn.c_attn.weight\", \"decoder.transformer.h.10.attn.c_attn.bias\", \"decoder.transformer.h.10.attn.c_proj.weight\", \"decoder.transformer.h.10.attn.c_proj.bias\", \"decoder.transformer.h.10.ln_2.weight\", \"decoder.transformer.h.10.ln_2.bias\", \"decoder.transformer.h.10.crossattention.bias\", \"decoder.transformer.h.10.crossattention.masked_bias\", \"decoder.transformer.h.10.crossattention.c_attn.weight\", \"decoder.transformer.h.10.crossattention.c_attn.bias\", \"decoder.transformer.h.10.crossattention.q_attn.weight\", \"decoder.transformer.h.10.crossattention.q_attn.bias\", \"decoder.transformer.h.10.crossattention.c_proj.weight\", \"decoder.transformer.h.10.crossattention.c_proj.bias\", \"decoder.transformer.h.10.ln_cross_attn.weight\", \"decoder.transformer.h.10.ln_cross_attn.bias\", \"decoder.transformer.h.10.mlp.c_fc.weight\", \"decoder.transformer.h.10.mlp.c_fc.bias\", \"decoder.transformer.h.10.mlp.c_proj.weight\", \"decoder.transformer.h.10.mlp.c_proj.bias\", \"decoder.transformer.h.11.ln_1.weight\", \"decoder.transformer.h.11.ln_1.bias\", \"decoder.transformer.h.11.attn.bias\", \"decoder.transformer.h.11.attn.masked_bias\", \"decoder.transformer.h.11.attn.c_attn.weight\", \"decoder.transformer.h.11.attn.c_attn.bias\", \"decoder.transformer.h.11.attn.c_proj.weight\", \"decoder.transformer.h.11.attn.c_proj.bias\", \"decoder.transformer.h.11.ln_2.weight\", \"decoder.transformer.h.11.ln_2.bias\", \"decoder.transformer.h.11.crossattention.bias\", \"decoder.transformer.h.11.crossattention.masked_bias\", \"decoder.transformer.h.11.crossattention.c_attn.weight\", \"decoder.transformer.h.11.crossattention.c_attn.bias\", \"decoder.transformer.h.11.crossattention.q_attn.weight\", \"decoder.transformer.h.11.crossattention.q_attn.bias\", \"decoder.transformer.h.11.crossattention.c_proj.weight\", \"decoder.transformer.h.11.crossattention.c_proj.bias\", \"decoder.transformer.h.11.ln_cross_attn.weight\", \"decoder.transformer.h.11.ln_cross_attn.bias\", \"decoder.transformer.h.11.mlp.c_fc.weight\", \"decoder.transformer.h.11.mlp.c_fc.bias\", \"decoder.transformer.h.11.mlp.c_proj.weight\", \"decoder.transformer.h.11.mlp.c_proj.bias\", \"decoder.transformer.ln_f.weight\", \"decoder.transformer.ln_f.bias\", \"decoder.lm_head.weight\", \"pos_enb.pe\", \"embedder.0.weight\", \"embedder.0.bias\", \"embedder.2.weight\", \"embedder.2.bias\", \"embedder.2.running_mean\", \"embedder.2.running_var\", \"embedder.3.weight\", \"embedder.3.bias\". \n\tUnexpected key(s) in state_dict: \"transformer.wte.weight\", \"transformer.wpe.weight\", \"transformer.h.0.ln_1.weight\", \"transformer.h.0.ln_1.bias\", \"transformer.h.0.attn.bias\", \"transformer.h.0.attn.masked_bias\", \"transformer.h.0.attn.c_attn.weight\", \"transformer.h.0.attn.c_attn.bias\", \"transformer.h.0.attn.c_proj.weight\", \"transformer.h.0.attn.c_proj.bias\", \"transformer.h.0.ln_2.weight\", \"transformer.h.0.ln_2.bias\", \"transformer.h.0.crossattention.bias\", \"transformer.h.0.crossattention.masked_bias\", \"transformer.h.0.crossattention.c_attn.weight\", \"transformer.h.0.crossattention.c_attn.bias\", \"transformer.h.0.crossattention.q_attn.weight\", \"transformer.h.0.crossattention.q_attn.bias\", \"transformer.h.0.crossattention.c_proj.weight\", \"transformer.h.0.crossattention.c_proj.bias\", \"transformer.h.0.ln_cross_attn.weight\", \"transformer.h.0.ln_cross_attn.bias\", \"transformer.h.0.mlp.c_fc.weight\", \"transformer.h.0.mlp.c_fc.bias\", \"transformer.h.0.mlp.c_proj.weight\", \"transformer.h.0.mlp.c_proj.bias\", \"transformer.h.1.ln_1.weight\", \"transformer.h.1.ln_1.bias\", \"transformer.h.1.attn.bias\", \"transformer.h.1.attn.masked_bias\", \"transformer.h.1.attn.c_attn.weight\", \"transformer.h.1.attn.c_attn.bias\", \"transformer.h.1.attn.c_proj.weight\", \"transformer.h.1.attn.c_proj.bias\", \"transformer.h.1.ln_2.weight\", \"transformer.h.1.ln_2.bias\", \"transformer.h.1.crossattention.bias\", \"transformer.h.1.crossattention.masked_bias\", \"transformer.h.1.crossattention.c_attn.weight\", \"transformer.h.1.crossattention.c_attn.bias\", \"transformer.h.1.crossattention.q_attn.weight\", \"transformer.h.1.crossattention.q_attn.bias\", \"transformer.h.1.crossattention.c_proj.weight\", \"transformer.h.1.crossattention.c_proj.bias\", \"transformer.h.1.ln_cross_attn.weight\", \"transformer.h.1.ln_cross_attn.bias\", \"transformer.h.1.mlp.c_fc.weight\", \"transformer.h.1.mlp.c_fc.bias\", \"transformer.h.1.mlp.c_proj.weight\", \"transformer.h.1.mlp.c_proj.bias\", \"transformer.h.2.ln_1.weight\", \"transformer.h.2.ln_1.bias\", \"transformer.h.2.attn.bias\", \"transformer.h.2.attn.masked_bias\", \"transformer.h.2.attn.c_attn.weight\", \"transformer.h.2.attn.c_attn.bias\", \"transformer.h.2.attn.c_proj.weight\", \"transformer.h.2.attn.c_proj.bias\", \"transformer.h.2.ln_2.weight\", \"transformer.h.2.ln_2.bias\", \"transformer.h.2.crossattention.bias\", \"transformer.h.2.crossattention.masked_bias\", \"transformer.h.2.crossattention.c_attn.weight\", \"transformer.h.2.crossattention.c_attn.bias\", \"transformer.h.2.crossattention.q_attn.weight\", \"transformer.h.2.crossattention.q_attn.bias\", \"transformer.h.2.crossattention.c_proj.weight\", \"transformer.h.2.crossattention.c_proj.bias\", \"transformer.h.2.ln_cross_attn.weight\", \"transformer.h.2.ln_cross_attn.bias\", \"transformer.h.2.mlp.c_fc.weight\", \"transformer.h.2.mlp.c_fc.bias\", \"transformer.h.2.mlp.c_proj.weight\", \"transformer.h.2.mlp.c_proj.bias\", \"transformer.h.3.ln_1.weight\", \"transformer.h.3.ln_1.bias\", \"transformer.h.3.attn.bias\", \"transformer.h.3.attn.masked_bias\", \"transformer.h.3.attn.c_attn.weight\", \"transformer.h.3.attn.c_attn.bias\", \"transformer.h.3.attn.c_proj.weight\", \"transformer.h.3.attn.c_proj.bias\", \"transformer.h.3.ln_2.weight\", \"transformer.h.3.ln_2.bias\", \"transformer.h.3.crossattention.bias\", \"transformer.h.3.crossattention.masked_bias\", \"transformer.h.3.crossattention.c_attn.weight\", \"transformer.h.3.crossattention.c_attn.bias\", \"transformer.h.3.crossattention.q_attn.weight\", \"transformer.h.3.crossattention.q_attn.bias\", \"transformer.h.3.crossattention.c_proj.weight\", \"transformer.h.3.crossattention.c_proj.bias\", \"transformer.h.3.ln_cross_attn.weight\", \"transformer.h.3.ln_cross_attn.bias\", \"transformer.h.3.mlp.c_fc.weight\", \"transformer.h.3.mlp.c_fc.bias\", \"transformer.h.3.mlp.c_proj.weight\", \"transformer.h.3.mlp.c_proj.bias\", \"transformer.h.4.ln_1.weight\", \"transformer.h.4.ln_1.bias\", \"transformer.h.4.attn.bias\", \"transformer.h.4.attn.masked_bias\", \"transformer.h.4.attn.c_attn.weight\", \"transformer.h.4.attn.c_attn.bias\", \"transformer.h.4.attn.c_proj.weight\", \"transformer.h.4.attn.c_proj.bias\", \"transformer.h.4.ln_2.weight\", \"transformer.h.4.ln_2.bias\", \"transformer.h.4.crossattention.bias\", \"transformer.h.4.crossattention.masked_bias\", \"transformer.h.4.crossattention.c_attn.weight\", \"transformer.h.4.crossattention.c_attn.bias\", \"transformer.h.4.crossattention.q_attn.weight\", \"transformer.h.4.crossattention.q_attn.bias\", \"transformer.h.4.crossattention.c_proj.weight\", \"transformer.h.4.crossattention.c_proj.bias\", \"transformer.h.4.ln_cross_attn.weight\", \"transformer.h.4.ln_cross_attn.bias\", \"transformer.h.4.mlp.c_fc.weight\", \"transformer.h.4.mlp.c_fc.bias\", \"transformer.h.4.mlp.c_proj.weight\", \"transformer.h.4.mlp.c_proj.bias\", \"transformer.h.5.ln_1.weight\", \"transformer.h.5.ln_1.bias\", \"transformer.h.5.attn.bias\", \"transformer.h.5.attn.masked_bias\", \"transformer.h.5.attn.c_attn.weight\", \"transformer.h.5.attn.c_attn.bias\", \"transformer.h.5.attn.c_proj.weight\", \"transformer.h.5.attn.c_proj.bias\", \"transformer.h.5.ln_2.weight\", \"transformer.h.5.ln_2.bias\", \"transformer.h.5.crossattention.bias\", \"transformer.h.5.crossattention.masked_bias\", \"transformer.h.5.crossattention.c_attn.weight\", \"transformer.h.5.crossattention.c_attn.bias\", \"transformer.h.5.crossattention.q_attn.weight\", \"transformer.h.5.crossattention.q_attn.bias\", \"transformer.h.5.crossattention.c_proj.weight\", \"transformer.h.5.crossattention.c_proj.bias\", \"transformer.h.5.ln_cross_attn.weight\", \"transformer.h.5.ln_cross_attn.bias\", \"transformer.h.5.mlp.c_fc.weight\", \"transformer.h.5.mlp.c_fc.bias\", \"transformer.h.5.mlp.c_proj.weight\", \"transformer.h.5.mlp.c_proj.bias\", \"transformer.h.6.ln_1.weight\", \"transformer.h.6.ln_1.bias\", \"transformer.h.6.attn.bias\", \"transformer.h.6.attn.masked_bias\", \"transformer.h.6.attn.c_attn.weight\", \"transformer.h.6.attn.c_attn.bias\", \"transformer.h.6.attn.c_proj.weight\", \"transformer.h.6.attn.c_proj.bias\", \"transformer.h.6.ln_2.weight\", \"transformer.h.6.ln_2.bias\", \"transformer.h.6.crossattention.bias\", \"transformer.h.6.crossattention.masked_bias\", \"transformer.h.6.crossattention.c_attn.weight\", \"transformer.h.6.crossattention.c_attn.bias\", \"transformer.h.6.crossattention.q_attn.weight\", \"transformer.h.6.crossattention.q_attn.bias\", \"transformer.h.6.crossattention.c_proj.weight\", \"transformer.h.6.crossattention.c_proj.bias\", \"transformer.h.6.ln_cross_attn.weight\", \"transformer.h.6.ln_cross_attn.bias\", \"transformer.h.6.mlp.c_fc.weight\", \"transformer.h.6.mlp.c_fc.bias\", \"transformer.h.6.mlp.c_proj.weight\", \"transformer.h.6.mlp.c_proj.bias\", \"transformer.h.7.ln_1.weight\", \"transformer.h.7.ln_1.bias\", \"transformer.h.7.attn.bias\", \"transformer.h.7.attn.masked_bias\", \"transformer.h.7.attn.c_attn.weight\", \"transformer.h.7.attn.c_attn.bias\", \"transformer.h.7.attn.c_proj.weight\", \"transformer.h.7.attn.c_proj.bias\", \"transformer.h.7.ln_2.weight\", \"transformer.h.7.ln_2.bias\", \"transformer.h.7.crossattention.bias\", \"transformer.h.7.crossattention.masked_bias\", \"transformer.h.7.crossattention.c_attn.weight\", \"transformer.h.7.crossattention.c_attn.bias\", \"transformer.h.7.crossattention.q_attn.weight\", \"transformer.h.7.crossattention.q_attn.bias\", \"transformer.h.7.crossattention.c_proj.weight\", \"transformer.h.7.crossattention.c_proj.bias\", \"transformer.h.7.ln_cross_attn.weight\", \"transformer.h.7.ln_cross_attn.bias\", \"transformer.h.7.mlp.c_fc.weight\", \"transformer.h.7.mlp.c_fc.bias\", \"transformer.h.7.mlp.c_proj.weight\", \"transformer.h.7.mlp.c_proj.bias\", \"transformer.h.8.ln_1.weight\", \"transformer.h.8.ln_1.bias\", \"transformer.h.8.attn.bias\", \"transformer.h.8.attn.masked_bias\", \"transformer.h.8.attn.c_attn.weight\", \"transformer.h.8.attn.c_attn.bias\", \"transformer.h.8.attn.c_proj.weight\", \"transformer.h.8.attn.c_proj.bias\", \"transformer.h.8.ln_2.weight\", \"transformer.h.8.ln_2.bias\", \"transformer.h.8.crossattention.bias\", \"transformer.h.8.crossattention.masked_bias\", \"transformer.h.8.crossattention.c_attn.weight\", \"transformer.h.8.crossattention.c_attn.bias\", \"transformer.h.8.crossattention.q_attn.weight\", \"transformer.h.8.crossattention.q_attn.bias\", \"transformer.h.8.crossattention.c_proj.weight\", \"transformer.h.8.crossattention.c_proj.bias\", \"transformer.h.8.ln_cross_attn.weight\", \"transformer.h.8.ln_cross_attn.bias\", \"transformer.h.8.mlp.c_fc.weight\", \"transformer.h.8.mlp.c_fc.bias\", \"transformer.h.8.mlp.c_proj.weight\", \"transformer.h.8.mlp.c_proj.bias\", \"transformer.h.9.ln_1.weight\", \"transformer.h.9.ln_1.bias\", \"transformer.h.9.attn.bias\", \"transformer.h.9.attn.masked_bias\", \"transformer.h.9.attn.c_attn.weight\", \"transformer.h.9.attn.c_attn.bias\", \"transformer.h.9.attn.c_proj.weight\", \"transformer.h.9.attn.c_proj.bias\", \"transformer.h.9.ln_2.weight\", \"transformer.h.9.ln_2.bias\", \"transformer.h.9.crossattention.bias\", \"transformer.h.9.crossattention.masked_bias\", \"transformer.h.9.crossattention.c_attn.weight\", \"transformer.h.9.crossattention.c_attn.bias\", \"transformer.h.9.crossattention.q_attn.weight\", \"transformer.h.9.crossattention.q_attn.bias\", \"transformer.h.9.crossattention.c_proj.weight\", \"transformer.h.9.crossattention.c_proj.bias\", \"transformer.h.9.ln_cross_attn.weight\", \"transformer.h.9.ln_cross_attn.bias\", \"transformer.h.9.mlp.c_fc.weight\", \"transformer.h.9.mlp.c_fc.bias\", \"transformer.h.9.mlp.c_proj.weight\", \"transformer.h.9.mlp.c_proj.bias\", \"transformer.h.10.ln_1.weight\", \"transformer.h.10.ln_1.bias\", \"transformer.h.10.attn.bias\", \"transformer.h.10.attn.masked_bias\", \"transformer.h.10.attn.c_attn.weight\", \"transformer.h.10.attn.c_attn.bias\", \"transformer.h.10.attn.c_proj.weight\", \"transformer.h.10.attn.c_proj.bias\", \"transformer.h.10.ln_2.weight\", \"transformer.h.10.ln_2.bias\", \"transformer.h.10.crossattention.bias\", \"transformer.h.10.crossattention.masked_bias\", \"transformer.h.10.crossattention.c_attn.weight\", \"transformer.h.10.crossattention.c_attn.bias\", \"transformer.h.10.crossattention.q_attn.weight\", \"transformer.h.10.crossattention.q_attn.bias\", \"transformer.h.10.crossattention.c_proj.weight\", \"transformer.h.10.crossattention.c_proj.bias\", \"transformer.h.10.ln_cross_attn.weight\", \"transformer.h.10.ln_cross_attn.bias\", \"transformer.h.10.mlp.c_fc.weight\", \"transformer.h.10.mlp.c_fc.bias\", \"transformer.h.10.mlp.c_proj.weight\", \"transformer.h.10.mlp.c_proj.bias\", \"transformer.h.11.ln_1.weight\", \"transformer.h.11.ln_1.bias\", \"transformer.h.11.attn.bias\", \"transformer.h.11.attn.masked_bias\", \"transformer.h.11.attn.c_attn.weight\", \"transformer.h.11.attn.c_attn.bias\", \"transformer.h.11.attn.c_proj.weight\", \"transformer.h.11.attn.c_proj.bias\", \"transformer.h.11.ln_2.weight\", \"transformer.h.11.ln_2.bias\", \"transformer.h.11.crossattention.bias\", \"transformer.h.11.crossattention.masked_bias\", \"transformer.h.11.crossattention.c_attn.weight\", \"transformer.h.11.crossattention.c_attn.bias\", \"transformer.h.11.crossattention.q_attn.weight\", \"transformer.h.11.crossattention.q_attn.bias\", \"transformer.h.11.crossattention.c_proj.weight\", \"transformer.h.11.crossattention.c_proj.bias\", \"transformer.h.11.ln_cross_attn.weight\", \"transformer.h.11.ln_cross_attn.bias\", \"transformer.h.11.mlp.c_fc.weight\", \"transformer.h.11.mlp.c_fc.bias\", \"transformer.h.11.mlp.c_proj.weight\", \"transformer.h.11.mlp.c_proj.bias\", \"transformer.ln_f.weight\", \"transformer.ln_f.bias\", \"lm_head.weight\". "
     ]
    }
   ],
   "source": [
    "enc_dec_model.load_state_dict(torch.load('model/gpt2_enc_dec.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/gpt2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT2Config()\n",
    "model.config.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (q_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4286, 0.4545, 0.5000,  ..., 0.5714, 0.3846, 0.3846],\n",
       "        [0.5714, 0.6364, 0.7500,  ..., 0.6190, 0.4615, 0.4615],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        ...,\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.7143, 0.6364, 0.5000,  ..., 0.0476, 0.0000, 0.0000],\n",
       "        [0.5714, 0.5455, 0.5000,  ..., 0.1429, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
