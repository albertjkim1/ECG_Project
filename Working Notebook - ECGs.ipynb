{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from base64 import b64decode as decode\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.fft import fftn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_id</th>\n",
       "      <th>lead_id</th>\n",
       "      <th>decoded_waveform</th>\n",
       "      <th>waveform_type</th>\n",
       "      <th>Full_text</th>\n",
       "      <th>Original_Diag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549871</td>\n",
       "      <td>I</td>\n",
       "      <td>[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>No previous ECGs available</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549871</td>\n",
       "      <td>I</td>\n",
       "      <td>[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>Otherwise normal ECG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>549871</td>\n",
       "      <td>I</td>\n",
       "      <td>[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>Sinus bradycardia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>549871</td>\n",
       "      <td>I</td>\n",
       "      <td>[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>549871</td>\n",
       "      <td>I</td>\n",
       "      <td>[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>Sinus bradycardia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>554080</td>\n",
       "      <td>V6</td>\n",
       "      <td>[10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>Abnormal ECG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>554080</td>\n",
       "      <td>V6</td>\n",
       "      <td>[10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>No previous ECGs available</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>554080</td>\n",
       "      <td>V6</td>\n",
       "      <td>[10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>No previous ECGs available</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>554080</td>\n",
       "      <td>V6</td>\n",
       "      <td>[10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>Sinus rhythm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>554080</td>\n",
       "      <td>V6</td>\n",
       "      <td>[10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>Prolonged QT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1424 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      exam_id lead_id                                   decoded_waveform  \\\n",
       "0      549871       I  [-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...   \n",
       "1      549871       I  [-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...   \n",
       "2      549871       I  [-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...   \n",
       "3      549871       I  [-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...   \n",
       "4      549871       I  [-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, -...   \n",
       "...       ...     ...                                                ...   \n",
       "1419   554080      V6  [10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...   \n",
       "1420   554080      V6  [10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...   \n",
       "1421   554080      V6  [10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...   \n",
       "1422   554080      V6  [10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...   \n",
       "1423   554080      V6  [10, 10, 10, 11, 12, 12, 12, 12, 12, 13, 14, 1...   \n",
       "\n",
       "     waveform_type                   Full_text  Original_Diag  \n",
       "0           Rhythm  No previous ECGs available              0  \n",
       "1           Rhythm        Otherwise normal ECG              0  \n",
       "2           Rhythm           Sinus bradycardia              0  \n",
       "3           Rhythm                         NaN              0  \n",
       "4           Rhythm           Sinus bradycardia              1  \n",
       "...            ...                         ...            ...  \n",
       "1419        Rhythm                Abnormal ECG              1  \n",
       "1420        Rhythm  No previous ECGs available              1  \n",
       "1421        Rhythm  No previous ECGs available              0  \n",
       "1422        Rhythm                Sinus rhythm              0  \n",
       "1423        Rhythm                Prolonged QT              0  \n",
       "\n",
       "[1424 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/d_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/d_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/d_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/d_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead data\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "\n",
    "\n",
    "# adding the diagnosis and labels\n",
    "waveform_and_diag = pd.merge(waveform_lead[['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']], diagnosis_data[[\"exam_id\", \"Full_text\", \"Original_Diag\"]], left_on= \"exam_id\", right_on=\"exam_id\")\n",
    "waveform_and_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_id</th>\n",
       "      <th>waveform_type</th>\n",
       "      <th>decoded_waveform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>548759</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[4, 3, 2, -1, -4, -4, -4, -4, -4, -7, -10, -8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>550602</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-22, -20, -18, -16, -14, -14, -14, -12, -10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>551485</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[46, 45, 44, 42, 40, 35, 30, 26, 22, 18, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>552077</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-7, -4, -1, -6, -10, -12, -14, -11, -11, -14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>552856</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-32, -32, -32, -33, -34, -34, -34, -33, -32,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>553115</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-8, -5, -2, -2, -2, -5, -8, -8, -8, -7, -6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>553528</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-12, -12, -12, -12, -12, -12, -12, -12, -12,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    exam_id waveform_type                                   decoded_waveform\n",
       "1    548759        Rhythm  [[4, 3, 2, -1, -4, -4, -4, -4, -4, -7, -10, -8...\n",
       "3    549871        Rhythm  [[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, ...\n",
       "5    550602        Rhythm  [[-22, -20, -18, -16, -14, -14, -14, -12, -10,...\n",
       "7    551485        Rhythm  [[46, 45, 44, 42, 40, 35, 30, 26, 22, 18, 14, ...\n",
       "9    552077        Rhythm  [[-7, -4, -1, -6, -10, -12, -14, -11, -11, -14...\n",
       "11   552856        Rhythm  [[-32, -32, -32, -33, -34, -34, -34, -33, -32,...\n",
       "14   553115        Rhythm  [[-8, -5, -2, -2, -2, -5, -8, -8, -8, -7, -6, ...\n",
       "16   553528        Rhythm  [[-12, -12, -12, -12, -12, -12, -12, -12, -12,..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: np.vstack(x))\n",
    "\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "waveform_lead_median = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Median\"]\n",
    "\n",
    "waveform_lead_rhythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_id</th>\n",
       "      <th>waveform_type</th>\n",
       "      <th>decoded_waveform</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>548759</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[4, 3, 2, -1, -4, -4, -4, -4, -4, -7, -10, -8...</td>\n",
       "      <td>normal sinus rhythm low voltage qrs borderline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549871</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, ...</td>\n",
       "      <td>sinus bradycardia otherwise normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550602</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-22, -20, -18, -16, -14, -14, -14, -12, -10,...</td>\n",
       "      <td>sinus tachycardia otherwise normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551485</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[46, 45, 44, 42, 40, 35, 30, 26, 22, 18, 14, ...</td>\n",
       "      <td>normal sinus rhythm normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>552077</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-7, -4, -1, -6, -10, -12, -14, -11, -11, -14...</td>\n",
       "      <td>normal sinus rhythm normal ecg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>552856</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-32, -32, -32, -33, -34, -34, -34, -33, -32,...</td>\n",
       "      <td>normal sinus rhythm with sinus arrhythmia mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>553115</td>\n",
       "      <td>Rhythm</td>\n",
       "      <td>[[-8, -5, -2, -2, -2, -5, -8, -8, -8, -7, -6, ...</td>\n",
       "      <td>atrial fibrillation abnormal ecg normal sinus ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exam_id waveform_type                                   decoded_waveform  \\\n",
       "0   548759        Rhythm  [[4, 3, 2, -1, -4, -4, -4, -4, -4, -7, -10, -8...   \n",
       "1   549871        Rhythm  [[-8, -8, -8, -8, -8, -8, -8, -7, -6, -5, -4, ...   \n",
       "2   550602        Rhythm  [[-22, -20, -18, -16, -14, -14, -14, -12, -10,...   \n",
       "3   551485        Rhythm  [[46, 45, 44, 42, 40, 35, 30, 26, 22, 18, 14, ...   \n",
       "4   552077        Rhythm  [[-7, -4, -1, -6, -10, -12, -14, -11, -11, -14...   \n",
       "5   552856        Rhythm  [[-32, -32, -32, -33, -34, -34, -34, -33, -32,...   \n",
       "6   553115        Rhythm  [[-8, -5, -2, -2, -2, -5, -8, -8, -8, -7, -6, ...   \n",
       "\n",
       "                                           diagnosis  \n",
       "0  normal sinus rhythm low voltage qrs borderline...  \n",
       "1             sinus bradycardia otherwise normal ecg  \n",
       "2             sinus tachycardia otherwise normal ecg  \n",
       "3                     normal sinus rhythm normal ecg  \n",
       "4                     normal sinus rhythm normal ecg  \n",
       "5  normal sinus rhythm with sinus arrhythmia mini...  \n",
       "6  atrial fibrillation abnormal ecg normal sinus ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the labels/sentences\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "# Let's look over this tomorrow\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "#\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if row[\"statement_order\"] == 1 and curr_string != \"\":\n",
    "        curr_string = curr_string.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        val = [curr_id, curr_string[1:]]\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    curr_string += \" \" + row[\"Full_text\"]\n",
    "\n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "#waveform_lead_rhythm_diag\n",
    "waveform_lead_rhythm_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normal', 'consider', 'ischemia', 'with', 'variant', 'inferior', 'may', 'criteria', 'tachycardia', 'abnormal', 'borderline', 'wave', 'voltage', 'for', 't', 'qrs', 'otherwise', 'arrhythmia', 'abnormality', 'atrial', 'bradycardia', 'be', 'ecg', 'fibrillation', 'sinus', 'lvh', 'rhythm', 'minimal', 'low'}\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "for num, sentence in diagnoses:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word)\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing datasets\n",
    "# y not included for now\n",
    "def one_hot(x, dict_words, max_length):\n",
    "    x = x.split(\" \")\n",
    "    array = []\n",
    "    for i in x:\n",
    "        array.append(dict_words.index(i))\n",
    "    while(len(array) < max_length):\n",
    "        array.append(29)\n",
    "    return array\n",
    "\n",
    "dict_words = list(unique_words)\n",
    "dict_words.append([\" \"])\n",
    "print(len(dict_words))\n",
    "Y = waveform_lead_rhythm_diag['diagnosis'].apply(lambda x: one_hot(x, dict_words, 20))\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(waveform_lead_rhythm_diag['decoded_waveform'], Y, test_size = 0.1, random_state = 2021)\n",
    "train_x = torch.tensor(list(train_x)).float()\n",
    "train_y = torch.tensor(list(train_y))\n",
    "\n",
    "test_x = torch.tensor(list(test_x)).float()\n",
    "test_y = torch.tensor(list(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Conv1D Encoder w/ LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# 1D grouped encoder model\n",
    "encoder_conv = nn.Sequential()\n",
    "encoder_conv.add_module('initial_norm', nn.BatchNorm1d(8))\n",
    "encoder_conv.add_module('conv_1', nn.Conv1d(in_channels = 8, out_channels = 8, groups = 8, kernel_size = 5, padding = 2))\n",
    "for i in range(2, (J+2), 2):\n",
    "    if (i-2) == 0: \n",
    "        prev = 8\n",
    "    else:\n",
    "        prev = (i-2)*8\n",
    "    encoder_conv.add_module('conv_{num}'.format(num = int(i / 2 + 1)), nn.Conv1d(in_channels = prev, out_channels = i*8, groups = 8, kernel_size = 5, padding = 2, stride = 2))\n",
    "    encoder_conv.add_module('activation_{num}'.format(num = int(i / 2 + 1)), nn.ELU())\n",
    "    encoder_conv.add_module('batch_norm_{num}'.format(num = int(i / 2 + 1)), nn.BatchNorm1d(i*8))\n",
    "    \n",
    "encoder_conv.add_module('final_conv', nn.Conv1d(in_channels = J * 8, out_channels = 8, groups = 8, kernel_size = 5, padding = 2))\n",
    "encoder_conv.add_module('max_pool', nn.MaxPool1d(kernel_size = 7, padding = 3, stride = 4))\n",
    "encoder_conv.add_module('reshape', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "\n",
    "\n",
    "# summarize model, verify output is of desired shape\n",
    "print(encoder_conv)\n",
    "print(encoder_conv(train_x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (initial_norm): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_1): Conv1d(8, 8, kernel_size=(5,), stride=(1,), padding=(4,), groups=8)\n",
      "  (conv_2): Conv1d(8, 16, kernel_size=(5,), stride=(2,), padding=(2,), groups=8)\n",
      "  (activation_2): ELU(alpha=1.0)\n",
      "  (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_3): Conv1d(16, 32, kernel_size=(5,), stride=(2,), padding=(2,), groups=8)\n",
      "  (activation_3): ELU(alpha=1.0)\n",
      "  (batch_norm_3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (final_conv): Conv1d(32, 8, kernel_size=(5,), stride=(1,), padding=(2,), groups=8)\n",
      "  (max_pool): MaxPool1d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      ")\n",
      "torch.Size([6, 8, 626])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 4 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# 1D grouped encoder model\n",
    "encoder_conv = nn.Sequential()\n",
    "encoder_conv.add_module('initial_norm', nn.BatchNorm1d(8))\n",
    "encoder_conv.add_module('conv_1', nn.Conv1d(in_channels = 8, out_channels = 8, groups = 8, kernel_size = 5, padding = 4, stride = 1))\n",
    "for i in range(2, (J+2), 2):\n",
    "    if (i-2) == 0: \n",
    "        prev = 8\n",
    "    else:\n",
    "        prev = (i-2)*8\n",
    "    encoder_conv.add_module('conv_{num}'.format(num = int(i / 2 + 1)), nn.Conv1d(in_channels = prev, out_channels = i*8, groups = 8, kernel_size = 5, padding = 2, stride = 2))\n",
    "    encoder_conv.add_module('activation_{num}'.format(num = int(i / 2 + 1)), nn.ELU())\n",
    "    encoder_conv.add_module('batch_norm_{num}'.format(num = int(i / 2 + 1)), nn.BatchNorm1d(i*8))\n",
    "    \n",
    "encoder_conv.add_module('final_conv', nn.Conv1d(in_channels = J * 8, out_channels = 8, groups = 8, kernel_size = 5, padding = 2))\n",
    "encoder_conv.add_module('max_pool', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "#encoder_conv.add_module('reshape', nn.MaxPool1d(kernel_size = 5, padding = 2, stride = 1))\n",
    "\n",
    "# summarize model, verify output is of desired shape\n",
    "print(encoder_conv)\n",
    "print(encoder_conv(train_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - LSTM Encoder w/ Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 512\n",
    "embedding_dim = 8\n",
    "num_words = len(dict_words)\n",
    "\n",
    "class ECG_LSTM(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim, word_list_length):\n",
    "        super(ECG_LSTM, self).__init__()\n",
    "        # LSTM decoder  \n",
    "        self.lstm = nn.LSTM(e_dim, h_dim)\n",
    "        self.linear = nn.Linear(h_dim, word_list_length)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq_embedded = seq.view(len(seq), -1, embedding_dim)\n",
    "        final_hidd, _ = self.lstm(seq_embedded)\n",
    "        dec_seq = self.linear(final_hidd)\n",
    "        return F.log_softmax(dec_seq, dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-5708ab1cc4ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_mod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-9d533def8691>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mseq_embedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mfinal_hidd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_embedded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdec_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_hidd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2058\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2059\u001b[0m     )\n\u001b[0;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(lstm_mod.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j, k in zip(train_x, train_y):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_mod(j.unsqueeze(0)).squeeze(0)\n",
    "        loss = loss_fn(outputs, k)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 30)\n",
      "[21 11 10 21  5 26 23 25 22  6 29 25 27 26  4 26  4 26 26  7]\n",
      "tensor([21, 11, 10, 21,  5, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29])\n"
     ]
    }
   ],
   "source": [
    "#torch.save(lstm_mod.state_dict(), 'model/lstm.pt')\n",
    "lstm_mod = ECG_LSTM(encoder_conv, hidden_layers, embedding_dim, num_words)\n",
    "lstm_mod.load_state_dict(torch.load('model/lstm.pt'))\n",
    "\n",
    "out = lstm_mod(train_x[5].unsqueeze(0))\n",
    "print(out.squeeze(0).detach().numpy().shape)\n",
    "out = np.argmax(out.squeeze(0).detach().numpy(), axis = 1)\n",
    "print(out)\n",
    "print(train_y[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Basic Transformer Architecture with Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(features, features * expansion)\n",
    "        self.linear_2 = nn.Linear(features * expansion, features)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = F.gelu(self.linear_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.dropout_2(self.linear_2(x))\n",
    "        x = self.norm_1(x + res)\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class FNETLayer(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FNETLayer, self).__init__()\n",
    "        self.feed_forward = FeedForwardNet(features, expansion, dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = fftn(x, dim = (-1, -2)).real\n",
    "        x = self.norm_1(x + res)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "    \n",
    "class FNETEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self, features, expansion=2, dropout=0.5, num_layers=6):\n",
    "        encoder_layer = FNETLayer(features, expansion, dropout)\n",
    "        super().__init__(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class FNETModel(nn.Module):\n",
    "    def __init__(self, expansion, dropout, num_layers, embedder, decoder):\n",
    "        super(FNETModel, self).__init__()\n",
    "        self.embedder = embedder\n",
    "        self.decoder = decoder\n",
    "        temp = self.embedder(train_x).shape[2]\n",
    "        self.pos_enb = PositionalEncoding(d_model = temp)\n",
    "        self.encoder = FNETEncoder(features = temp, expansion = expansion, dropout = dropout, num_layers = num_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x)\n",
    "        x = self.pos_enb(x)\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(x)\n",
    "        return out    \n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "decoder = nn.Sequential(Transpose(), nn.Linear(8, 15), nn.ReLU(), nn.Linear(15, 20), nn.ReLU(), Transpose(), nn.Linear(626, 400), nn.ReLU(), nn.Linear(400, 200), nn.ReLU(), nn.Linear(200, 100), nn.ReLU(), nn.Linear(100, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.0516, grad_fn=<AddBackward0>)\n",
      "tensor(19.7664, grad_fn=<AddBackward0>)\n",
      "tensor(19.3583, grad_fn=<AddBackward0>)\n",
      "tensor(18.6789, grad_fn=<AddBackward0>)\n",
      "tensor(17.6058, grad_fn=<AddBackward0>)\n",
      "tensor(16.2079, grad_fn=<AddBackward0>)\n",
      "tensor(14.6231, grad_fn=<AddBackward0>)\n",
      "tensor(13.4965, grad_fn=<AddBackward0>)\n",
      "tensor(13.2718, grad_fn=<AddBackward0>)\n",
      "tensor(13.2807, grad_fn=<AddBackward0>)\n",
      "tensor(13.0155, grad_fn=<AddBackward0>)\n",
      "tensor(12.4728, grad_fn=<AddBackward0>)\n",
      "tensor(12.0347, grad_fn=<AddBackward0>)\n",
      "tensor(11.6499, grad_fn=<AddBackward0>)\n",
      "tensor(11.4429, grad_fn=<AddBackward0>)\n",
      "tensor(11.3366, grad_fn=<AddBackward0>)\n",
      "tensor(11.2243, grad_fn=<AddBackward0>)\n",
      "tensor(10.8664, grad_fn=<AddBackward0>)\n",
      "tensor(10.6123, grad_fn=<AddBackward0>)\n",
      "tensor(10.4745, grad_fn=<AddBackward0>)\n",
      "tensor(10.3840, grad_fn=<AddBackward0>)\n",
      "tensor(10.1632, grad_fn=<AddBackward0>)\n",
      "tensor(10.0592, grad_fn=<AddBackward0>)\n",
      "tensor(9.9358, grad_fn=<AddBackward0>)\n",
      "tensor(9.8227, grad_fn=<AddBackward0>)\n",
      "tensor(9.7008, grad_fn=<AddBackward0>)\n",
      "tensor(9.6143, grad_fn=<AddBackward0>)\n",
      "tensor(9.4758, grad_fn=<AddBackward0>)\n",
      "tensor(9.3703, grad_fn=<AddBackward0>)\n",
      "tensor(9.2605, grad_fn=<AddBackward0>)\n",
      "tensor(9.2125, grad_fn=<AddBackward0>)\n",
      "tensor(9.0462, grad_fn=<AddBackward0>)\n",
      "tensor(8.9760, grad_fn=<AddBackward0>)\n",
      "tensor(8.8731, grad_fn=<AddBackward0>)\n",
      "tensor(8.7622, grad_fn=<AddBackward0>)\n",
      "tensor(8.6544, grad_fn=<AddBackward0>)\n",
      "tensor(8.5587, grad_fn=<AddBackward0>)\n",
      "tensor(8.4162, grad_fn=<AddBackward0>)\n",
      "tensor(8.2815, grad_fn=<AddBackward0>)\n",
      "tensor(8.1596, grad_fn=<AddBackward0>)\n",
      "tensor(8.0710, grad_fn=<AddBackward0>)\n",
      "tensor(7.9575, grad_fn=<AddBackward0>)\n",
      "tensor(7.8707, grad_fn=<AddBackward0>)\n",
      "tensor(7.7585, grad_fn=<AddBackward0>)\n",
      "tensor(7.6762, grad_fn=<AddBackward0>)\n",
      "tensor(7.5870, grad_fn=<AddBackward0>)\n",
      "tensor(7.5596, grad_fn=<AddBackward0>)\n",
      "tensor(7.4921, grad_fn=<AddBackward0>)\n",
      "tensor(7.4058, grad_fn=<AddBackward0>)\n",
      "tensor(7.3149, grad_fn=<AddBackward0>)\n",
      "tensor(7.1819, grad_fn=<AddBackward0>)\n",
      "tensor(7.2033, grad_fn=<AddBackward0>)\n",
      "tensor(6.9916, grad_fn=<AddBackward0>)\n",
      "tensor(6.9871, grad_fn=<AddBackward0>)\n",
      "tensor(6.9321, grad_fn=<AddBackward0>)\n",
      "tensor(6.7563, grad_fn=<AddBackward0>)\n",
      "tensor(6.7047, grad_fn=<AddBackward0>)\n",
      "tensor(6.7601, grad_fn=<AddBackward0>)\n",
      "tensor(6.5636, grad_fn=<AddBackward0>)\n",
      "tensor(6.4602, grad_fn=<AddBackward0>)\n",
      "tensor(6.3603, grad_fn=<AddBackward0>)\n",
      "tensor(6.2580, grad_fn=<AddBackward0>)\n",
      "tensor(6.2196, grad_fn=<AddBackward0>)\n",
      "tensor(6.0947, grad_fn=<AddBackward0>)\n",
      "tensor(5.9498, grad_fn=<AddBackward0>)\n",
      "tensor(5.9065, grad_fn=<AddBackward0>)\n",
      "tensor(5.7405, grad_fn=<AddBackward0>)\n",
      "tensor(5.7367, grad_fn=<AddBackward0>)\n",
      "tensor(5.5932, grad_fn=<AddBackward0>)\n",
      "tensor(5.4995, grad_fn=<AddBackward0>)\n",
      "tensor(5.4247, grad_fn=<AddBackward0>)\n",
      "tensor(5.2901, grad_fn=<AddBackward0>)\n",
      "tensor(5.1869, grad_fn=<AddBackward0>)\n",
      "tensor(5.1222, grad_fn=<AddBackward0>)\n",
      "tensor(4.9534, grad_fn=<AddBackward0>)\n",
      "tensor(4.9278, grad_fn=<AddBackward0>)\n",
      "tensor(4.8276, grad_fn=<AddBackward0>)\n",
      "tensor(4.7520, grad_fn=<AddBackward0>)\n",
      "tensor(4.6485, grad_fn=<AddBackward0>)\n",
      "tensor(4.6535, grad_fn=<AddBackward0>)\n",
      "tensor(4.5213, grad_fn=<AddBackward0>)\n",
      "tensor(4.3947, grad_fn=<AddBackward0>)\n",
      "tensor(4.2958, grad_fn=<AddBackward0>)\n",
      "tensor(4.1509, grad_fn=<AddBackward0>)\n",
      "tensor(4.0887, grad_fn=<AddBackward0>)\n",
      "tensor(4.0340, grad_fn=<AddBackward0>)\n",
      "tensor(3.8923, grad_fn=<AddBackward0>)\n",
      "tensor(3.8652, grad_fn=<AddBackward0>)\n",
      "tensor(3.9776, grad_fn=<AddBackward0>)\n",
      "tensor(3.7573, grad_fn=<AddBackward0>)\n",
      "tensor(3.6308, grad_fn=<AddBackward0>)\n",
      "tensor(3.6303, grad_fn=<AddBackward0>)\n",
      "tensor(3.3987, grad_fn=<AddBackward0>)\n",
      "tensor(3.3418, grad_fn=<AddBackward0>)\n",
      "tensor(3.4999, grad_fn=<AddBackward0>)\n",
      "tensor(3.2062, grad_fn=<AddBackward0>)\n",
      "tensor(5.0155, grad_fn=<AddBackward0>)\n",
      "tensor(3.5807, grad_fn=<AddBackward0>)\n",
      "tensor(3.5021, grad_fn=<AddBackward0>)\n",
      "tensor(4.8811, grad_fn=<AddBackward0>)\n",
      "tensor(3.3560, grad_fn=<AddBackward0>)\n",
      "tensor(3.2110, grad_fn=<AddBackward0>)\n",
      "tensor(3.1363, grad_fn=<AddBackward0>)\n",
      "tensor(3.3137, grad_fn=<AddBackward0>)\n",
      "tensor(6.6295, grad_fn=<AddBackward0>)\n",
      "tensor(3.7070, grad_fn=<AddBackward0>)\n",
      "tensor(7.4792, grad_fn=<AddBackward0>)\n",
      "tensor(3.8788, grad_fn=<AddBackward0>)\n",
      "tensor(3.7095, grad_fn=<AddBackward0>)\n",
      "tensor(3.4792, grad_fn=<AddBackward0>)\n",
      "tensor(3.5113, grad_fn=<AddBackward0>)\n",
      "tensor(3.8048, grad_fn=<AddBackward0>)\n",
      "tensor(3.9666, grad_fn=<AddBackward0>)\n",
      "tensor(3.5068, grad_fn=<AddBackward0>)\n",
      "tensor(3.3084, grad_fn=<AddBackward0>)\n",
      "tensor(2.9946, grad_fn=<AddBackward0>)\n",
      "tensor(2.8157, grad_fn=<AddBackward0>)\n",
      "tensor(2.7447, grad_fn=<AddBackward0>)\n",
      "tensor(2.6603, grad_fn=<AddBackward0>)\n",
      "tensor(2.6682, grad_fn=<AddBackward0>)\n",
      "tensor(2.5202, grad_fn=<AddBackward0>)\n",
      "tensor(2.6404, grad_fn=<AddBackward0>)\n",
      "tensor(2.2520, grad_fn=<AddBackward0>)\n",
      "tensor(2.1457, grad_fn=<AddBackward0>)\n",
      "tensor(2.1592, grad_fn=<AddBackward0>)\n",
      "tensor(2.0063, grad_fn=<AddBackward0>)\n",
      "tensor(2.1182, grad_fn=<AddBackward0>)\n",
      "tensor(1.9397, grad_fn=<AddBackward0>)\n",
      "tensor(1.9302, grad_fn=<AddBackward0>)\n",
      "tensor(1.7782, grad_fn=<AddBackward0>)\n",
      "tensor(1.6659, grad_fn=<AddBackward0>)\n",
      "tensor(1.6220, grad_fn=<AddBackward0>)\n",
      "tensor(1.6412, grad_fn=<AddBackward0>)\n",
      "tensor(1.5387, grad_fn=<AddBackward0>)\n",
      "tensor(1.5043, grad_fn=<AddBackward0>)\n",
      "tensor(1.4242, grad_fn=<AddBackward0>)\n",
      "tensor(1.3456, grad_fn=<AddBackward0>)\n",
      "tensor(1.3055, grad_fn=<AddBackward0>)\n",
      "tensor(1.2612, grad_fn=<AddBackward0>)\n",
      "tensor(1.2195, grad_fn=<AddBackward0>)\n",
      "tensor(1.1579, grad_fn=<AddBackward0>)\n",
      "tensor(1.1127, grad_fn=<AddBackward0>)\n",
      "tensor(1.0722, grad_fn=<AddBackward0>)\n",
      "tensor(1.0010, grad_fn=<AddBackward0>)\n",
      "tensor(0.9689, grad_fn=<AddBackward0>)\n",
      "tensor(0.9199, grad_fn=<AddBackward0>)\n",
      "tensor(0.9068, grad_fn=<AddBackward0>)\n",
      "tensor(0.8451, grad_fn=<AddBackward0>)\n",
      "tensor(0.7911, grad_fn=<AddBackward0>)\n",
      "tensor(0.7580, grad_fn=<AddBackward0>)\n",
      "tensor(0.6846, grad_fn=<AddBackward0>)\n",
      "tensor(0.7250, grad_fn=<AddBackward0>)\n",
      "tensor(0.6411, grad_fn=<AddBackward0>)\n",
      "tensor(0.6040, grad_fn=<AddBackward0>)\n",
      "tensor(0.5490, grad_fn=<AddBackward0>)\n",
      "tensor(0.5254, grad_fn=<AddBackward0>)\n",
      "tensor(0.5236, grad_fn=<AddBackward0>)\n",
      "tensor(0.4728, grad_fn=<AddBackward0>)\n",
      "tensor(0.4621, grad_fn=<AddBackward0>)\n",
      "tensor(0.4078, grad_fn=<AddBackward0>)\n",
      "tensor(0.3969, grad_fn=<AddBackward0>)\n",
      "tensor(0.3787, grad_fn=<AddBackward0>)\n",
      "tensor(0.3521, grad_fn=<AddBackward0>)\n",
      "tensor(0.3404, grad_fn=<AddBackward0>)\n",
      "tensor(0.3221, grad_fn=<AddBackward0>)\n",
      "tensor(0.3193, grad_fn=<AddBackward0>)\n",
      "tensor(0.2945, grad_fn=<AddBackward0>)\n",
      "tensor(0.2776, grad_fn=<AddBackward0>)\n",
      "tensor(0.2640, grad_fn=<AddBackward0>)\n",
      "tensor(0.2620, grad_fn=<AddBackward0>)\n",
      "tensor(0.2505, grad_fn=<AddBackward0>)\n",
      "tensor(0.2386, grad_fn=<AddBackward0>)\n",
      "tensor(0.2346, grad_fn=<AddBackward0>)\n",
      "tensor(0.2264, grad_fn=<AddBackward0>)\n",
      "tensor(0.2147, grad_fn=<AddBackward0>)\n",
      "tensor(0.2136, grad_fn=<AddBackward0>)\n",
      "tensor(0.2005, grad_fn=<AddBackward0>)\n",
      "tensor(0.1929, grad_fn=<AddBackward0>)\n",
      "tensor(0.1915, grad_fn=<AddBackward0>)\n",
      "tensor(0.1851, grad_fn=<AddBackward0>)\n",
      "tensor(0.1813, grad_fn=<AddBackward0>)\n",
      "tensor(0.1779, grad_fn=<AddBackward0>)\n",
      "tensor(0.1803, grad_fn=<AddBackward0>)\n",
      "tensor(0.1659, grad_fn=<AddBackward0>)\n",
      "tensor(0.1656, grad_fn=<AddBackward0>)\n",
      "tensor(0.1661, grad_fn=<AddBackward0>)\n",
      "tensor(0.1694, grad_fn=<AddBackward0>)\n",
      "tensor(0.1584, grad_fn=<AddBackward0>)\n",
      "tensor(0.1558, grad_fn=<AddBackward0>)\n",
      "tensor(0.1593, grad_fn=<AddBackward0>)\n",
      "tensor(0.1532, grad_fn=<AddBackward0>)\n",
      "tensor(0.1601, grad_fn=<AddBackward0>)\n",
      "tensor(0.1528, grad_fn=<AddBackward0>)\n",
      "tensor(0.1542, grad_fn=<AddBackward0>)\n",
      "tensor(0.1493, grad_fn=<AddBackward0>)\n",
      "tensor(0.1515, grad_fn=<AddBackward0>)\n",
      "tensor(0.1542, grad_fn=<AddBackward0>)\n",
      "tensor(0.1477, grad_fn=<AddBackward0>)\n",
      "tensor(0.1481, grad_fn=<AddBackward0>)\n",
      "tensor(0.1471, grad_fn=<AddBackward0>)\n",
      "tensor(0.1486, grad_fn=<AddBackward0>)\n",
      "tensor(0.1459, grad_fn=<AddBackward0>)\n",
      "tensor(0.1488, grad_fn=<AddBackward0>)\n",
      "tensor(0.1461, grad_fn=<AddBackward0>)\n",
      "tensor(0.1452, grad_fn=<AddBackward0>)\n",
      "tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "tensor(0.1459, grad_fn=<AddBackward0>)\n",
      "tensor(0.1433, grad_fn=<AddBackward0>)\n",
      "tensor(0.1446, grad_fn=<AddBackward0>)\n",
      "tensor(0.1430, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1467, grad_fn=<AddBackward0>)\n",
      "tensor(0.1447, grad_fn=<AddBackward0>)\n",
      "tensor(0.1435, grad_fn=<AddBackward0>)\n",
      "tensor(0.1454, grad_fn=<AddBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "tensor(0.1438, grad_fn=<AddBackward0>)\n",
      "tensor(0.1435, grad_fn=<AddBackward0>)\n",
      "tensor(0.1428, grad_fn=<AddBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "tensor(0.1416, grad_fn=<AddBackward0>)\n",
      "tensor(0.1420, grad_fn=<AddBackward0>)\n",
      "tensor(0.1418, grad_fn=<AddBackward0>)\n",
      "tensor(0.1422, grad_fn=<AddBackward0>)\n",
      "tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "tensor(0.1413, grad_fn=<AddBackward0>)\n",
      "tensor(0.1415, grad_fn=<AddBackward0>)\n",
      "tensor(0.1407, grad_fn=<AddBackward0>)\n",
      "tensor(0.1427, grad_fn=<AddBackward0>)\n",
      "tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "tensor(0.1430, grad_fn=<AddBackward0>)\n",
      "tensor(0.1401, grad_fn=<AddBackward0>)\n",
      "tensor(0.1403, grad_fn=<AddBackward0>)\n",
      "tensor(0.1402, grad_fn=<AddBackward0>)\n",
      "tensor(0.1405, grad_fn=<AddBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "tensor(0.1401, grad_fn=<AddBackward0>)\n",
      "tensor(0.1400, grad_fn=<AddBackward0>)\n",
      "tensor(0.1400, grad_fn=<AddBackward0>)\n",
      "tensor(0.1407, grad_fn=<AddBackward0>)\n",
      "tensor(0.1406, grad_fn=<AddBackward0>)\n",
      "tensor(0.1401, grad_fn=<AddBackward0>)\n",
      "tensor(0.1400, grad_fn=<AddBackward0>)\n",
      "tensor(0.1402, grad_fn=<AddBackward0>)\n",
      "tensor(0.1397, grad_fn=<AddBackward0>)\n",
      "tensor(0.1404, grad_fn=<AddBackward0>)\n",
      "tensor(0.1404, grad_fn=<AddBackward0>)\n",
      "tensor(0.1396, grad_fn=<AddBackward0>)\n",
      "tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "tensor(0.1394, grad_fn=<AddBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "tensor(0.1390, grad_fn=<AddBackward0>)\n",
      "tensor(0.1395, grad_fn=<AddBackward0>)\n",
      "tensor(0.1393, grad_fn=<AddBackward0>)\n",
      "tensor(0.1393, grad_fn=<AddBackward0>)\n",
      "tensor(0.1396, grad_fn=<AddBackward0>)\n",
      "tensor(0.1393, grad_fn=<AddBackward0>)\n",
      "tensor(0.1388, grad_fn=<AddBackward0>)\n",
      "tensor(0.1396, grad_fn=<AddBackward0>)\n",
      "tensor(0.1388, grad_fn=<AddBackward0>)\n",
      "tensor(0.1392, grad_fn=<AddBackward0>)\n",
      "tensor(0.1396, grad_fn=<AddBackward0>)\n",
      "tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "tensor(0.1400, grad_fn=<AddBackward0>)\n",
      "tensor(0.1392, grad_fn=<AddBackward0>)\n",
      "tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "tensor(0.1390, grad_fn=<AddBackward0>)\n",
      "tensor(0.1386, grad_fn=<AddBackward0>)\n",
      "tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "tensor(0.1383, grad_fn=<AddBackward0>)\n",
      "tensor(0.1390, grad_fn=<AddBackward0>)\n",
      "tensor(0.1388, grad_fn=<AddBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "tensor(0.1381, grad_fn=<AddBackward0>)\n",
      "tensor(0.1386, grad_fn=<AddBackward0>)\n",
      "tensor(0.1385, grad_fn=<AddBackward0>)\n",
      "tensor(0.1383, grad_fn=<AddBackward0>)\n",
      "tensor(0.1381, grad_fn=<AddBackward0>)\n",
      "tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "tensor(0.1382, grad_fn=<AddBackward0>)\n",
      "tensor(0.1382, grad_fn=<AddBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "tensor(0.1383, grad_fn=<AddBackward0>)\n",
      "tensor(0.1382, grad_fn=<AddBackward0>)\n",
      "tensor(0.1377, grad_fn=<AddBackward0>)\n",
      "tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "tensor(0.1382, grad_fn=<AddBackward0>)\n",
      "tensor(0.1382, grad_fn=<AddBackward0>)\n",
      "tensor(0.1381, grad_fn=<AddBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "tensor(0.1380, grad_fn=<AddBackward0>)\n",
      "tensor(0.1377, grad_fn=<AddBackward0>)\n",
      "tensor(0.1377, grad_fn=<AddBackward0>)\n",
      "tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "tensor(0.1381, grad_fn=<AddBackward0>)\n",
      "tensor(0.1386, grad_fn=<AddBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "tensor(0.1380, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epoch = 300\n",
    "#ECG_LSTM(hidden_layers, embedding_dim, num_words)\n",
    "model = FNETModel(expansion = 2, dropout = 0.1, num_layers = 8, embedder = encoder_conv, decoder = decoder)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for i in range(epoch):\n",
    "    losses = 0\n",
    "    for j, k in zip(train_x, train_y):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(j.unsqueeze(0)).squeeze(0)\n",
    "        loss = loss_fn(outputs, k)\n",
    "        losses += loss\n",
    "    losses.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 30)\n",
      "[20 16 18 23 21 24 10 25 24  1 21 23 29 29 29 29 29 29 29 29]\n",
      "tensor([20, 16, 18, 23, 21, 24, 10, 25, 24,  1, 21, 23, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29])\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'model/fnet.pt')\n",
    "model = FNETModel(expansion = 2, dropout = 0.1, num_layers = 8, embedder = encoder_conv, decoder = decoder)\n",
    "model.load_state_dict(torch.load('model/fnet.pt'))\n",
    "\n",
    "out = (model(train_x[1].unsqueeze(0)))\n",
    "print(out.squeeze(0).detach().numpy().shape)\n",
    "out = np.argmax(out.squeeze(0).detach().numpy(), axis = 1)\n",
    "print(out)\n",
    "print(train_y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "#encoded_inputs = tokenizer(list(waveform_lead_rhythm_diag['diagnosis'])[0], return_tensors='pt')\n",
    "\n",
    "encoded_inputs = tokenizer('Hello my name is Daniel', return_tensors = 'pt')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 - FNET/Basic Mixup Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c0966bbf36472bb65e233f82b02a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608b0c2be36d4d2d929f2d62602bcd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea77f14fcacf4ba19aa9b3642ab5d2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cefd1b4c384ce8a8f2cffcad772a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957e0fae0bc24df99a59f315a04bcc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "logits = outputs.logits\n",
    "assert logits[0, 0] < logits[0, 1] # next sentence was random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
