{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from base64 import b64decode as decode\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.fft as fft\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2PreTrainedModel, GPT2Model\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "from typing import Optional, Tuple\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# The only time we need to define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[548561, 'sinus bradycardia; otherwise normal ecg']\n",
      "[548592, 'normal sinus rhythm; normal ecg']\n",
      "[548593, 'normal sinus rhythm; normal ecg']\n",
      "[548609, 'normal sinus rhythm; normal ecg']\n",
      "[548759, 'normal sinus rhythm; normal sinus rhythm; low voltage qrs; low voltage qrs; borderline ecg; borderline ecg']\n",
      "[549810, 'normal sinus rhythm; left axis deviation; right bundle branch block; abnormal ecg']\n",
      "[549871, 'sinus bradycardia; sinus bradycardia; otherwise normal ecg; otherwise normal ecg']\n",
      "[549964, 'sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg']\n",
      "[550065, 'normal sinus rhythm; with sinus arrhythmia; cannot rule out; inferior infarct; age undetermined; abnormal ecg']\n",
      "[550307, 'normal sinus rhythm t wave abnormality, consider anterior ischemia; prolonged qt; abnormal ecg; t wave inversion now evident in; anterior leads; qt has lengthened']\n",
      "[550391, 'sinus bradycardia premature atrial complexes; in a pattern of bigeminy; minimal voltage criteria for lvh, may be normal variant; borderline ecg; premature atrial complexes; are now; present']\n",
      "[550416, 'sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg']\n",
      "[550432, 'normal sinus rhythm; normal ecg']\n",
      "[550602, 'sinus tachycardia; sinus tachycardia; otherwise normal ecg; otherwise normal ecg']\n",
      "[550715, 'atrial fibrillation; left axis deviation; pulmonary disease pattern; nonspecific st and t wave abnormality;  ; abnormal ecg']\n",
      "[550802, 'normal sinus rhythm; left axis deviation; abnormal ecg']\n",
      "[550912, 'atrial fibrillation; with a competing junctional pacemaker; with premature ventricular or aberrantly conducted complexes; possible; right ventricular hypertrophy; cannot rule out; anterior infarct; age undetermined; abnormal ecg anterior infarct; are now; present; t wave amplitude has increased in; anterolateral leads']\n",
      "[551206, 'normal sinus rhythm; possible; left atrial enlargement; incomplete left bundle branch block; borderline ecg']\n",
      "[551243, 'normal sinus rhythm; normal ecg; incomplete left bundle branch block; is no longer; present']\n",
      "[551344, 'sinus tachycardia; left axis deviation; pulmonary disease pattern; inferior infarct; age undetermined t wave abnormality, consider lateral ischemia; abnormal ecg; sinus rhythm; has replaced; atrial fibrillation; inverted t waves have replaced nonspecific t wave abnormality in; lateral leads']\n",
      "[551407, 'sinus rhythm fusion complexes; right bundle branch block; left posterior fascicular block; abnormal ecg; sinus rhythm; has replaced; electronic ventricular pacemaker']\n",
      "[551415, 'atrial fibrillation; with rapid ventricular response; with premature ventricular or aberrantly conducted complexes; right bundle branch block; left posterior fascicular block; abnormal ecg; atrial fibrillation; has replaced; sinus rhythm; qrs duration; has increased']\n",
      "[551437, 'undetermined rhythm; right bundle branch block; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; qrs duration; has increased; qt has lengthened']\n",
      "[551471, 'undetermined rhythm; possible; right ventricular hypertrophy; nonspecific t wave abnormality; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; is no longer; present']\n",
      "[551485, 'normal sinus rhythm; normal sinus rhythm; normal ecg; normal ecg']\n",
      "[551560, 'normal sinus rhythm; normal ecg']\n",
      "[551761, 'electronic atrial pacemaker; indeterminate axis; pulmonary disease pattern; st elevation consider anterolateral injury or acute infarct; st elevation consider inferior injury or acute infarct; abnormal ecg; electronic atrial pacemaker; has replaced; electronic ventricular pacemaker']\n",
      "[551831, 'sinus bradycardia; otherwise normal ecg; nonspecific t wave abnormality no longer evident in; inferior leads; nonspecific t wave abnormality no longer evident in; anterolateral leads; qt has lengthened']\n",
      "[551862, 'normal sinus rhythm; normal ecg']\n",
      "[551864, 'normal sinus rhythm; normal ecg']\n",
      "[551865, 'sinus tachycardia; nonspecific st and t wave abnormality; abnormal ecg']\n",
      "[551991, 'atrial fibrillation; left axis deviation; pulmonary disease pattern; septal infarct; age undetermined; abnormal ecg; atrial fibrillation; has replaced; sinus rhythm; nonspecific t wave abnormality has replaced inverted t waves in; lateral leads']\n",
      "[551996, 'supraventricular tachycardia; nonspecific st and t wave abnormality; abnormal ecg; vent. rate; has increased; qrs duration; has increased; st now depressed in; anterior leads; nonspecific t wave abnormality now evident in; lateral leads']\n",
      "[552077, 'normal sinus rhythm; normal sinus rhythm; normal ecg; normal ecg']\n",
      "[552284, 'sinus tachycardia; right bundle branch block; left anterior fascicular block; abnormal ecg']\n",
      "[552648, 'electronic ventricular pacemaker']\n",
      "[552829, 'normal sinus rhythm; normal ecg']\n",
      "[552838, 'normal sinus rhythm; normal ecg']\n",
      "[552856, 'normal sinus rhythm; normal sinus rhythm; with sinus arrhythmia; with sinus arrhythmia; minimal voltage criteria for lvh, may be normal variant; minimal voltage criteria for lvh, may be normal variant; borderline ecg; borderline ecg']\n",
      "[552892, 'normal sinus rhythm; st elevation, consider early repolarization; borderline ecg']\n",
      "[552943, 'sinus rhythm fusion complexes; otherwise normal ecg; fusion complexes; are now; present']\n",
      "[552983, 'normal sinus rhythm; normal sinus rhythm; t wave abnormality, consider inferior ischemia; t wave abnormality, consider inferior ischemia; abnormal ecg; abnormal ecg']\n",
      "[553008, 'sinus tachycardia; with short pr premature supraventricular complexes; and; fusion complexes; right bundle branch block; cannot rule out; inferior infarct; age undetermined; t wave abnormality, consider lateral ischemia; abnormal ecg; previous ecg has undetermined rhythm, needs review; right bundle branch block; is now; present']\n",
      "[553064, 'sinus tachycardia; with short pr premature supraventricular complexes; right bundle branch block; t wave abnormality, consider lateral ischemia; abnormal ecg; premature supraventricular complexes; are now; present']\n",
      "[553089, 'av sequential or dual chamber electronic pacemaker; electronic ventricular pacemaker; has replaced; sinus rhythm; vent. rate; has decreased']\n",
      "[553115, 'atrial fibrillation; atrial fibrillation; abnormal ecg; abnormal ecg']\n",
      "[553215, 'av sequential or dual chamber electronic pacemaker']\n",
      "[553528, 'normal sinus rhythm; normal sinus rhythm; with sinus arrhythmia; with sinus arrhythmia; normal ecg; normal ecg']\n",
      "[553541, 'normal sinus rhythm; normal ecg']\n",
      "[553542, 'normal sinus rhythm; normal ecg']\n",
      "[553554, 'normal sinus rhythm; normal ecg']\n",
      "[553651, 'electronic ventricular pacemaker; vent. rate; has increased']\n",
      "[553804, 'normal sinus rhythm; cannot rule out; anterior infarct; age undetermined; abnormal ecg']\n",
      "[553858, 'normal sinus rhythm; normal ecg']\n",
      "[554003, 'normal sinus rhythm; left axis deviation; incomplete left bundle branch block; left ventricular hypertrophy; with repolarization abnormality; abnormal ecg; t wave inversion more evident in; lateral leads']\n",
      "[554010, 'sinus rhythm premature atrial complexes; left ventricular hypertrophy; with repolarization abnormality; abnormal ecg; premature atrial complexes; are now; present']\n",
      "[554080, 'sinus rhythm; sinus rhythm; with 1st degree a-v block; with 1st degree a-v block; prolonged qt; prolonged qt; abnormal ecg; abnormal ecg']\n",
      "[554100, 'normal sinus rhythm; possible; left atrial enlargement; incomplete right bundle branch block; borderline ecg; incomplete right bundle branch block; is now; present']\n",
      "[554104, 'normal sinus rhythm; inferior infarct; prolonged qt; abnormal ecg; nonspecific t wave abnormality, improved in; anterior leads']\n",
      "[554202, 'normal sinus rhythm; normal ecg']\n",
      "[554215, 'electronic ventricular pacemaker']\n",
      "[554280, 'normal sinus rhythm; left anterior fascicular block; abnormal ecg']\n",
      "[554321, 'normal sinus rhythm; normal ecg']\n",
      "[554415, 'sinus bradycardia; otherwise normal ecg']\n",
      "[554429, 'sinus rhythm premature atrial complexes aberrant conduction; nonspecific st abnormality; abnormal ecg; aberrant conduction; is now; present']\n",
      "[554474, 'sinus bradycardia']\n",
      "[554525, 'sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg']\n",
      "[554582, 'normal sinus rhythm; minimal voltage criteria for lvh, may be normal variant; borderline ecg']\n",
      "[554633, 'normal sinus rhythm; normal ecg']\n",
      "[554672, 'sinus bradycardia; otherwise normal ecg']\n",
      "[554708, 'sinus tachycardia; cannot rule out; anterior infarct; age undetermined; abnormal ecg']\n",
      "[554799, 'normal sinus rhythm; normal ecg']\n",
      "[555011, 'normal sinus rhythm; normal ecg']\n",
      "[555046, 'normal sinus rhythm; normal ecg']\n",
      "[555075, 'normal sinus rhythm; normal ecg']\n",
      "[555540, 'normal sinus rhythm; inferior infarct; age undetermined; abnormal ecg; t wave amplitude has decreased in; lateral leads']\n",
      "[555635, 'undetermined rhythm; right axis deviation; right ventricular hypertrophy; nonspecific st and t wave abnormality; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; questionable change in; qrs duration']\n",
      "[555649, 'electronic ventricular pacemaker; previous ecg has undetermined rhythm, needs review']\n",
      "[555655, 'normal sinus rhythm; inferior infarct; age undetermined; abnormal ecg; inverted t waves have replaced nonspecific t wave abnormality in; inferior leads']\n",
      "[555846, 'normal sinus rhythm; normal ecg; nonspecific t wave abnormality has replaced inverted t waves in; inferior leads']\n",
      "[555885, 'normal sinus rhythm; cannot rule out; anterior infarct; age undetermined; abnormal ecg']\n",
      "[555907, 'normal sinus rhythm; possible; anterior infarct; abnormal ecg']\n",
      "[555923, 'normal sinus rhythm; normal ecg']\n",
      "[556067, 'normal sinus rhythm; inferior infarct; prolonged qt; abnormal ecg; t wave inversion now evident in; lateral leads']\n",
      "[556130, 'normal sinus rhythm; normal ecg']\n",
      "[556183, 'normal sinus rhythm; left axis deviation; abnormal ecg']\n",
      "[556190, 'normal sinus rhythm; inferior infarct; prolonged qt; abnormal ecg; t wave inversion no longer evident in; lateral leads']\n",
      "[556203, 'sinus bradycardia; otherwise normal ecg']\n",
      "[556225, 'normal sinus rhythm; normal ecg septal infarct; are no longer; present']\n",
      "[556303, 'normal sinus rhythm; normal ecg']\n",
      "[556328, 'normal sinus rhythm; inferior infarct; prolonged qt; abnormal ecg']\n",
      "[556329, 'normal sinus rhythm; st abnormality, possible digitalis effect; abnormal ecg']\n",
      "[556426, 'normal sinus rhythm; normal ecg']\n",
      "[556562, 'electronic ventricular pacemaker; vent. rate; has increased']\n",
      "[556642, 'electronic ventricular pacemaker; vent. rate; has decreased']\n",
      "[556737, \"normal sinus rhythm; with sinus arrhythmia; rsr' or qr pattern in v1 suggests right ventricular conduction delay; borderline ecg\"]\n",
      "[556808, 'sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg']\n",
      "[556851, 'normal sinus rhythm; normal ecg']\n",
      "[556934, 'normal sinus rhythm; minimal voltage criteria for lvh, may be normal variant; borderline ecg']\n",
      "sinus bradycardia; otherwise normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg\n",
      "normal sinus rhythm; with sinus arrhythmia; cannot rule out; inferior infarct; age undetermined; abnormal ecg\n",
      "normal sinus rhythm t wave abnormality, consider anterior ischemia; prolonged qt; abnormal ecg; t wave inversion now evident in; anterior leads; qt has lengthened\n",
      "sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg\n",
      "normal sinus rhythm; possible; left atrial enlargement; incomplete left bundle branch block; borderline ecg\n",
      "normal sinus rhythm; normal ecg; incomplete left bundle branch block; is no longer; present\n",
      "sinus tachycardia; left axis deviation; pulmonary disease pattern; inferior infarct; age undetermined t wave abnormality, consider lateral ischemia; abnormal ecg; sinus rhythm; has replaced; atrial fibrillation; inverted t waves have replaced nonspecific t wave abnormality in; lateral leads\n",
      "atrial fibrillation; with rapid ventricular response; with premature ventricular or aberrantly conducted complexes; right bundle branch block; left posterior fascicular block; abnormal ecg; atrial fibrillation; has replaced; sinus rhythm; qrs duration; has increased\n",
      "undetermined rhythm; right bundle branch block; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; qrs duration; has increased; qt has lengthened\n",
      "undetermined rhythm; possible; right ventricular hypertrophy; nonspecific t wave abnormality; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; is no longer; present\n",
      "normal sinus rhythm; normal ecg\n",
      "electronic atrial pacemaker; indeterminate axis; pulmonary disease pattern; st elevation consider anterolateral injury or acute infarct; st elevation consider inferior injury or acute infarct; abnormal ecg; electronic atrial pacemaker; has replaced; electronic ventricular pacemaker\n",
      "normal sinus rhythm; normal ecg\n",
      "atrial fibrillation; left axis deviation; pulmonary disease pattern; septal infarct; age undetermined; abnormal ecg; atrial fibrillation; has replaced; sinus rhythm; nonspecific t wave abnormality has replaced inverted t waves in; lateral leads\n",
      "sinus tachycardia; with short pr premature supraventricular complexes; and; fusion complexes; right bundle branch block; cannot rule out; inferior infarct; age undetermined; t wave abnormality, consider lateral ischemia; abnormal ecg; previous ecg has undetermined rhythm, needs review; right bundle branch block; is now; present\n",
      "sinus tachycardia; with short pr premature supraventricular complexes; right bundle branch block; t wave abnormality, consider lateral ischemia; abnormal ecg; premature supraventricular complexes; are now; present\n",
      "av sequential or dual chamber electronic pacemaker; electronic ventricular pacemaker; has replaced; sinus rhythm; vent. rate; has decreased\n",
      "av sequential or dual chamber electronic pacemaker\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; cannot rule out; anterior infarct; age undetermined; abnormal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; possible; left atrial enlargement; incomplete right bundle branch block; borderline ecg; incomplete right bundle branch block; is now; present\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "sinus bradycardia\n",
      "sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg\n",
      "normal sinus rhythm; minimal voltage criteria for lvh, may be normal variant; borderline ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "sinus bradycardia; otherwise normal ecg\n",
      "sinus tachycardia; cannot rule out; anterior infarct; age undetermined; abnormal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; inferior infarct; age undetermined; abnormal ecg; t wave amplitude has decreased in; lateral leads\n",
      "electronic ventricular pacemaker; previous ecg has undetermined rhythm, needs review\n",
      "normal sinus rhythm; inferior infarct; age undetermined; abnormal ecg; inverted t waves have replaced nonspecific t wave abnormality in; inferior leads\n",
      "normal sinus rhythm; possible; anterior infarct; abnormal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; inferior infarct; prolonged qt; abnormal ecg; t wave inversion no longer evident in; lateral leads\n",
      "normal sinus rhythm; normal ecg septal infarct; are no longer; present\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; inferior infarct; prolonged qt; abnormal ecg\n",
      "normal sinus rhythm; st abnormality, possible digitalis effect; abnormal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "electronic ventricular pacemaker; vent. rate; has decreased\n",
      "sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg\n"
     ]
    }
   ],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16).astype(np.float32)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/combined_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/combined_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/combined_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/combined_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead dataz\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "\n",
    "\n",
    "# adding the diagnosis and labels\n",
    "waveform_and_diag = pd.merge(waveform_lead[['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']], diagnosis_data[[\"exam_id\", \"Full_text\", \"Original_Diag\"]], left_on= \"exam_id\", right_on=\"exam_id\")\n",
    "\n",
    "\n",
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "#waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat = waveform_lead_concat[waveform_lead_concat[\"decoded_waveform\"].apply(lambda x: len(x[0]) == 2500)]\n",
    "waveform_lead_concat = waveform_lead_concat[waveform_lead_concat[\"decoded_waveform\"].apply(lambda x: len(x) == 8)]\n",
    "   \n",
    "\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: np.vstack(x))\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "\n",
    "waveform_lead_rhythm[\"decoded_waveform\"] = waveform_lead_rhythm[\"decoded_waveform\"].apply(lambda value: MinMaxScaler().fit_transform(value))\n",
    "\n",
    "\n",
    "\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "\n",
    "\n",
    "# making the tokens\n",
    "tokens = set()\n",
    "\n",
    "\n",
    "prefixed_phrase = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    if row[\"exam_id\"] != curr_id and curr_string != \"\":\n",
    "        curr_string = curr_string.lower()\n",
    "        curr_string = curr_string.replace(\"     \", \"\").replace(\" ,\", \"\")\n",
    "        val = [curr_id, curr_string[2:]]\n",
    "        print(val)\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    \n",
    "    if \"*\" in row[\"Full_text\"] or \"(\" in row[\"Full_text\"]:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    if row[\"Full_text\"][-3:] == \"for\" or row[\"Full_text\"][-4:] == \"with\" or row[\"Full_text\"][-1] == \"&\":\n",
    "        prefixed_phrase = row[\"Full_text\"].lower() + \" \"\n",
    "        curr_string += \"@\"\n",
    "        continue\n",
    "    \n",
    "    if curr_string and curr_string[-1] == \"@\":\n",
    "        curr_string = curr_string[:-1]\n",
    "        curr_string += \" \" + row[\"Full_text\"]\n",
    "    else:\n",
    "        curr_string += \"; \" + row[\"Full_text\"]\n",
    "    \n",
    "    tokens.add(prefixed_phrase + row[\"Full_text\"].lower())\n",
    "    prefixed_phrase = \"\"\n",
    "    \n",
    "    \n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "full_x = torch.tensor(waveform_lead_rhythm_diag['decoded_waveform']).float()\n",
    "full_y = waveform_lead_rhythm_diag['diagnosis']\n",
    "for i in full_y:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "tokens = list(tokens)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordPiece, WordLevel\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer, WordLevelTrainer\n",
    "\n",
    "\n",
    "class custom_tokenizer(nn.Module):\n",
    "    def __init__(self, vocab, max_len=22):\n",
    "        super(custom_tokenizer, self).__init__()\n",
    "        self.tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        self.trainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "        self.tokenizer.train_from_iterator(np.array(tokens), trainer)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, x):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sentence in x:\n",
    "            tokenized = self.tokenizer.encode(sentence.split(\"; \"), is_pretokenized=True)\n",
    "            input_id = [0] + tokenized.ids + [0 for i in range(self.max_len - len(tokenized.ids))]\n",
    "            attention_mask = [1] + tokenized.attention_mask + [0 for i in range(self.max_len - len(tokenized.attention_mask))]\n",
    "            input_ids.append(input_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "        return {\"input_ids\": torch.tensor(input_ids).detach(), \"attention_mask\": torch.tensor(attention_masks).detach()}\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.tokenizer.decode(list(x))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# BPE, Unigram, WordPiece, WordLevel,  unk_token=\"[UNK]\"\n",
    "transformer_tokenizer = custom_tokenizer(vocab=tokens)\n",
    "print(len(transformer_tokenizer.tokenizer.get_vocab()))\n",
    "output = transformer_tokenizer(full_y)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we define components to be used for both the Conv1D encoder, and a Conv1D pre-embedder into a Transformer Encoder.\n",
    "\n",
    "LR = 1e-3\n",
    "KER_SIZE = 11\n",
    "PADDING = 5\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 1: ResNet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv_0): Conv1d(8, 16, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_0): ELU(alpha=1.0)\n",
      "  (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_0): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_0): ELU(alpha=1.0)\n",
      "  (conv_1): Conv1d(16, 32, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_1): ELU(alpha=1.0)\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_1): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_1): ELU(alpha=1.0)\n",
      "  (conv_2): Conv1d(32, 64, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_2): ELU(alpha=1.0)\n",
      "  (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_2): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_2): ELU(alpha=1.0)\n",
      "  (conv_3): Conv1d(64, 128, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_3): ELU(alpha=1.0)\n",
      "  (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_3): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_3): ELU(alpha=1.0)\n",
      "  (conv_fin): Conv1d(128, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_fin): ELU(alpha=1.0)\n",
      "  (batch_fin): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 256, 157])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(x):\n",
    "    if isinstance(x, nn.Conv1d):\n",
    "        nn.init.kaiming_uniform_(x.weight, mode='fan_in', nonlinearity='relu')\n",
    "        x.bias.data.fill_(0.01)\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# build resent model and display the shape of feed through\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(4):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = 249, padding = 124, stride = 2))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ELU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = 249, padding = 124))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ELU())\n",
    "    init_channels = next_channels\n",
    "    \n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 256, kernel_size = 249, padding = 124))\n",
    "conv_model.add_module('act_fin', nn.ELU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(256))\n",
    "\n",
    "conv_model.apply(init_weights)\n",
    "\n",
    "print(conv_model)\n",
    "conv_model(full_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 256\n",
    "embedding_dim = 256\n",
    "word_list_length = 98\n",
    "start_token = end_token = 0\n",
    "\n",
    "class LSTM_Encoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim):\n",
    "        super(LSTM_Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim, num_layers = 4, bidirectional = True)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.01)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param, gain=nn.init.calculate_gain('tanh'))\n",
    "\n",
    "        \n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        if hidden is None and cell_state is None:\n",
    "            final, comp = self.lstm(x)\n",
    "        else:\n",
    "            final, comp = self.lstm(x, (hidden, cell_state))\n",
    "        hid, cell = comp\n",
    "        return final, hid, cell\n",
    "    \n",
    "    def initial_hidden_cell(self):\n",
    "        return torch.zeros(8, 1, 256), torch.zeros(8, 1, 256)\n",
    "    \n",
    "class LSTM_Decoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim, word_list_length, max_length = 157):\n",
    "        super(LSTM_Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(word_list_length, e_dim)\n",
    "        \n",
    "        self.attention = nn.Linear(h_dim*2, max_length)\n",
    "        torch.nn.init.xavier_uniform_(self.attention.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        self.attention.bias.data.fill_(0.01)\n",
    "        \n",
    "        self.attention_combined = nn.Linear(h_dim * 3, h_dim)\n",
    "        torch.nn.init.xavier_uniform_(self.attention_combined.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        self.attention_combined.bias.data.fill_(0.01)\n",
    "        \n",
    "        self.lstm = nn.LSTM(e_dim, h_dim)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.01)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param, gain=nn.init.calculate_gain('tanh'))\n",
    "\n",
    "        self.out = nn.Linear(h_dim, word_list_length)\n",
    "        torch.nn.init.xavier_uniform_(self.out.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        self.out.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, hidden, cell_state, encoder_outputs):\n",
    "        seq_embedded = self.emb(x).view(1, 1, -1)\n",
    "        \n",
    "        attention_weights = F.softmax(self.attention(torch.cat((seq_embedded[0], hidden[0]), 1)), 1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = F.relu(self.attention_combined(torch.cat((seq_embedded[0], attention_applied[0]), 1)))\n",
    "        \n",
    "        final, states  = self.lstm(output.unsqueeze(0), (hidden, cell_state))\n",
    "        hidden, cell_state = states\n",
    "        dec_seq = self.out(final)\n",
    "        return F.log_softmax(dec_seq[0], dim = -1), hidden, cell_state\n",
    "    \n",
    "    def initial_hidden_cell(self):\n",
    "        return torch.zeros(1, 1, 256), torch.zeros(1, 1, 256)\n",
    "\n",
    "def train(x, y, embedder, encoder, decoder, emb_optimizer, enc_optimizer, dec_optimizer, teacher_ratio = 0.5):\n",
    "    hidden_enc, cell_enc = encoder.initial_hidden_cell()\n",
    "    enc_outputs = torch.zeros(157, hidden_layers * 2)\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    loss = 0\n",
    "    \n",
    "    emb_optimizer.zero_grad()\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    \n",
    "    emb_x = embedder(x.unsqueeze(0)).permute(2, 0, 1)\n",
    "    \n",
    "    for i in range(len(emb_x)):\n",
    "        seq = emb_x[i].unsqueeze(0)\n",
    "        enc_out, hidden_enc, cell_enc = encoder(seq, hidden_enc, cell_enc)\n",
    "        enc_outputs[i] = enc_out[0, 0]\n",
    "    \n",
    "    hidden_dec, cell_dec = decoder.initial_hidden_cell()\n",
    "    target_lab = torch.tensor(y, dtype = torch.long)\n",
    "    decoder_input = torch.tensor([[start_token]], dtype = torch.long)\n",
    "    teacher_forcing = True if torch.rand(1) <= teacher_ratio else False\n",
    "    fin_len = 0\n",
    "    if teacher_forcing:\n",
    "        for j in range(len(target_lab)):\n",
    "            logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "            current_targ = target_lab[j].unsqueeze(0)\n",
    "            loss = loss_fn(logit, current_targ) + loss\n",
    "            decoder_input = current_targ\n",
    "            fin_len = j + 1\n",
    "            if decoder_input == end_token:\n",
    "                break\n",
    "    else:\n",
    "        for j in range(len(target_lab)):\n",
    "            logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "            _, val = logit.topk(1)\n",
    "            current_targ = target_lab[j].unsqueeze(0)\n",
    "            loss = loss_fn(logit, current_targ) + loss\n",
    "            decoder_input = val.squeeze(0).detach()\n",
    "            fin_len = j + 1\n",
    "            if decoder_input == end_token:\n",
    "                print(\"trained without teacher enforcing\")\n",
    "                break\n",
    "    \n",
    "    loss.backward()\n",
    "        \n",
    "    emb_optimizer.step()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    \n",
    "    return (loss.item() / fin_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  del sys.path[0]\n",
      "/Users/danielbang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "def append_end_token(i):\n",
    "    i.append(end_token)\n",
    "    return i\n",
    "\n",
    "# define tokenizer\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "labels = list(full_y)\n",
    "for i, sentence in enumerate(labels):\n",
    "    labels[i] = sentence.split(\"; \")\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "tokenizer.train_from_iterator(np.array(tokens), trainer)\n",
    "token_y = [tokenizer.encode(label, is_pretokenized=True).ids for label in labels]\n",
    "for i in token_y:\n",
    "    i.append(0)\n",
    "\n",
    "#token_y = [torch.cat((i, torch.tensor([0]))) for i in token_y]\n",
    "encoder = LSTM_Encoder(hidden_layers, embedding_dim)\n",
    "decoder = LSTM_Decoder(hidden_layers, embedding_dim, word_list_length)\n",
    "\n",
    "emb_optimizer = torch.optim.Adam(conv_model.parameters(), lr = 1e-3)\n",
    "enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = 1e-3)\n",
    "dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.load_state_dict(torch.load('model/lstm_embedder.pt'))\n",
    "encoder.load_state_dict(torch.load('model/lstm_encoder.pt'))\n",
    "decoder.load_state_dict(torch.load(\"model/lstm_decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "2.9374151184227006\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "2.186834273994042\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/lstm_enc_dec_part_3')\n",
    "for epoch in range(30):\n",
    "    tot = 0.0\n",
    "    for j, k in zip(full_x, token_y):\n",
    "        loss = train(j, k, conv_model, encoder, decoder, emb_optimizer, enc_optimizer, dec_optimizer, teacher_ratio = 0.8)\n",
    "        tot = tot + loss\n",
    "    avg_loss = tot / len(token_y)\n",
    "    \n",
    "    print(avg_loss)\n",
    "    info_dict = {'train_loss': avg_loss } #, 'train_acc': train_accuracy, 'val_loss': avg_val_loss,'val_acc': val_accuracy}\n",
    "           \n",
    "    for tag, value in conv_model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_emb' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_emb' + '/grad', value.grad.data.cpu().numpy(), epoch)           \n",
    "    \n",
    "    for tag, value in encoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_enc' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_enc' + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "    \n",
    "    \n",
    "    for tag, value in decoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+'_dec' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+'_dec' + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "    \n",
    "\n",
    "    for tag, value in info_dict.items():\n",
    "        writer.add_scalar(tag, value, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(conv_model.state_dict(), 'model/lstm_embedder.pt')\n",
    "torch.save(encoder.state_dict(), 'model/lstm_encoder.pt')\n",
    "torch.save(decoder.state_dict(), 'model/lstm_decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(full_x, token_y):\n",
    "    #print(x)\n",
    "    print(\"ground truth: \", \" \".join([tokenizer.id_to_token(i) for i in y]))\n",
    "    emb_x = conv_model(x.unsqueeze(0)).permute(2, 0, 1)\n",
    "    hidden_enc, cell_enc = encoder.initial_hidden_cell()\n",
    "    enc_outputs = torch.zeros(79, hidden_layers * 2)\n",
    "    \n",
    "    for i in range(len(emb_x)):\n",
    "        seq = emb_x[i].unsqueeze(0)\n",
    "        enc_out, hidden_enc, cell_enc = encoder(seq, hidden_enc, cell_enc)\n",
    "        enc_outputs[i] = enc_out[0, 0]       \n",
    "    \n",
    "    hidden_dec, cell_dec = decoder.initial_hidden_cell()\n",
    "    decoder_input = torch.tensor([[start_token]], dtype = torch.long)\n",
    "    for i in range(len(y)):\n",
    "        logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "        _, val = logit.topk(1)\n",
    "        decoder_input = val.squeeze(0).detach()\n",
    "        print(\"predicted: \", tokenizer.id_to_token(decoder_input))\n",
    "        if decoder_input == end_token:\n",
    "            break\n",
    "    print(\"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 3 - Multi-Head Attention Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress, will clean later\n",
    "\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ECGTransformerEncoder(nn.Module):\n",
    "    # Takes the ECG discrete signals sequence and maps into a probability distribution of diagnosis\n",
    "    # For working/verification purposes\n",
    "    def __init__(self, vector_size, embed_dim, n_heads, hidden_linear_dim, n_layers, dropout):\n",
    "        super(ECGTransformerEncoder, self).__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.positional_encoder = PositionalEncoder(embed_dim, dropout)\n",
    "    \n",
    "        #Since our data is already discrete numbers, might need some tweaking for this\n",
    "        self.embedder = conv_embedder\n",
    "                        #64 31              #39        64\n",
    "        \n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(embed_dim, n_heads, hidden_linear_dim, dropout),\n",
    "            n_layers)\n",
    "        \n",
    "        self.n_inputs = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Simple linear decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(768, 17),\n",
    "                        Transpose(17, 2500),\n",
    "                        nn.Linear(2500, 30),\n",
    "                        nn.LogSoftmax()\n",
    "                        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #self.embedder.weight.data.uniform_(-.1, .1)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        #self.decoder.weight.data.uniform_(-.1, .1)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.embedder(x) # * math.sqrt(self.n_inputs)\n",
    "        x = x.squeeze(0)\n",
    "        #x = x.view(2500, 8)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1) \n",
    "        #x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(features, features * expansion)\n",
    "        self.linear_2 = nn.Linear(features * expansion, features)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        #self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.norm_1(x + res)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FNETLayer(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FNETLayer, self).__init__()\n",
    "        self.feed_forward = FeedForwardNet(features, expansion, dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "        \n",
    "        # additions to add\n",
    "        self.attention_layer = nn.TransformerEncoderLayer(256, 16, 512, 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = fft.fftn(x, dim = (-2, -1)).real\n",
    "        x = self.norm_1(x + res)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.attention_layer(x)\n",
    "        return x\n",
    "    \n",
    "class FNETEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self, features, expansion=2, dropout=0.5, num_layers=6):\n",
    "        encoder_layer = FNETLayer(features, expansion, dropout)\n",
    "        super().__init__(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Encoder Helper Functions/Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    # Necessary to store positional data about the input data\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=2500):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, 1, embed_dim)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        divisor = torch.exp(torch.arange(0, embed_dim, 2).float() * (- math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pos_encoding[:, 0, 0::2] = torch.sin(position * divisor)\n",
    "        pos_encoding[:, 0, 1::2] = torch.cos(position * divisor)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoding.repeat(1, x.shape[1], 1)\n",
    "        x = x + pos_encoding[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "   \n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the number of the last batch sample in the data set is smaller than the defined batch_batch size, mismatch problems will occur. You can modify it yourself, for example, just pass in the shape behind, and then enter it through x.szie(0).\n",
    "        return x.view(self.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class WindowEmbedder(nn.Module):\n",
    "    # Necessary to convert the signal into \"word\" vectors for transformer processing.\n",
    "    # Currently a simple group and slice method, but will modify later for multi-channel inputs\n",
    "    \n",
    "    def __init__(self, num_slices, size_of_slice):\n",
    "        super(SignalEmbedder, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.size_of_slice = size_of_slice\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[: self.num_slices * self.size_of_slice]\n",
    "        x = x.reshape((self.num_slices, self.size_of_slice))\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 1 - Huggingface GPT2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with child class of GPT2LMHeadModel\n",
    "\n",
    "class GPT2LMHeadModel(GPT2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"attn.masked_bias\", r\"attn.bias\", r\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        #self.transformer.forward = forward2.__get__(self.transformer, GPT2Model)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "    def parallelize(self, device_map=None):\n",
    "        self.device_map = (\n",
    "            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n",
    "            if device_map is None\n",
    "            else device_map\n",
    "        )\n",
    "        assert_device_map(self.device_map, len(self.transformer.h))\n",
    "        self.transformer.parallelize(self.device_map)\n",
    "        self.lm_head = self.lm_head.to(self.transformer.first_device)\n",
    "        self.model_parallel = True\n",
    "        \n",
    "    def deparallelize(self):\n",
    "        self.transformer.deparallelize()\n",
    "        self.transformer = self.transformer.to(\"cpu\")\n",
    "        self.lm_head = self.lm_head.to(\"cpu\")\n",
    "        self.model_parallel = False\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            position_ids = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"encoder_hidden_states\": kwargs.get(\"encoder_hidden_states\", None), # The one line changed hehe\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.transformer.first_device)\n",
    "            hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reorder_cache(past: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the :obj:`past_key_values` cache if\n",
    "        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 31369,   385,  ..., 50256, 50256, 50256],\n",
       "        [50256, 11265,  7813,  ..., 50256, 50256, 50256],\n",
       "        [50256, 11265,  7813,  ..., 50256, 50256, 50256],\n",
       "        ...,\n",
       "        [50256, 11265,  7813,  ..., 50256, 50256, 50256],\n",
       "        [50256,  9509,  4565,  ..., 50256, 50256, 50256],\n",
       "        [50256, 31369,   385,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# preprocess training labels and tokenize\n",
    "train_labels = list(waveform_lead_rhythm_diag['diagnosis'])\n",
    "inputs = tokenizer(train_labels, padding = True, verbose = False, return_tensors=\"pt\")\n",
    "\n",
    "#Necessary to add for generating first word\n",
    "inputs[\"input_ids\"] = torch.cat((torch.tensor([[50256] for i in range(len(inputs[\"input_ids\"]))]), inputs[\"input_ids\"]), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((torch.tensor([[1] for i in range(len(inputs[\"attention_mask\"]))]), inputs[\"attention_mask\"]), dim=1)\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder - FNET Encoder Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder decoder model with GPT2 \n",
    "class CustEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder, tokenizer):\n",
    "        super(CustEncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pos_enb = PositionalEncoder(embed_dim = 256)\n",
    "        self.embedder = embedder\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ecgs, labels = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(**labels, labels = labels[\"input_ids\"], encoder_hidden_states = x.contiguous())\n",
    "        return out\n",
    "    \n",
    "    # Should only take 1 input at a time\n",
    "    def predict_single(self, x):\n",
    "        ecgs = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        print(x.shape)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        print(x.shape)\n",
    "        x = self.encoder(x)\n",
    "        print(x.shape)\n",
    "        return self.tokenizer.decode(self.decoder.generate(encoder_hidden_states = x.contiguous())[0])\n",
    "\n",
    "    \n",
    "    # Takes in multiple inputs\n",
    "    def predict_batch(self, x):\n",
    "        ecgs = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        output = []\n",
    "        for ecg in x:\n",
    "            output.append(self.tokenizer.decode(self.decoder.generate(encoder_hidden_states = ecg.unsqueeze(0).contiguous())[0]))\n",
    "        return output\n",
    "    \n",
    "    def return_enc(self):\n",
    "        return self.encoder\n",
    "\n",
    "    \n",
    "# Connect an embedder and de-embedder for training (we will then isolate the Encoder portion of this autoencoder as our embedder)\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def make_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def make_decoder(self):\n",
    "        return self.decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is for the pre-embedder\n",
    "    \n",
    "# Make embedder\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(2):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 768, kernel_size = KER_SIZE, padding = PADDING))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(768))\n",
    "\n",
    "\n",
    "# Make de-embedder\n",
    "deconv_model = nn.Sequential()\n",
    "init_channels = 768\n",
    "for i in range(2):\n",
    "    next_channels = init_channels // 2\n",
    "    deconv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    deconv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    deconv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    deconv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    deconv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "deconv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = KER_SIZE, padding = PADDING))\n",
    "deconv_model.add_module('act_fin', nn.ReLU())\n",
    "deconv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "\n",
    "auto_model = ConvAutoEncoder(conv_model, deconv_model).to(device)\n",
    "auto_optimizer = torch.optim.Adam(auto_model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "# Training params\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    auto_optimizer.zero_grad()\n",
    "    outputs = auto_model(ecg_data)\n",
    "    loss = loss_function(outputs, ecg_data)\n",
    "    loss.backward(retain_graph=True)\n",
    "    auto_optimizer.step()\n",
    "    print(loss)\n",
    "        \n",
    "# Saving/loading weights\n",
    "torch.save(auto_model.state_dict(), 'model/autoencoder.pt')\n",
    "auto_model.load_state_dict(torch.load('model/autoencoder.pt'))\n",
    "conv_embedder = auto_model.make_encoder()\n",
    "torch.save(conv_embedder.state_dict(), \"model/embedder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define encoder, we don't need to pretrain rn\n",
    "encoder = FNETEncoder(768, expansion = 2, dropout=0.1, num_layers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 256,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 99\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-a0b293e15971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define and pretrain Decoder\n",
    "config = GPT2Config(vocab_size = 99, n_embd = 256, n_head = 16, add_cross_attention = True, is_encoder_decoder = False, bos_token_id=0, eos_token_id= 0)\n",
    "print(config)\n",
    "decoder = GPT2LMHeadModel(config = config)\n",
    "# pretrain decoder\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "inputs = transformer_tokenizer(full_y)\n",
    "\n",
    "\n",
    "#decoder.load_state_dict(torch.load('model/gpt2.pt'))\n",
    "# set number of epochs\n",
    "epochs = 150\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = decoder(**inputs, labels = inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "    for tag, value in decoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag+ \"_gpt2\", value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_gpt2' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_gpt2' + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "    \n",
    "    \n",
    "    \n",
    "torch.save(decoder.state_dict(), 'model/gpt2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define component models\n",
    "\n",
    "conv_embedder = conv_model #auto_model.make_encoder()\n",
    "\n",
    "encoder = FNETEncoder(256, expansion = 2, dropout=0.1, num_layers = 6)\n",
    "\n",
    "#decoder.load_state_dict(torch.load('model/gpt2.pt'))\n",
    "\n",
    "enc_dec_model = CustEncoderDecoder(encoder, decoder, conv_embedder, transformer_tokenizer)\n",
    "\n",
    "enc_dec_model.predict_single(ecg_data[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 0, 94, 92,  ...,  0,  0,  0],\n",
      "        [ 0, 32,  8,  ...,  0,  0,  0],\n",
      "        [ 0, 32,  8,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 0, 32,  8,  ...,  0,  0,  0],\n",
      "        [ 0, 81, 50,  ...,  0,  0,  0],\n",
      "        [ 0, 60, 88,  ...,  0,  0,  0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train encoder decoder model!\n",
    "optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 130\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    losses = 0\n",
    "    j = 0\n",
    "    for ecg in full_x:\n",
    "        outputs = enc_dec_model((ecg.unsqueeze(0), {\"input_ids\": inputs[\"input_ids\"][j], \"attention_mask\": inputs[\"attention_mask\"][j]}))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss\n",
    "        j += 1\n",
    "    if i == 40:\n",
    "        optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-4)\n",
    "    if i == 60:\n",
    "        optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-5)\n",
    "    if i == 80:\n",
    "        optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-6)\n",
    "    if i == 105:\n",
    "        optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-7)\n",
    "    print(losses)\n",
    "    for tag, value in decoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag+ \"_mixup2\", value.data.cpu().numpy(), i)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_mixup2' + '/grad', 0, i)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_mixup2' + '/grad', value.grad.data.cpu().numpy(), i)\n",
    " \n",
    "\n",
    "   \n",
    "torch.save(enc_dec_model.state_dict(), 'model/gpt2_enc_dec2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_model.load_state_dict(torch.load('model/gpt2_enc_dec.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/gpt2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth:  sinus bradycardia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia inferior infarct abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm with sinus arrhythmia cannot rule out inferior infarct abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  prolonged qt abnormal ecg t wave inversion now evident in anterior leads qt has lengthened\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block present abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible left atrial enlargement incomplete left bundle branch block borderline ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg incomplete left bundle branch block is no longer present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block right bundle branch block abnormal ecg abnormal ecg abnormal ecg abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia left axis deviation pulmonary disease pattern inferior infarct abnormal ecg sinus rhythm has replaced atrial fibrillation inverted t waves have replaced nonspecific t wave abnormality in lateral leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  atrial fibrillation with rapid ventricular response with premature ventricular or aberrantly conducted complexes right bundle branch block left posterior fascicular block abnormal ecg atrial fibrillation has replaced sinus rhythm qrs duration has increased\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  undetermined rhythm right bundle branch block abnormal ecg current undetermined rhythm precludes rhythm comparison, needs review qrs duration has increased qt has lengthened\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out right bundle branch block present abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  undetermined rhythm possible right ventricular hypertrophy nonspecific t wave abnormality abnormal ecg current undetermined rhythm precludes rhythm comparison, needs review is no longer present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia right bundle branch block abnormal ecg abnormal ecg premature supraventricular complexes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  normal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic atrial pacemaker indeterminate axis pulmonary disease pattern st elevation consider anterolateral injury or acute infarct st elevation consider inferior injury or acute infarct abnormal ecg electronic atrial pacemaker has replaced electronic ventricular pacemaker\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg abnormal ecg abnormal ecg abnormal ecg present sinus rhythm atrial fibrillation sinus rhythm lateral leads lateral leads\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  atrial fibrillation left axis deviation pulmonary disease pattern septal infarct abnormal ecg atrial fibrillation has replaced sinus rhythm nonspecific t wave abnormality has replaced inverted t waves in lateral leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia and fusion complexes right bundle branch block cannot rule out inferior infarct t wave abnormality, consider lateral ischemia abnormal ecg previous ecg has undetermined rhythm, needs review right bundle branch block is now present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia right bundle branch block t wave abnormality, consider lateral ischemia abnormal ecg premature supraventricular complexes are now present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  av sequential or dual chamber electronic pacemaker electronic ventricular pacemaker has replaced sinus rhythm vent. rate has decreased\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  av sequential or dual chamber electronic pacemaker\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation pulmonary disease pattern t wave inversion now evident in qrs duration abnormal ecg atrial fibrillation has increased\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out right bundle branch block premature supraventricular complexes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm cannot rule out anterior infarct abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible left atrial enlargement incomplete right bundle branch block borderline ecg incomplete right bundle branch block is now present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg t wave inversion now evident in anterior leads abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia right bundle branch block abnormal ecg abnormal ecg is no longer present\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus bradycardia\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm minimal voltage criteria for lvh, may be normal variant borderline ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg premature supraventricular complexes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus bradycardia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia cannot rule out anterior infarct abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct abnormal ecg t wave amplitude has decreased in lateral leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic ventricular pacemaker previous ecg has undetermined rhythm, needs review\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  normal ecg pulmonary disease pattern is no longer present qrs duration\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct abnormal ecg inverted t waves have replaced nonspecific t wave abnormality in inferior leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  normal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible anterior infarct abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct prolonged qt abnormal ecg t wave inversion no longer evident in lateral leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm are no longer present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation pulmonary disease pattern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct prolonged qt abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm st abnormality, possible digitalis effect abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic ventricular pacemaker vent. rate has decreased\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp, out in zip(ecg_data, inputs[\"input_ids\"]):\n",
    "    print(\"ground truth: \", transformer_tokenizer.decode(out))\n",
    "    print(\"predicted label: \", enc_dec_model.predict_single(inp.unsqueeze(0)))\n",
    "    print(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
