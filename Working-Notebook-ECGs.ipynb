{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# import all packages needed\n",
    "import string, copy, math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from base64 import b64decode as decode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import CrossEntropyLoss, MSELoss, TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2PreTrainedModel, GPT2Model\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordPiece, WordLevel\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer, WordLevelTrainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# The only time we need to define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16).astype(np.float32)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/combined_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/combined_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/combined_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/combined_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead dataz\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "# sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "\n",
    "\n",
    "# adding the diagnosis and labels\n",
    "waveform_and_diag = pd.merge(waveform_lead[['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']], diagnosis_data[[\"exam_id\", \"Full_text\", \"Original_Diag\"]], left_on= \"exam_id\", right_on=\"exam_id\")\n",
    "\n",
    "\n",
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "waveform_lead_concat = waveform_lead_concat[waveform_lead_concat[\"decoded_waveform\"].apply(lambda x: len(x[0]) == 2500)]\n",
    "waveform_lead_concat = waveform_lead_concat[waveform_lead_concat[\"decoded_waveform\"].apply(lambda x: len(x) == 8)]\n",
    "   \n",
    "\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: np.vstack(x))\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "\n",
    "waveform_lead_rhythm[\"decoded_waveform\"] = waveform_lead_rhythm[\"decoded_waveform\"].apply(lambda value: MinMaxScaler().fit_transform(value))\n",
    "\n",
    "\n",
    "\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "\n",
    "\n",
    "# making the tokens\n",
    "tokens = set()\n",
    "\n",
    "prefixed_phrase = \"\"\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    if row[\"exam_id\"] != curr_id and curr_string != \"\":\n",
    "        curr_string = curr_string.lower()\n",
    "        curr_string = curr_string.replace(\"     \", \"\").replace(\" ,\", \"\")\n",
    "        val = [curr_id, curr_string[2:]]\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    \n",
    "    if \"*\" in row[\"Full_text\"] or \"(\" in row[\"Full_text\"]:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    if row[\"Full_text\"][-3:] == \"for\" or row[\"Full_text\"][-4:] == \"with\" or row[\"Full_text\"][-1] == \"&\":\n",
    "        prefixed_phrase = row[\"Full_text\"].lower() + \" \"\n",
    "        curr_string += \"@\"\n",
    "        continue\n",
    "    \n",
    "    if curr_string and curr_string[-1] == \"@\":\n",
    "        curr_string = curr_string[:-1]\n",
    "        curr_string += \" \" + row[\"Full_text\"]\n",
    "    else:\n",
    "        curr_string += \"; \" + row[\"Full_text\"]\n",
    "    \n",
    "    tokens.add(prefixed_phrase + row[\"Full_text\"].lower())\n",
    "    prefixed_phrase = \"\"\n",
    "    \n",
    "    \n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "\n",
    "\n",
    "# Define all the data here\n",
    "\n",
    "\n",
    "tokens = list(tokens)\n",
    "full_x = torch.tensor(waveform_lead_rhythm_diag['decoded_waveform']).float().to(device)\n",
    "full_y = list(waveform_lead_rhythm_diag['diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and pretraining tokenizer for the LSTM Encoder-Decoder model\n",
    "\n",
    "class tokenizer(nn.Module):\n",
    "    def __init__(self, vocab, max_len=20):\n",
    "        super(tokenizer, self).__init__()\n",
    "        self.tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        self.trainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "        self.tokenizer.train_from_iterator(np.array(tokens), self.trainer)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # expects list of sentence fragments, within each sentence separated by ;\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        for sentence in x:\n",
    "            tokenized = self.tokenizer.encode(sentence.split(\"; \"), is_pretokenized=True)\n",
    "            input_id = [0] + tokenized.ids + [0 for i in range(self.max_len - len(tokenized.ids))]\n",
    "            attention_mask = [1] + tokenized.attention_mask + [0 for i in range(self.max_len - len(tokenized.attention_mask))]\n",
    "            input_ids.append(input_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "        return {\"input_ids\": torch.tensor(input_ids).detach(), \"attention_mask\": torch.tensor(attention_masks).detach()}\n",
    "    \n",
    "    def forward_lstm(self, x):\n",
    "        # expects list of sentence fragments, within each sentence separated by ; length is variable\n",
    "        x = copy.deepcopy(x)\n",
    "        for i, sentence in enumerate(x):\n",
    "            x[i] = sentence.split(\"; \")\n",
    "        \n",
    "        output = [self.tokenizer.encode(label, is_pretokenized=True).ids for label in x]\n",
    "        for i in output:\n",
    "            i.append(0)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def decode(self, x):\n",
    "        # expects tensor of ids for a single label\n",
    "        return self.tokenizer.decode(list(x))\n",
    "\n",
    "    \n",
    "tokenizer = tokenizer(vocab=tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[31, 51, 0],\n",
       " [27, 39, 0],\n",
       " [27, 39, 0],\n",
       " [24, 52, 51, 0],\n",
       " [27, 37, 85, 68, 1, 48, 0],\n",
       " [1, 20, 48, 86, 45, 82, 0],\n",
       " [24, 52, 51, 0],\n",
       " [27, 32, 44, 9, 63, 0],\n",
       " [27, 39, 9, 70, 80, 0],\n",
       " [95, 38, 55, 68, 1, 48, 24, 7, 91, 92, 6, 0],\n",
       " [91, 53, 89, 59, 35, 48, 91, 7, 24, 28, 14, 0],\n",
       " [3, 59, 48, 47, 28, 14, 82, 0],\n",
       " [3, 32, 60, 64, 48, 47, 70, 80, 0],\n",
       " [27, 39, 0],\n",
       " [57, 19, 55, 5, 90, 48, 57, 7, 41, 0],\n",
       " [27, 39, 0],\n",
       " [91, 38, 55, 15, 1, 48, 91, 7, 24, 34, 6, 0],\n",
       " [95, 1, 97, 46, 59, 85, 68, 1, 73, 48, 36, 59, 50, 80, 0],\n",
       " [95, 1, 59, 73, 48, 87, 42, 80, 0],\n",
       " [40, 41, 7, 24, 18, 43, 0],\n",
       " [40, 0],\n",
       " [27, 39, 0],\n",
       " [27, 39, 0],\n",
       " [27, 39, 0],\n",
       " [27, 85, 30, 1, 48, 0],\n",
       " [27, 39, 0],\n",
       " [27, 32, 44, 23, 63, 23, 50, 80, 0],\n",
       " [27, 39, 0],\n",
       " [27, 39, 0],\n",
       " [31, 0],\n",
       " [24, 52, 51, 0],\n",
       " [27, 2, 63, 0],\n",
       " [27, 39, 0],\n",
       " [31, 51, 0],\n",
       " [95, 85, 30, 1, 48, 0],\n",
       " [27, 39, 0],\n",
       " [27, 39, 0],\n",
       " [27, 39, 0],\n",
       " [27, 68, 1, 48, 13, 6, 0],\n",
       " [41, 36, 0],\n",
       " [27, 68, 1, 48, 92, 81, 0],\n",
       " [27, 32, 30, 48, 0],\n",
       " [27, 39, 0],\n",
       " [27, 39, 0],\n",
       " [27, 68, 20, 48, 75, 6, 0],\n",
       " [27, 1, 21, 80, 0],\n",
       " [27, 39, 0],\n",
       " [27, 68, 20, 48, 0],\n",
       " [27, 49, 48, 0],\n",
       " [27, 39, 0],\n",
       " [41, 18, 43, 0],\n",
       " [24, 52, 51, 0]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making labels for the LSTM model\n",
    "token_y_lstm = tokenizer.forward_lstm(full_y)\n",
    "\n",
    "token_y_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 31, 51,  ...,  0,  0,  0],\n",
       "         [ 0, 27, 39,  ...,  0,  0,  0],\n",
       "         [ 0, 27, 39,  ...,  0,  0,  0],\n",
       "         ...,\n",
       "         [ 0, 27, 39,  ...,  0,  0,  0],\n",
       "         [ 0, 41, 18,  ...,  0,  0,  0],\n",
       "         [ 0, 24, 52,  ...,  0,  0,  0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making labels for the transformer model\n",
    "token_y_transformer = tokenizer(full_y)\n",
    "token_y_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools to help with the creation of an Embedder\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = stride)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "def init_weights(x):\n",
    "    if isinstance(x, nn.Conv1d):\n",
    "        nn.init.kaiming_uniform_(x.weight, mode='fan_in', nonlinearity='relu')\n",
    "        x.bias.data.fill_(0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect an embedder and de-embedder for training (we will then isolate the Encoder portion of this autoencoder as our embedder)\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def make_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def make_decoder(self):\n",
    "        return self.decoder    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder 1: Conv (AutoEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 8, 2500])\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "STRIDE = 1\n",
    "LR = 1e-3\n",
    "KER_SIZE = 249\n",
    "PADDING = 124\n",
    "EMBED_DIM = 256\n",
    "NUM_LEADS = 8\n",
    "\n",
    "\n",
    "# build resnet model and display the shape of feed through\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = NUM_LEADS\n",
    "for i in range(NUM_LAYERS):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = STRIDE))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ELU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ELU())\n",
    "    init_channels = next_channels\n",
    "    \n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = EMBED_DIM, kernel_size = KER_SIZE, padding = PADDING))\n",
    "conv_model.add_module('act_fin', nn.ELU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(EMBED_DIM))\n",
    "conv_model.apply(init_weights)\n",
    "\n",
    "\n",
    "# Make de-embedder\n",
    "deconv_model = nn.Sequential()\n",
    "init_channels = EMBED_DIM\n",
    "for i in range(NUM_LAYERS):\n",
    "    next_channels = init_channels // 2\n",
    "    deconv_model.add_module('conv_{num}'.format(num = i), nn.ConvTranspose1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = STRIDE))\n",
    "    deconv_model.add_module('act_{num}'.format(num = i), nn.ELU())\n",
    "    deconv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    deconv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    deconv_model.add_module('act_res_{num}'.format(num = i), nn.ELU())\n",
    "    init_channels = next_channels\n",
    "deconv_model.add_module('conv_fin', nn.ConvTranspose1d(in_channels = init_channels, out_channels = NUM_LEADS, kernel_size = KER_SIZE, padding = PADDING))\n",
    "deconv_model.add_module('act_fin', nn.ELU())\n",
    "deconv_model.add_module('batch_fin', nn.BatchNorm1d(NUM_LEADS))\n",
    "\n",
    "\n",
    "print(deconv_model(conv_model(full_x)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Optional AutoEncoder Pretraining of the Embedder\n",
    "\n",
    "# Training Params\n",
    "auto_model = ConvAutoEncoder(conv_model, deconv_model).to(device)\n",
    "auto_optimizer = torch.optim.Adam(auto_model.parameters(), lr = LR)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Set this to a non-zero number for pretraining\n",
    "epochs = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    auto_optimizer.zero_grad()\n",
    "    outputs = auto_model(full_x)\n",
    "    loss = loss_function(outputs, full_x)\n",
    "    loss.backward(retain_graph=True)\n",
    "    auto_optimizer.step()\n",
    "    print(loss)\n",
    "        \n",
    "# Saving/loading weights\n",
    "torch.save(auto_model.state_dict(), 'model/autoencoder.pt')\n",
    "auto_model.load_state_dict(torch.load('model/autoencoder.pt'))\n",
    "\n",
    "# Embedder for our other models\n",
    "conv_embedder = auto_model.make_encoder()\n",
    "\n",
    "# If powerful enough, then can freeze weights\n",
    "if epochs > 100:\n",
    "    for param in conv_embedder.params():\n",
    "        param.requires_grad = False\n",
    "\n",
    "torch.save(conv_embedder.state_dict(), \"model/embedder.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 256\n",
    "embedding_dim = EMBED_DIM\n",
    "\n",
    "word_list_length = len(tokenizer.tokenizer.get_vocab())\n",
    "start_token = end_token = 0\n",
    "seq_length = conv_embedder(full_x[0:1]).size(2) # depends on embedding shrinkage factor\n",
    "\n",
    "class LSTM_Encoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim):\n",
    "        super(LSTM_Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim, num_layers = 4, bidirectional = True)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.01)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param, gain=nn.init.calculate_gain('tanh'))\n",
    "\n",
    "        \n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        if hidden is None and cell_state is None:\n",
    "            final, comp = self.lstm(x)\n",
    "        else:\n",
    "            final, comp = self.lstm(x, (hidden, cell_state))\n",
    "        hid, cell = comp\n",
    "        return final, hid, cell\n",
    "    \n",
    "    def initial_hidden_cell(self):\n",
    "        return torch.zeros(8, 1, 256), torch.zeros(8, 1, 256)\n",
    "\n",
    "    \n",
    "\n",
    "class LSTM_Decoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim, word_list_length, max_length = seq_length):\n",
    "        super(LSTM_Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(word_list_length, e_dim)\n",
    "        \n",
    "        self.attention = nn.Linear(h_dim*2, max_length)\n",
    "        torch.nn.init.xavier_uniform_(self.attention.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        self.attention.bias.data.fill_(0.01)\n",
    "        \n",
    "        self.attention_combined = nn.Linear(h_dim * 3, h_dim)\n",
    "        torch.nn.init.xavier_uniform_(self.attention_combined.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        self.attention_combined.bias.data.fill_(0.01)\n",
    "        \n",
    "        self.lstm = nn.LSTM(e_dim, h_dim)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.01)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param, gain=nn.init.calculate_gain('tanh'))\n",
    "\n",
    "        self.out = nn.Linear(h_dim, word_list_length)\n",
    "        torch.nn.init.xavier_uniform_(self.out.weight, gain=nn.init.calculate_gain('linear'))\n",
    "        self.out.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, hidden, cell_state, encoder_outputs):\n",
    "        seq_embedded = self.emb(x).view(1, 1, -1)\n",
    "        \n",
    "        attention_weights = F.softmax(self.attention(torch.cat((seq_embedded[0], hidden[0]), 1)), 1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = F.relu(self.attention_combined(torch.cat((seq_embedded[0], attention_applied[0]), 1)))\n",
    "        \n",
    "        final, states  = self.lstm(output.unsqueeze(0), (hidden, cell_state))\n",
    "        hidden, cell_state = states\n",
    "        dec_seq = self.out(final)\n",
    "        return F.log_softmax(dec_seq[0], dim = -1), hidden, cell_state\n",
    "    \n",
    "    def initial_hidden_cell(self):\n",
    "        return torch.zeros(1, 1, 256), torch.zeros(1, 1, 256)\n",
    "\n",
    "\n",
    "    \n",
    "def train(x, y, embedder, encoder, decoder, emb_optimizer, enc_optimizer, dec_optimizer, teacher_ratio = 0.5):\n",
    "    hidden_enc, cell_enc = encoder.initial_hidden_cell()\n",
    "    enc_outputs = torch.zeros(seq_length, hidden_layers * 2)\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    loss = 0\n",
    "    \n",
    "    emb_optimizer.zero_grad()\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    \n",
    "    emb_x = embedder(x.unsqueeze(0)).permute(2, 0, 1)\n",
    "    \n",
    "    for i in range(len(emb_x)):\n",
    "        seq = emb_x[i].unsqueeze(0)\n",
    "        enc_out, hidden_enc, cell_enc = encoder(seq, hidden_enc, cell_enc)\n",
    "        enc_outputs[i] = enc_out[0, 0]\n",
    "    \n",
    "    hidden_dec, cell_dec = decoder.initial_hidden_cell()\n",
    "    target_lab = torch.tensor(y, dtype = torch.long)\n",
    "    decoder_input = torch.tensor([[start_token]], dtype = torch.long)\n",
    "    teacher_forcing = True if torch.rand(1) <= teacher_ratio else False\n",
    "    fin_len = 0\n",
    "    if teacher_forcing:\n",
    "        for j in range(len(target_lab)):\n",
    "            logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "            current_targ = target_lab[j].unsqueeze(0)\n",
    "            loss = loss_fn(logit, current_targ) + loss\n",
    "            decoder_input = current_targ\n",
    "            fin_len = j + 1\n",
    "            if decoder_input == end_token:\n",
    "                break\n",
    "    else:\n",
    "        for j in range(len(target_lab)):\n",
    "            logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "            _, val = logit.topk(1)\n",
    "            current_targ = target_lab[j].unsqueeze(0)\n",
    "            loss = loss_fn(logit, current_targ) + loss\n",
    "            decoder_input = val.squeeze(0).detach()\n",
    "            fin_len = j + 1\n",
    "            if decoder_input == end_token:\n",
    "                print(\"trained without teacher enforcing\")\n",
    "                break    \n",
    "    \n",
    "    loss.backward()\n",
    "    emb_optimizer.step()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/danielbang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "lstm_embedder = copy.deepcopy(conv_embedder)    \n",
    "lstm_encoder = LSTM_Encoder(hidden_layers, embedding_dim)\n",
    "lstm_decoder = LSTM_Decoder(hidden_layers, embedding_dim, word_list_length)\n",
    "\n",
    "emb_optimizer = torch.optim.Adam(lstm_embedder.parameters(), lr = LR)\n",
    "enc_optimizer = torch.optim.Adam(lstm_encoder.parameters(), lr = LR)\n",
    "dec_optimizer = torch.optim.Adam(lstm_decoder.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_embedder.load_state_dict(torch.load('model/lstm_embedder.pt'))\n",
    "lstm_encoder.load_state_dict(torch.load('model/lstm_encoder.pt'))\n",
    "lstm_decoder.load_state_dict(torch.load(\"model/lstm_decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/lstm_encoder_decoder')\n",
    "ratio = 1\n",
    "\n",
    "for epoch in range(1):\n",
    "    losses = 0.0\n",
    "    for j, k in zip(full_x, token_y_lstm):\n",
    "        loss = train(j, k, lstm_embedder, lstm_encoder, lstm_decoder, emb_optimizer, enc_optimizer, dec_optimizer, teacher_ratio = ratio)\n",
    "        losses += loss\n",
    "\n",
    "    print(losses)\n",
    "    \n",
    "    if epoch == 20:\n",
    "        emb_optimizer = torch.optim.Adam(conv_model.parameters(), lr = 1e-4)\n",
    "        enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = 1e-4)\n",
    "        dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-4)\n",
    "        ratio = .95\n",
    "    if epoch == 60:\n",
    "        emb_optimizer = torch.optim.Adam(conv_model.parameters(), lr = 1e-5)\n",
    "        enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = 1e-5)\n",
    "        dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-5)\n",
    "        ratio = .9\n",
    "    if epoch == 80:\n",
    "        emb_optimizer = torch.optim.Adam(conv_model.parameters(), lr = 1e-6)\n",
    "        enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = 1e-6)\n",
    "        dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-6)\n",
    "        ratio = .8\n",
    "    if epoch == 130:\n",
    "        emb_optimizer = torch.optim.Adam(conv_model.parameters(), lr = 1e-7)\n",
    "        enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = 1e-7)\n",
    "        dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-7)\n",
    "        ratio = .6\n",
    "              \n",
    "    for tag, value in conv_model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_emb' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_emb' + '/grad', value.grad.data.cpu().numpy(), epoch)           \n",
    "    \n",
    "    for tag, value in encoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_enc' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_enc' + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "    \n",
    "    \n",
    "    for tag, value in decoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+'_dec' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+'_dec' + '/grad', value.grad.data.cpu().numpy(), epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_embedder.state_dict(), 'model/lstm_embedder.pt')\n",
    "torch.save(lstm_encoder.state_dict(), 'model/lstm_encoder.pt')\n",
    "torch.save(lstm_decoder.state_dict(), 'model/lstm_decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth:  sinus bradycardia otherwise normal ecg [PAD]\n",
      "predicted:  with sinus arrhythmia\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg [PAD]\n",
      "predicted:  nonspecific t wave abnormality, improved in\n",
      "predicted:  cannot rule out\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm with sinus arrhythmia cannot rule out inferior infarct [UNK] abnormal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  septal infarct\n",
      "predicted:  t wave inversion now evident in\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  [UNK] prolonged qt abnormal ecg t wave inversion now evident in anterior leads qt has lengthened [PAD]\n",
      "predicted:  [UNK]\n",
      "predicted:  has replaced\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  fusion complexes\n",
      "predicted:  present\n",
      "predicted:  nonspecific t wave abnormality has replaced inverted t waves in\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg [PAD]\n",
      "predicted:  nonspecific t wave abnormality, improved in\n",
      "predicted:  cannot rule out\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible left atrial enlargement incomplete left bundle branch block borderline ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  septal infarct\n",
      "predicted:  nonspecific st and t wave abnormality\n",
      "predicted:  lateral leads\n",
      "predicted:  nonspecific t wave abnormality no longer evident in\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg incomplete left bundle branch block is no longer present [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia left axis deviation pulmonary disease pattern inferior infarct [UNK] abnormal ecg sinus rhythm has replaced atrial fibrillation inverted t waves have replaced nonspecific t wave abnormality in lateral leads [PAD]\n",
      "predicted:  with premature supraventricular complexes\n",
      "predicted:  st elevation consider inferior injury or acute infarct\n",
      "predicted:  vent. rate\n",
      "predicted:  right ventricular hypertrophy\n",
      "predicted:  [UNK]\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  nonspecific t wave abnormality, improved in\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  normal sinus rhythm\n",
      "predicted:  supraventricular tachycardia\n",
      "predicted:  inferior leads\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  atrial fibrillation with rapid ventricular response with premature ventricular or aberrantly conducted complexes right bundle branch block left posterior fascicular block abnormal ecg atrial fibrillation has replaced sinus rhythm qrs duration has increased [PAD]\n",
      "predicted:  nonspecific t wave abnormality, improved in\n",
      "predicted:  cannot rule out\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  undetermined rhythm right bundle branch block abnormal ecg current undetermined rhythm precludes rhythm comparison, needs review qrs duration has increased qt has lengthened [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  undetermined rhythm possible right ventricular hypertrophy nonspecific t wave abnormality abnormal ecg current undetermined rhythm precludes rhythm comparison, needs review is no longer present [PAD]\n",
      "predicted:  t wave amplitude has increased in\n",
      "predicted:  st now depressed in\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  nonspecific t wave abnormality, improved in\n",
      "predicted:  with premature ventricular or aberrantly conducted complexes\n",
      "predicted:  with rapid ventricular response\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic atrial pacemaker indeterminate axis pulmonary disease pattern st elevation consider anterolateral injury or acute infarct st elevation consider inferior injury or acute infarct abnormal ecg electronic atrial pacemaker has replaced electronic ventricular pacemaker [PAD]\n",
      "predicted:  nonspecific t wave abnormality now evident in\n",
      "predicted:  atrial fibrillation\n",
      "predicted:  vent. rate\n",
      "predicted:  , age undetermined\n",
      "predicted:  aberrant conduction\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  nonspecific t wave abnormality now evident in\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  st now depressed in\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  atrial fibrillation left axis deviation pulmonary disease pattern septal infarct [UNK] abnormal ecg atrial fibrillation has replaced sinus rhythm nonspecific t wave abnormality has replaced inverted t waves in lateral leads [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia [UNK] and fusion complexes right bundle branch block cannot rule out inferior infarct [UNK] t wave abnormality, consider lateral ischemia abnormal ecg previous ecg has undetermined rhythm, needs review right bundle branch block is now present [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  septal infarct\n",
      "predicted:  t wave inversion now evident in\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia [UNK] right bundle branch block t wave abnormality, consider lateral ischemia abnormal ecg premature supraventricular complexes are now present [PAD]\n",
      "predicted:  with premature supraventricular complexes\n",
      "predicted:  [UNK]\n",
      "predicted:  with premature atrial complexes\n",
      "predicted:  sinus tachycardia\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  st elevation, consider early repolarization\n",
      "predicted:  current undetermined rhythm precludes rhythm comparison, needs review\n",
      "predicted:  rsr' or qr pattern in v1 suggests right ventricular conduction delay\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  av sequential or dual chamber electronic pacemaker electronic ventricular pacemaker has replaced sinus rhythm vent. rate has decreased [PAD]\n",
      "predicted:  t wave amplitude has increased in\n",
      "predicted:  st now depressed in\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  nonspecific t wave abnormality, improved in\n",
      "predicted:  with premature ventricular or aberrantly conducted complexes\n",
      "predicted:  with rapid ventricular response\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  av sequential or dual chamber electronic pacemaker [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  right ventricular hypertrophy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm cannot rule out anterior infarct [UNK] abnormal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  right ventricular hypertrophy\n",
      "predicted:  has replaced\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible left atrial enlargement incomplete right bundle branch block borderline ecg incomplete right bundle branch block is now present [PAD]\n",
      "predicted:  with sinus arrhythmia\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus bradycardia [PAD]\n",
      "predicted:  with sinus arrhythmia\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg [PAD]\n",
      "predicted:  nonspecific t wave abnormality, improved in\n",
      "predicted:  cannot rule out\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm minimal voltage criteria for lvh, may be normal variant borderline ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus bradycardia otherwise normal ecg [PAD]\n",
      "predicted:  with sinus arrhythmia\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia cannot rule out anterior infarct [UNK] abnormal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  right ventricular hypertrophy\n",
      "predicted:  [UNK]\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  st elevation consider anterolateral injury or acute infarct\n",
      "predicted:  inferior leads\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct [UNK] abnormal ecg t wave amplitude has decreased in lateral leads [PAD]\n",
      "predicted:  with premature supraventricular complexes\n",
      "predicted:  [UNK]\n",
      "predicted:  with premature atrial complexes\n",
      "predicted:  sinus tachycardia\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  st elevation, consider early repolarization\n",
      "predicted:  current undetermined rhythm precludes rhythm comparison, needs review\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic ventricular pacemaker previous ecg has undetermined rhythm, needs review [PAD]\n",
      "predicted:  with sinus arrhythmia\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct [UNK] abnormal ecg inverted t waves have replaced nonspecific t wave abnormality in inferior leads [PAD]\n",
      "predicted:  with premature supraventricular complexes\n",
      "predicted:  [UNK]\n",
      "predicted:  with premature atrial complexes\n",
      "predicted:  sinus tachycardia\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  st elevation, consider early repolarization\n",
      "predicted:  current undetermined rhythm precludes rhythm comparison, needs review\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible anterior infarct abnormal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  septal infarct\n",
      "predicted:  t wave inversion now evident in\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct prolonged qt abnormal ecg t wave inversion no longer evident in lateral leads [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  right ventricular hypertrophy\n",
      "predicted:  has replaced\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  av sequential or dual chamber electronic pacemaker\n",
      "predicted:  inferior leads\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm [UNK] are no longer present [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct prolonged qt abnormal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  right ventricular hypertrophy\n",
      "predicted:  has replaced\n",
      "predicted:  minimal voltage criteria for lvh, may be normal variant\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm st abnormality, possible digitalis effect abnormal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  qt has lengthened\n",
      "predicted:  abnormal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic ventricular pacemaker vent. rate has decreased [PAD]\n",
      "predicted:  st now depressed in\n",
      "predicted:  with premature ventricular or aberrantly conducted complexes\n",
      "predicted:  with rapid ventricular response\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg [PAD]\n",
      "predicted:  nonspecific t wave abnormality, improved in\n",
      "predicted:  cannot rule out\n",
      "predicted:   \n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the predictions\n",
    "\n",
    "for x, y in zip(full_x, token_y_lstm):\n",
    "    #print(x)\n",
    "    print(\"ground truth: \", \" \".join([tokenizer.id_to_token(i) for i in y]))\n",
    "    emb_x = conv_model(x.unsqueeze(0)).permute(2, 0, 1)\n",
    "    hidden_enc, cell_enc = encoder.initial_hidden_cell()\n",
    "    enc_outputs = torch.zeros(seq_length, hidden_layers * 2)\n",
    "    \n",
    "    for i in range(len(emb_x)):\n",
    "        seq = emb_x[i].unsqueeze(0)\n",
    "        enc_out, hidden_enc, cell_enc = encoder(seq, hidden_enc, cell_enc)\n",
    "        enc_outputs[i] = enc_out[0, 0]       \n",
    "    \n",
    "    hidden_dec, cell_dec = decoder.initial_hidden_cell()\n",
    "    decoder_input = torch.tensor([[start_token]], dtype = torch.long)\n",
    "    for i in range(len(y)):\n",
    "        logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "        _, val = logit.topk(1)\n",
    "        decoder_input = val.squeeze(0).detach()\n",
    "        print(\"predicted: \", tokenizer.id_to_token(decoder_input))\n",
    "        if decoder_input == end_token:\n",
    "            break\n",
    "    print(\"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 3 - Multi-Head Attention Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress, will clean later\n",
    "\n",
    "\n",
    "class ECGTransformerEncoder(nn.Module):\n",
    "    # Takes the ECG discrete signals sequence and maps into a probability distribution of diagnosis\n",
    "    # For working/verification purposes\n",
    "    def __init__(self, vector_size, embed_dim, n_heads, hidden_linear_dim, n_layers, dropout):\n",
    "        super(ECGTransformerEncoder, self).__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.positional_encoder = PositionalEncoder(embed_dim, dropout)\n",
    "    \n",
    "        #Since our data is already discrete numbers, might need some tweaking for this\n",
    "        self.embedder = conv_embedder\n",
    "                        #64 31              #39        64\n",
    "        \n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(embed_dim, n_heads, hidden_linear_dim, dropout),\n",
    "            n_layers)\n",
    "        \n",
    "        self.n_inputs = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Simple linear decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(768, 17),\n",
    "                        Transpose(17, 2500),\n",
    "                        nn.Linear(2500, 30),\n",
    "                        nn.LogSoftmax()\n",
    "                        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #self.embedder.weight.data.uniform_(-.1, .1)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        #self.decoder.weight.data.uniform_(-.1, .1)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.embedder(x) # * math.sqrt(self.n_inputs)\n",
    "        x = x.squeeze(0)\n",
    "        #x = x.view(2500, 8)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1) \n",
    "        #x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(features, features * expansion)\n",
    "        self.linear_2 = nn.Linear(features * expansion, features)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        #self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.norm_1(x + res)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FNETLayer(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FNETLayer, self).__init__()\n",
    "        self.feed_forward = FeedForwardNet(features, expansion, dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "        \n",
    "        # additions to add\n",
    "        self.attention_layer = nn.TransformerEncoderLayer(256, 16, 512, 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = fft.fftn(x, dim = (-2, -1)).real\n",
    "        x = self.norm_1(x + res)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.attention_layer(x)\n",
    "        return x\n",
    "    \n",
    "class FNETEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self, features, expansion=2, dropout=0.5, num_layers=6):\n",
    "        encoder_layer = FNETLayer(features, expansion, dropout)\n",
    "        super().__init__(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Encoder Helper Functions/Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    # Necessary to store positional data about the input data\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=2500):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, 1, embed_dim)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        divisor = torch.exp(torch.arange(0, embed_dim, 2).float() * (- math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pos_encoding[:, 0, 0::2] = torch.sin(position * divisor)\n",
    "        pos_encoding[:, 0, 1::2] = torch.cos(position * divisor)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoding.repeat(1, x.shape[1], 1)\n",
    "        x = x + pos_encoding[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "   \n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the number of the last batch sample in the data set is smaller than the defined batch_batch size, mismatch problems will occur. You can modify it yourself, for example, just pass in the shape behind, and then enter it through x.szie(0).\n",
    "        return x.view(self.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class WindowEmbedder(nn.Module):\n",
    "    # Necessary to convert the signal into \"word\" vectors for transformer processing.\n",
    "    # Currently a simple group and slice method, but will modify later for multi-channel inputs\n",
    "    \n",
    "    def __init__(self, num_slices, size_of_slice):\n",
    "        super(SignalEmbedder, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.size_of_slice = size_of_slice\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[: self.num_slices * self.size_of_slice]\n",
    "        x = x.reshape((self.num_slices, self.size_of_slice))\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 1 - Huggingface GPT2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with child class of GPT2LMHeadModel\n",
    "\n",
    "class GPT2LMHeadModel(GPT2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"attn.masked_bias\", r\"attn.bias\", r\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        #self.transformer.forward = forward2.__get__(self.transformer, GPT2Model)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "    def parallelize(self, device_map=None):\n",
    "        self.device_map = (\n",
    "            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n",
    "            if device_map is None\n",
    "            else device_map\n",
    "        )\n",
    "        assert_device_map(self.device_map, len(self.transformer.h))\n",
    "        self.transformer.parallelize(self.device_map)\n",
    "        self.lm_head = self.lm_head.to(self.transformer.first_device)\n",
    "        self.model_parallel = True\n",
    "        \n",
    "    def deparallelize(self):\n",
    "        self.transformer.deparallelize()\n",
    "        self.transformer = self.transformer.to(\"cpu\")\n",
    "        self.lm_head = self.lm_head.to(\"cpu\")\n",
    "        self.model_parallel = False\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            position_ids = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"encoder_hidden_states\": kwargs.get(\"encoder_hidden_states\", None), # The one line changed hehe\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.transformer.first_device)\n",
    "            hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reorder_cache(past: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the :obj:`past_key_values` cache if\n",
    "        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder - FNET Encoder Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder decoder model with GPT2 \n",
    "class CustEncoderDecoder(nn.Module):\n",
    "    def __init__(self, embedder, encoder, decoder, tokenizer):\n",
    "        super(CustEncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pos_enb = PositionalEncoder(embed_dim = EMBED_DIM)\n",
    "        self.embedder = embedder\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ecgs, labels = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(**labels, labels = labels[\"input_ids\"], encoder_hidden_states = x.contiguous())\n",
    "        return out\n",
    "    \n",
    "    # Should only take 1 input at a time\n",
    "    def predict_single(self, x):\n",
    "        ecgs = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        return self.tokenizer.decode(self.decoder.generate(encoder_hidden_states = x.contiguous())[0])\n",
    "\n",
    "    \n",
    "    # Takes in multiple inputs\n",
    "    def predict_batch(self, x):\n",
    "        ecgs = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        output = []\n",
    "        for ecg in x:\n",
    "            output.append(self.tokenizer.decode(self.decoder.generate(encoder_hidden_states = ecg.unsqueeze(0).contiguous())[0]))\n",
    "        return output\n",
    "    \n",
    "    def return_enc(self):\n",
    "        return self.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define encoder\n",
    "transformer_encoder = FNETEncoder(EMBED_DIM, expansion = 2, dropout=0.1, num_layers = 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 256,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 99\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define and pretrain Decoder\n",
    "config = GPT2Config(vocab_size = 99, n_embd = EMBED_DIM, n_head = 16, add_cross_attention = True, is_encoder_decoder = False, bos_token_id=0, eos_token_id= 0)\n",
    "print(config)\n",
    "transformer_decoder = GPT2LMHeadModel(config = config)\n",
    "\n",
    "# optional pretrain decoder\n",
    "optimizer = torch.optim.Adam(transformer_decoder.parameters(), lr = LR)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "#defining our input data\n",
    "inputs = token_y_transformer\n",
    "\n",
    "\n",
    "\n",
    "epochs = 0\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = decoder(**inputs, labels = inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "    for tag, value in decoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag+ \"_gpt2\", value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_gpt2' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_gpt2' + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "    \n",
    "    \n",
    "\n",
    "torch.save(transformer_decoder.state_dict(), 'model/gpt2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nonspecific st and t wave abnormality electronic ventricular pacemaker'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define component models\n",
    "\n",
    "transformer_embedder = copy.deepcopy(conv_embedder) #auto_model.make_encoder()\n",
    "\n",
    "enc_dec_model = CustEncoderDecoder(transformer_embedder, transformer_encoder, transformer_decoder, tokenizer)\n",
    "\n",
    "enc_dec_model.predict_single(full_x[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(43.0028, grad_fn=<AddBackward0>)\n",
      "tensor(62.0004, grad_fn=<AddBackward0>)\n",
      "tensor(48.5624, grad_fn=<AddBackward0>)\n",
      "tensor(50.6918, grad_fn=<AddBackward0>)\n",
      "tensor(44.3999, grad_fn=<AddBackward0>)\n",
      "tensor(53.2900, grad_fn=<AddBackward0>)\n",
      "tensor(42.6164, grad_fn=<AddBackward0>)\n",
      "tensor(42.9362, grad_fn=<AddBackward0>)\n",
      "tensor(41.2970, grad_fn=<AddBackward0>)\n",
      "tensor(50.7889, grad_fn=<AddBackward0>)\n",
      "tensor(42.7179, grad_fn=<AddBackward0>)\n",
      "tensor(46.8119, grad_fn=<AddBackward0>)\n",
      "tensor(41.8253, grad_fn=<AddBackward0>)\n",
      "tensor(34.8980, grad_fn=<AddBackward0>)\n",
      "tensor(34.1458, grad_fn=<AddBackward0>)\n",
      "tensor(33.3154, grad_fn=<AddBackward0>)\n",
      "tensor(33.5782, grad_fn=<AddBackward0>)\n",
      "tensor(32.4202, grad_fn=<AddBackward0>)\n",
      "tensor(32.3369, grad_fn=<AddBackward0>)\n",
      "tensor(33.7016, grad_fn=<AddBackward0>)\n",
      "tensor(36.1852, grad_fn=<AddBackward0>)\n",
      "tensor(39.3040, grad_fn=<AddBackward0>)\n",
      "tensor(36.9915, grad_fn=<AddBackward0>)\n",
      "tensor(46.6696, grad_fn=<AddBackward0>)\n",
      "tensor(43.7181, grad_fn=<AddBackward0>)\n",
      "tensor(48.0583, grad_fn=<AddBackward0>)\n",
      "tensor(40.0637, grad_fn=<AddBackward0>)\n",
      "tensor(56.5901, grad_fn=<AddBackward0>)\n",
      "tensor(41.4295, grad_fn=<AddBackward0>)\n",
      "tensor(43.4535, grad_fn=<AddBackward0>)\n",
      "tensor(42.7937, grad_fn=<AddBackward0>)\n",
      "tensor(38.7044, grad_fn=<AddBackward0>)\n",
      "tensor(34.5791, grad_fn=<AddBackward0>)\n",
      "tensor(36.3791, grad_fn=<AddBackward0>)\n",
      "tensor(42.3462, grad_fn=<AddBackward0>)\n",
      "tensor(42.4764, grad_fn=<AddBackward0>)\n",
      "tensor(53.4962, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/transformer_encoder_decoder')\n",
    "\n",
    "# train encoder decoder model!\n",
    "optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = LR)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 130\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    losses = 0\n",
    "    j = 0\n",
    "    for ecg in full_x:\n",
    "        outputs = enc_dec_model((ecg.unsqueeze(0), {\"input_ids\": inputs[\"input_ids\"][j], \"attention_mask\": inputs[\"attention_mask\"][j]}))\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss\n",
    "        j += 1\n",
    "    if i == 40:\n",
    "        optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-4)\n",
    "    if i == 60:\n",
    "        optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-5)\n",
    "    if i == 80:\n",
    "        optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-6)\n",
    "    if i == 105:\n",
    "        optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-7)\n",
    "    print(losses)\n",
    "       \n",
    "\n",
    "    for tag, value in enc_dec_model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag+ \"_mixup2\", value.data.cpu().numpy(), i)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_mixup2' + '/grad', 0, i)\n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_mixup2' + '/grad', value.grad.data.cpu().numpy(), i)\n",
    "            \n",
    "torch.save(enc_dec_model.state_dict(), 'model/gpt2_enc_dec2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_model.load_state_dict(torch.load('model/gpt2_enc_dec.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth:  sinus bradycardia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia inferior infarct abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm with sinus arrhythmia cannot rule out inferior infarct abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  prolonged qt abnormal ecg t wave inversion now evident in anterior leads qt has lengthened\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block present abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible left atrial enlargement incomplete left bundle branch block borderline ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg incomplete left bundle branch block is no longer present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block right bundle branch block abnormal ecg abnormal ecg abnormal ecg abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia left axis deviation pulmonary disease pattern inferior infarct abnormal ecg sinus rhythm has replaced atrial fibrillation inverted t waves have replaced nonspecific t wave abnormality in lateral leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  atrial fibrillation with rapid ventricular response with premature ventricular or aberrantly conducted complexes right bundle branch block left posterior fascicular block abnormal ecg atrial fibrillation has replaced sinus rhythm qrs duration has increased\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  undetermined rhythm right bundle branch block abnormal ecg current undetermined rhythm precludes rhythm comparison, needs review qrs duration has increased qt has lengthened\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out right bundle branch block present abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  undetermined rhythm possible right ventricular hypertrophy nonspecific t wave abnormality abnormal ecg current undetermined rhythm precludes rhythm comparison, needs review is no longer present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia right bundle branch block abnormal ecg abnormal ecg premature supraventricular complexes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  normal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic atrial pacemaker indeterminate axis pulmonary disease pattern st elevation consider anterolateral injury or acute infarct st elevation consider inferior injury or acute infarct abnormal ecg electronic atrial pacemaker has replaced electronic ventricular pacemaker\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg abnormal ecg abnormal ecg abnormal ecg present sinus rhythm atrial fibrillation sinus rhythm lateral leads lateral leads\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  atrial fibrillation left axis deviation pulmonary disease pattern septal infarct abnormal ecg atrial fibrillation has replaced sinus rhythm nonspecific t wave abnormality has replaced inverted t waves in lateral leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia and fusion complexes right bundle branch block cannot rule out inferior infarct t wave abnormality, consider lateral ischemia abnormal ecg previous ecg has undetermined rhythm, needs review right bundle branch block is now present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia right bundle branch block t wave abnormality, consider lateral ischemia abnormal ecg premature supraventricular complexes are now present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  av sequential or dual chamber electronic pacemaker electronic ventricular pacemaker has replaced sinus rhythm vent. rate has decreased\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  av sequential or dual chamber electronic pacemaker\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation pulmonary disease pattern t wave inversion now evident in qrs duration abnormal ecg atrial fibrillation has increased\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out right bundle branch block premature supraventricular complexes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm cannot rule out anterior infarct abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible left atrial enlargement incomplete right bundle branch block borderline ecg incomplete right bundle branch block is now present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg t wave inversion now evident in anterior leads abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  sinus tachycardia right bundle branch block abnormal ecg abnormal ecg is no longer present\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus bradycardia\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm minimal voltage criteria for lvh, may be normal variant borderline ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg premature supraventricular complexes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus bradycardia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia cannot rule out anterior infarct abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct abnormal ecg t wave amplitude has decreased in lateral leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic ventricular pacemaker previous ecg has undetermined rhythm, needs review\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  normal ecg pulmonary disease pattern is no longer present qrs duration\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct abnormal ecg inverted t waves have replaced nonspecific t wave abnormality in inferior leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  normal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible anterior infarct abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block cannot rule out right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct prolonged qt abnormal ecg t wave inversion no longer evident in lateral leads\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm are no longer present\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  left axis deviation pulmonary disease pattern\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct prolonged qt abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm st abnormality, possible digitalis effect abnormal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  cannot rule out\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic ventricular pacemaker vent. rate has decreased\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  right bundle branch block right bundle branch block abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg\n",
      "torch.Size([157, 1, 256])\n",
      "torch.Size([1, 157, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 157, 256])\n",
      "predicted label:  abnormal ecg abnormal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking predicted output of Transformer model\n",
    "\n",
    "for inp, out in zip(ecg_data, inputs[\"input_ids\"]):\n",
    "    print(\"ground truth: \", transformer_tokenizer.decode(out))\n",
    "    print(\"predicted label: \", enc_dec_model.predict_single(inp.unsqueeze(0)))\n",
    "    print(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
