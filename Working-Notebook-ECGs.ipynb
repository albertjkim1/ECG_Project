{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from base64 import b64decode as decode\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.fft as fft\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2PreTrainedModel, GPT2Model\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "from typing import Optional, Tuple\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# The only time we need to define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing / Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[548561, 'sinus bradycardia; otherwise normal ecg']\n",
      "[548592, 'normal sinus rhythm; normal ecg']\n",
      "[548593, 'normal sinus rhythm; normal ecg']\n",
      "[548609, 'normal sinus rhythm; normal ecg']\n",
      "[548759, 'normal sinus rhythm; normal sinus rhythm; low voltage qrs; low voltage qrs; borderline ecg; borderline ecg']\n",
      "[549810, 'normal sinus rhythm; left axis deviation; right bundle branch block; abnormal ecg']\n",
      "[549871, 'sinus bradycardia; sinus bradycardia; otherwise normal ecg; otherwise normal ecg']\n",
      "[549964, 'sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg']\n",
      "[550065, 'normal sinus rhythm; with sinus arrhythmia; cannot rule out; inferior infarct; , age undetermined; abnormal ecg']\n",
      "[550307, 'normal sinus rhythm; st &; t wave abnormality, consider anterior ischemia; prolonged qt; abnormal ecg; t wave inversion now evident in; anterior leads; qt has lengthened']\n",
      "[550391, 'sinus bradycardia; with; premature atrial complexes; in a pattern of bigeminy; minimal voltage criteria for lvh, may be normal variant; borderline ecg; premature atrial complexes; are now; present']\n",
      "[550416, 'sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg']\n",
      "[550432, 'normal sinus rhythm; normal ecg']\n",
      "[550602, 'sinus tachycardia; sinus tachycardia; otherwise normal ecg; otherwise normal ecg']\n",
      "[550715, 'atrial fibrillation; left axis deviation; pulmonary disease pattern; nonspecific st and t wave abnormality;  ; abnormal ecg']\n",
      "[550802, 'normal sinus rhythm; left axis deviation; abnormal ecg']\n",
      "[550912, 'atrial fibrillation; with a competing junctional pacemaker; with premature ventricular or aberrantly conducted complexes; possible; right ventricular hypertrophy; cannot rule out; anterior infarct; , age undetermined; abnormal ecg; minimal criteria for; anterior infarct; are now; present; t wave amplitude has increased in; anterolateral leads']\n",
      "[551206, 'normal sinus rhythm; possible; left atrial enlargement; incomplete left bundle branch block; borderline ecg']\n",
      "[551243, 'normal sinus rhythm; normal ecg; incomplete left bundle branch block; is no longer; present']\n",
      "[551344, 'sinus tachycardia; left axis deviation; pulmonary disease pattern; inferior infarct; , age undetermined; st &; t wave abnormality, consider lateral ischemia; abnormal ecg; sinus rhythm; has replaced; atrial fibrillation; inverted t waves have replaced nonspecific t wave abnormality in; lateral leads']\n",
      "[551407, 'sinus rhythm; with; fusion complexes; right bundle branch block; left posterior fascicular block; abnormal ecg; sinus rhythm; has replaced; electronic ventricular pacemaker']\n",
      "[551415, 'atrial fibrillation; with rapid ventricular response; with premature ventricular or aberrantly conducted complexes; right bundle branch block; left posterior fascicular block; abnormal ecg; atrial fibrillation; has replaced; sinus rhythm; qrs duration; has increased']\n",
      "[551437, 'undetermined rhythm; right bundle branch block; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; qrs duration; has increased; qt has lengthened']\n",
      "[551471, 'undetermined rhythm; possible; right ventricular hypertrophy; nonspecific t wave abnormality; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; (rbbb and left posterior fascicular block); is no longer; present']\n",
      "[551485, 'normal sinus rhythm; normal sinus rhythm; normal ecg; normal ecg']\n",
      "[551560, 'normal sinus rhythm; normal ecg']\n",
      "[551761, 'electronic atrial pacemaker; indeterminate axis; pulmonary disease pattern; st elevation consider anterolateral injury or acute infarct; st elevation consider inferior injury or acute infarct; abnormal ecg; electronic atrial pacemaker; has replaced; electronic ventricular pacemaker']\n",
      "[551831, 'sinus bradycardia; otherwise normal ecg; nonspecific t wave abnormality no longer evident in; inferior leads; nonspecific t wave abnormality no longer evident in; anterolateral leads; qt has lengthened']\n",
      "[551862, 'normal sinus rhythm; normal ecg']\n",
      "[551864, 'normal sinus rhythm; normal ecg']\n",
      "[551865, 'sinus tachycardia; nonspecific st and t wave abnormality; abnormal ecg']\n",
      "[551991, 'atrial fibrillation; left axis deviation; pulmonary disease pattern; septal infarct; , age undetermined; abnormal ecg; atrial fibrillation; has replaced; sinus rhythm; nonspecific t wave abnormality has replaced inverted t waves in; lateral leads']\n",
      "[551996, 'supraventricular tachycardia; nonspecific st and t wave abnormality; abnormal ecg; vent. rate; has increased; qrs duration; has increased; st now depressed in; anterior leads; nonspecific t wave abnormality now evident in; lateral leads']\n",
      "[552077, 'normal sinus rhythm; normal sinus rhythm; normal ecg; normal ecg']\n",
      "[552284, 'sinus tachycardia; right bundle branch block; left anterior fascicular block; abnormal ecg']\n",
      "[552648, 'electronic ventricular pacemaker']\n",
      "[552829, 'normal sinus rhythm; normal ecg']\n",
      "[552838, 'normal sinus rhythm; normal ecg']\n",
      "[552856, 'normal sinus rhythm; normal sinus rhythm; with sinus arrhythmia; with sinus arrhythmia; minimal voltage criteria for lvh, may be normal variant; minimal voltage criteria for lvh, may be normal variant; borderline ecg; borderline ecg']\n",
      "[552892, 'normal sinus rhythm; st elevation, consider early repolarization; borderline ecg']\n",
      "[552943, 'sinus rhythm; with; fusion complexes; otherwise normal ecg; fusion complexes; are now; present']\n",
      "[552983, 'normal sinus rhythm; normal sinus rhythm; t wave abnormality, consider inferior ischemia; t wave abnormality, consider inferior ischemia; abnormal ecg; abnormal ecg']\n",
      "[553008, 'sinus tachycardia; with short pr; with; premature supraventricular complexes; and; fusion complexes; right bundle branch block; cannot rule out; inferior infarct; , age undetermined; t wave abnormality, consider lateral ischemia; abnormal ecg; previous ecg has undetermined rhythm, needs review; right bundle branch block; is now; present']\n",
      "[553064, 'sinus tachycardia; with short pr; with; premature supraventricular complexes; right bundle branch block; t wave abnormality, consider lateral ischemia; abnormal ecg; premature supraventricular complexes; are now; present']\n",
      "[553089, 'av sequential or dual chamber electronic pacemaker; electronic ventricular pacemaker; has replaced; sinus rhythm; vent. rate; has decreased']\n",
      "[553115, 'atrial fibrillation; atrial fibrillation; abnormal ecg; abnormal ecg']\n",
      "[553215, 'av sequential or dual chamber electronic pacemaker']\n",
      "[553528, 'normal sinus rhythm; normal sinus rhythm; with sinus arrhythmia; with sinus arrhythmia; normal ecg; normal ecg']\n",
      "[553541, 'normal sinus rhythm; normal ecg']\n",
      "[553542, 'normal sinus rhythm; normal ecg']\n",
      "[553554, 'normal sinus rhythm; normal ecg']\n",
      "[553651, 'electronic ventricular pacemaker; vent. rate; has increased']\n",
      "[553804, 'normal sinus rhythm; cannot rule out; anterior infarct; , age undetermined; abnormal ecg']\n",
      "[553858, 'normal sinus rhythm; normal ecg']\n",
      "[554003, 'normal sinus rhythm; left axis deviation; incomplete left bundle branch block; left ventricular hypertrophy; with repolarization abnormality; abnormal ecg; t wave inversion more evident in; lateral leads']\n",
      "[554010, 'sinus rhythm; with; premature atrial complexes; left ventricular hypertrophy; with repolarization abnormality; abnormal ecg; premature atrial complexes; are now; present']\n",
      "[554080, 'sinus rhythm; sinus rhythm; with 1st degree a-v block; with 1st degree a-v block; prolonged qt; prolonged qt; abnormal ecg; abnormal ecg']\n",
      "[554100, 'normal sinus rhythm; possible; left atrial enlargement; incomplete right bundle branch block; borderline ecg; incomplete right bundle branch block; is now; present']\n",
      "[554104, 'normal sinus rhythm; inferior infarct; (cited on or before; prolonged qt; abnormal ecg; nonspecific t wave abnormality, improved in; anterior leads']\n",
      "[554202, 'normal sinus rhythm; normal ecg']\n",
      "[554215, 'electronic ventricular pacemaker']\n",
      "[554280, 'normal sinus rhythm; left anterior fascicular block; abnormal ecg']\n",
      "[554321, 'normal sinus rhythm; normal ecg']\n",
      "[554415, 'sinus bradycardia; otherwise normal ecg']\n",
      "[554429, 'sinus rhythm; with; premature atrial complexes; with; aberrant conduction; nonspecific st abnormality; abnormal ecg; aberrant conduction; is now; present']\n",
      "[554474, 'sinus bradycardia']\n",
      "[554525, 'sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg']\n",
      "[554582, 'normal sinus rhythm; minimal voltage criteria for lvh, may be normal variant; borderline ecg']\n",
      "[554633, 'normal sinus rhythm; normal ecg']\n",
      "[554672, 'sinus bradycardia; otherwise normal ecg']\n",
      "[554708, 'sinus tachycardia; cannot rule out; anterior infarct; , age undetermined; abnormal ecg']\n",
      "[554799, 'normal sinus rhythm; normal ecg']\n",
      "[555011, 'normal sinus rhythm; normal ecg']\n",
      "[555046, 'normal sinus rhythm; normal ecg']\n",
      "[555075, 'normal sinus rhythm; normal ecg']\n",
      "[555540, 'normal sinus rhythm; inferior infarct; , age undetermined; abnormal ecg; t wave amplitude has decreased in; lateral leads']\n",
      "[555635, 'undetermined rhythm; right axis deviation; right ventricular hypertrophy; nonspecific st and t wave abnormality; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; questionable change in; qrs duration']\n",
      "[555649, 'electronic ventricular pacemaker; previous ecg has undetermined rhythm, needs review']\n",
      "[555655, 'normal sinus rhythm; inferior infarct; , age undetermined; abnormal ecg; inverted t waves have replaced nonspecific t wave abnormality in; inferior leads']\n",
      "[555846, 'normal sinus rhythm; normal ecg; nonspecific t wave abnormality has replaced inverted t waves in; inferior leads']\n",
      "[555885, 'normal sinus rhythm; cannot rule out; anterior infarct; , age undetermined; abnormal ecg']\n",
      "[555907, 'normal sinus rhythm; possible; anterior infarct; (cited on or before; abnormal ecg']\n",
      "[555923, 'normal sinus rhythm; normal ecg']\n",
      "[556067, 'normal sinus rhythm; inferior infarct; (cited on or before; prolonged qt; abnormal ecg; t wave inversion now evident in; lateral leads']\n",
      "[556130, 'normal sinus rhythm; normal ecg']\n",
      "[556183, 'normal sinus rhythm; left axis deviation; abnormal ecg']\n",
      "[556190, 'normal sinus rhythm; inferior infarct; (cited on or before; prolonged qt; abnormal ecg; t wave inversion no longer evident in; lateral leads']\n",
      "[556203, 'sinus bradycardia; otherwise normal ecg']\n",
      "[556225, 'normal sinus rhythm; normal ecg; criteria for; septal infarct; are no longer; present']\n",
      "[556303, 'normal sinus rhythm; normal ecg']\n",
      "[556328, 'normal sinus rhythm; inferior infarct; (cited on or before; prolonged qt; abnormal ecg']\n",
      "[556329, 'normal sinus rhythm; st abnormality, possible digitalis effect; abnormal ecg']\n",
      "[556426, 'normal sinus rhythm; normal ecg']\n",
      "[556562, 'electronic ventricular pacemaker; vent. rate; has increased']\n",
      "[556642, 'electronic ventricular pacemaker; vent. rate; has decreased']\n",
      "[556737, \"normal sinus rhythm; with sinus arrhythmia; rsr' or qr pattern in v1 suggests right ventricular conduction delay; borderline ecg\"]\n",
      "[556808, 'sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg']\n",
      "[556851, 'normal sinus rhythm; normal ecg']\n",
      "[556934, 'normal sinus rhythm; minimal voltage criteria for lvh, may be normal variant; borderline ecg']\n",
      "sinus bradycardia; otherwise normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg\n",
      "normal sinus rhythm; with sinus arrhythmia; cannot rule out; inferior infarct; , age undetermined; abnormal ecg\n",
      "normal sinus rhythm; st &; t wave abnormality, consider anterior ischemia; prolonged qt; abnormal ecg; t wave inversion now evident in; anterior leads; qt has lengthened\n",
      "sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg\n",
      "normal sinus rhythm; possible; left atrial enlargement; incomplete left bundle branch block; borderline ecg\n",
      "normal sinus rhythm; normal ecg; incomplete left bundle branch block; is no longer; present\n",
      "sinus tachycardia; left axis deviation; pulmonary disease pattern; inferior infarct; , age undetermined; st &; t wave abnormality, consider lateral ischemia; abnormal ecg; sinus rhythm; has replaced; atrial fibrillation; inverted t waves have replaced nonspecific t wave abnormality in; lateral leads\n",
      "atrial fibrillation; with rapid ventricular response; with premature ventricular or aberrantly conducted complexes; right bundle branch block; left posterior fascicular block; abnormal ecg; atrial fibrillation; has replaced; sinus rhythm; qrs duration; has increased\n",
      "undetermined rhythm; right bundle branch block; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; qrs duration; has increased; qt has lengthened\n",
      "undetermined rhythm; possible; right ventricular hypertrophy; nonspecific t wave abnormality; abnormal ecg; current undetermined rhythm precludes rhythm comparison, needs review; (rbbb and left posterior fascicular block); is no longer; present\n",
      "normal sinus rhythm; normal ecg\n",
      "electronic atrial pacemaker; indeterminate axis; pulmonary disease pattern; st elevation consider anterolateral injury or acute infarct; st elevation consider inferior injury or acute infarct; abnormal ecg; electronic atrial pacemaker; has replaced; electronic ventricular pacemaker\n",
      "normal sinus rhythm; normal ecg\n",
      "atrial fibrillation; left axis deviation; pulmonary disease pattern; septal infarct; , age undetermined; abnormal ecg; atrial fibrillation; has replaced; sinus rhythm; nonspecific t wave abnormality has replaced inverted t waves in; lateral leads\n",
      "sinus tachycardia; with short pr; with; premature supraventricular complexes; and; fusion complexes; right bundle branch block; cannot rule out; inferior infarct; , age undetermined; t wave abnormality, consider lateral ischemia; abnormal ecg; previous ecg has undetermined rhythm, needs review; right bundle branch block; is now; present\n",
      "sinus tachycardia; with short pr; with; premature supraventricular complexes; right bundle branch block; t wave abnormality, consider lateral ischemia; abnormal ecg; premature supraventricular complexes; are now; present\n",
      "av sequential or dual chamber electronic pacemaker; electronic ventricular pacemaker; has replaced; sinus rhythm; vent. rate; has decreased\n",
      "av sequential or dual chamber electronic pacemaker\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; cannot rule out; anterior infarct; , age undetermined; abnormal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; possible; left atrial enlargement; incomplete right bundle branch block; borderline ecg; incomplete right bundle branch block; is now; present\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "sinus bradycardia\n",
      "sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg\n",
      "normal sinus rhythm; minimal voltage criteria for lvh, may be normal variant; borderline ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "sinus bradycardia; otherwise normal ecg\n",
      "sinus tachycardia; cannot rule out; anterior infarct; , age undetermined; abnormal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; inferior infarct; , age undetermined; abnormal ecg; t wave amplitude has decreased in; lateral leads\n",
      "electronic ventricular pacemaker; previous ecg has undetermined rhythm, needs review\n",
      "normal sinus rhythm; inferior infarct; , age undetermined; abnormal ecg; inverted t waves have replaced nonspecific t wave abnormality in; inferior leads\n",
      "normal sinus rhythm; possible; anterior infarct; (cited on or before; abnormal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; inferior infarct; (cited on or before; prolonged qt; abnormal ecg; t wave inversion no longer evident in; lateral leads\n",
      "normal sinus rhythm; normal ecg; criteria for; septal infarct; are no longer; present\n",
      "normal sinus rhythm; normal ecg\n",
      "normal sinus rhythm; inferior infarct; (cited on or before; prolonged qt; abnormal ecg\n",
      "normal sinus rhythm; st abnormality, possible digitalis effect; abnormal ecg\n",
      "normal sinus rhythm; normal ecg\n",
      "electronic ventricular pacemaker; vent. rate; has decreased\n",
      "sinus rhythm; with marked sinus arrhythmia; otherwise normal ecg\n"
     ]
    }
   ],
   "source": [
    "# use class base64 to decode waveform data\n",
    "def to_array(wf):\n",
    "    barr = bytearray(decode(wf))\n",
    "    vals = np.array(barr)\n",
    "    return vals.view(np.int16).astype(np.float32)\n",
    "\n",
    "# read in data\n",
    "exam_data = pd.read_csv(\"data/combined_exam.csv\").drop(columns = [\"site_num\", \"patient_id_edit\"])\n",
    "waveform_data = pd.read_csv(\"data/combined_waveform.csv\")\n",
    "lead_data = pd.read_csv(\"data/combined_lead_data.csv\").drop(columns = [\"exam_id\"])\n",
    "diagnosis_data = pd.read_csv(\"data/combined_diagnosis.csv\").drop(columns = [\"user_input\"])\n",
    "\n",
    "# add decoded data as a column to lead dataz\n",
    "waveforms = list(lead_data['waveform_data'])\n",
    "lead_data['decoded_waveform'] = [to_array(i) for i in waveforms]\n",
    "\n",
    "# merge waveform data and lead data\n",
    "waveform_lead = lead_data.merge(waveform_data, how = \"left\", left_on = \"waveform_id\", right_on = \"waveform_id\", suffixes = (None, None))\n",
    "\n",
    "#  sort by exam id and lead id\n",
    "waveform_lead.sort_values(by = [\"waveform_id\", \"lead_id\"], inplace = True)\n",
    "\n",
    "waveform_lead.loc[:, ['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']]\n",
    "\n",
    "\n",
    "# adding the diagnosis and labels\n",
    "waveform_and_diag = pd.merge(waveform_lead[['exam_id', 'lead_id', 'decoded_waveform', 'waveform_type']], diagnosis_data[[\"exam_id\", \"Full_text\", \"Original_Diag\"]], left_on= \"exam_id\", right_on=\"exam_id\")\n",
    "\n",
    "\n",
    "# concatenate all leads into a single array\n",
    "waveform_lead_concat = waveform_lead.groupby([\"exam_id\", \"waveform_type\"])['decoded_waveform'].apply(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "\n",
    "# remove irregular observations, concat tuple into numpy array\n",
    "#waveform_lead_concat = waveform_lead_concat.drop([12,17], axis = 0)\n",
    "waveform_lead_concat = waveform_lead_concat[waveform_lead_concat[\"decoded_waveform\"].apply(lambda x: len(x[0]) == 2500)]\n",
    "waveform_lead_concat = waveform_lead_concat[waveform_lead_concat[\"decoded_waveform\"].apply(lambda x: len(x) == 8)]\n",
    "   \n",
    "\n",
    "waveform_lead_concat['decoded_waveform'] = waveform_lead_concat['decoded_waveform'].apply(lambda x: np.vstack(x))\n",
    "waveform_lead_rhythm = waveform_lead_concat[waveform_lead_concat['waveform_type'] == \"Rhythm\"]\n",
    "\n",
    "waveform_lead_rhythm[\"decoded_waveform\"] = waveform_lead_rhythm[\"decoded_waveform\"].apply(lambda value: MinMaxScaler().fit_transform(value))\n",
    "\n",
    "\n",
    "\n",
    "exams = diagnosis_data[\"exam_id\"].unique()\n",
    "\n",
    "diagnosis_data = diagnosis_data[diagnosis_data['Original_Diag'] == 1].dropna()\n",
    "\n",
    "searchfor = ['previous', 'unconfirmed', 'compared', 'interpretation', 'significant']\n",
    "diagnosis_data = diagnosis_data.loc[diagnosis_data['Full_text'].str.contains('|'.join(searchfor)) != 1]\n",
    "\n",
    "diagnosis_data.sort_values(by=[\"exam_id\", \"statement_order\"], inplace=True)\n",
    "diagnoses = []\n",
    "curr_id = 0\n",
    "curr_string = \"\"\n",
    "\n",
    "\n",
    "# making the tokens\n",
    "tokens = set()\n",
    "\n",
    "\n",
    "for i, row in diagnosis_data.iterrows():\n",
    "    if curr_id == 0:\n",
    "        curr_id = row[\"exam_id\"]\n",
    "    \n",
    "    if row[\"exam_id\"] != curr_id and curr_string != \"\":\n",
    "        curr_string = curr_string.lower()\n",
    "        curr_string = curr_string.replace(\"     \", \"\")\n",
    "        val = [curr_id, curr_string[2:]]\n",
    "        print(val)\n",
    "        diagnoses.append(val)\n",
    "        curr_string = \"\"\n",
    "        curr_id = row[\"exam_id\"]\n",
    "\n",
    "    \n",
    "    if \"*\" in row[\"Full_text\"]:\n",
    "        continue\n",
    "    \n",
    "    curr_string += \"; \" + row[\"Full_text\"]\n",
    "    tokens.add(row[\"Full_text\"].lower())\n",
    "\n",
    "diagnosis_df = pd.DataFrame(diagnoses, columns = ['exam_id', 'diagnosis'])\n",
    "waveform_lead_rhythm_diag = pd.merge(left=waveform_lead_rhythm, right=diagnosis_df, left_on='exam_id', right_on='exam_id')\n",
    "\n",
    "full_x = torch.tensor(waveform_lead_rhythm_diag['decoded_waveform']).float()\n",
    "full_y = waveform_lead_rhythm_diag['diagnosis']\n",
    "for i in full_y:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ecg_data, the dataset we will be using for training purposes\n",
    "ecg_data = torch.tensor(list(waveform_lead_rhythm_diag['decoded_waveform'])).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premature supraventricular complexes': 69,\n",
       " \"rsr' or qr pattern in v1 suggests right ventricular conduction delay\": 75,\n",
       " 'has replaced': 79,\n",
       " 't wave abnormality, consider anterior ischemia': 54,\n",
       " 'incomplete left bundle branch block': 11,\n",
       " 'pulmonary disease pattern': 40,\n",
       " '[UNK]': 1,\n",
       " 'nonspecific t wave abnormality no longer evident in': 24,\n",
       " 'st now depressed in': 73,\n",
       " 't wave amplitude has increased in': 28,\n",
       " 'st elevation consider inferior injury or acute infarct': 87,\n",
       " 'left ventricular hypertrophy': 45,\n",
       " 't wave inversion no longer evident in': 8,\n",
       " 'electronic ventricular pacemaker': 30,\n",
       " '(cited on or before': 59,\n",
       " 'abnormal ecg': 61,\n",
       " 'qrs duration': 12,\n",
       " 'otherwise normal ecg': 60,\n",
       " 'in a pattern of bigeminy': 65,\n",
       " 'av sequential or dual chamber electronic pacemaker': 5,\n",
       " 'st elevation, consider early repolarization': 22,\n",
       " 'is no longer': 2,\n",
       " 'indeterminate axis': 41,\n",
       " 'fusion complexes': 86,\n",
       " 'nonspecific t wave abnormality, improved in': 23,\n",
       " 'left posterior fascicular block': 80,\n",
       " ' ': 36,\n",
       " 'atrial fibrillation': 9,\n",
       " 'anterior infarct': 47,\n",
       " 'lateral leads': 56,\n",
       " 'left axis deviation': 62,\n",
       " 'current undetermined rhythm precludes rhythm comparison, needs review': 50,\n",
       " 'borderline ecg': 17,\n",
       " 'inferior infarct': 34,\n",
       " 'with short pr': 63,\n",
       " 'with 1st degree a-v block': 67,\n",
       " 'cannot rule out': 16,\n",
       " 'normal sinus rhythm': 55,\n",
       " 'nonspecific t wave abnormality has replaced inverted t waves in': 46,\n",
       " 't wave abnormality, consider inferior ischemia': 64,\n",
       " 'minimal voltage criteria for lvh, may be normal variant': 74,\n",
       " 'with rapid ventricular response': 13,\n",
       " 'supraventricular tachycardia': 29,\n",
       " '(rbbb and left posterior fascicular block)': 6,\n",
       " 'are now': 76,\n",
       " 'previous ecg has undetermined rhythm, needs review': 77,\n",
       " 'nonspecific t wave abnormality': 19,\n",
       " 'is now': 21,\n",
       " 'prolonged qt': 31,\n",
       " 'with sinus arrhythmia': 32,\n",
       " 'with a competing junctional pacemaker': 35,\n",
       " 'vent. rate': 14,\n",
       " 'has increased': 33,\n",
       " 'electronic atrial pacemaker': 81,\n",
       " 'has decreased': 82,\n",
       " 'septal infarct': 83,\n",
       " 'st abnormality, possible digitalis effect': 84,\n",
       " 't wave amplitude has decreased in': 88,\n",
       " 'undetermined rhythm': 92,\n",
       " 'normal ecg': 94,\n",
       " 'present': 72,\n",
       " 'st elevation consider anterolateral injury or acute infarct': 58,\n",
       " 'nonspecific st abnormality': 15,\n",
       " 'incomplete right bundle branch block': 7,\n",
       " 't wave abnormality, consider lateral ischemia': 52,\n",
       " ', age undetermined': 53,\n",
       " 'questionable change in': 48,\n",
       " 'anterolateral leads': 71,\n",
       " 'nonspecific t wave abnormality now evident in': 27,\n",
       " 'sinus tachycardia': 78,\n",
       " 'qt has lengthened': 89,\n",
       " 'with repolarization abnormality': 91,\n",
       " 'sinus rhythm': 3,\n",
       " 'aberrant conduction': 51,\n",
       " '[PAD]': 0,\n",
       " 'left atrial enlargement': 57,\n",
       " 'st &': 95,\n",
       " 'anterior leads': 96,\n",
       " 'minimal criteria for': 97,\n",
       " 'inferior leads': 4,\n",
       " 'inverted t waves have replaced nonspecific t wave abnormality in': 70,\n",
       " 'right bundle branch block': 42,\n",
       " 'criteria for': 20,\n",
       " 'with premature ventricular or aberrantly conducted complexes': 68,\n",
       " 'left anterior fascicular block': 85,\n",
       " 'and': 39,\n",
       " 'sinus bradycardia': 37,\n",
       " 'right axis deviation': 44,\n",
       " 't wave inversion now evident in': 18,\n",
       " 'are no longer': 66,\n",
       " 'premature atrial complexes': 26,\n",
       " 'low voltage qrs': 43,\n",
       " 'nonspecific st and t wave abnormality': 93,\n",
       " 't wave inversion more evident in': 10,\n",
       " 'with': 49,\n",
       " 'right ventricular hypertrophy': 90,\n",
       " 'possible': 38,\n",
       " 'with marked sinus arrhythmia': 25}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(tokens)\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordPiece, WordLevel\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer, WordLevelTrainer\n",
    "\n",
    "\n",
    "# BPE, Unigram, WordPiece, WordLevel,  unk_token=\"[UNK]\"\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[PAD]\", \"[UNK]\"])\n",
    "tokenizer.train_from_iterator(np.array(tokens), trainer)\n",
    "\n",
    "\n",
    "inputs = tokenizer.encode(tokens, is_pretokenized=True)\n",
    "inputs.ids\n",
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we define components to be used for both the Conv1D encoder, and a Conv1D pre-embedder into a Transformer Encoder.\n",
    "\n",
    "LR = 1e-3\n",
    "KER_SIZE = 11\n",
    "PADDING = 5\n",
    "\n",
    "# define global max pooling\n",
    "class global_max_pooling_1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = torch.max(x, dim = 2)\n",
    "        return(x)\n",
    "\n",
    "# define resblock for neural nets\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, num_filters, kernel_size, padding, groups = 1, stride = 1):\n",
    "        super(ResBlock1D, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.conv1d_1 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.conv1d_2 = nn.Conv1d(num_filters, num_filters, kernel_size = kernel_size, padding = padding, groups = groups, stride = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(num_filters)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(num_filters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.batch_norm_1(self.act(self.conv1d_1(x)))\n",
    "        x = self.batch_norm_2(self.act(self.conv1d_2(x)))\n",
    "        return x + res\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.9.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(GPT2Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(GPT2Model.from_pretrained('gpt2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 1: ResNet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv_0): Conv1d(8, 16, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_0): ReLU()\n",
      "  (batch_norm_0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_0): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(16, 16, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_0): ReLU()\n",
      "  (conv_1): Conv1d(16, 32, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_1): ReLU()\n",
      "  (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_1): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(32, 32, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_1): ReLU()\n",
      "  (conv_2): Conv1d(32, 64, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_2): ReLU()\n",
      "  (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_2): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(64, 64, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_2): ReLU()\n",
      "  (conv_3): Conv1d(64, 128, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_3): ReLU()\n",
      "  (batch_norm_3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_3): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(128, 128, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_3): ReLU()\n",
      "  (conv_4): Conv1d(128, 256, kernel_size=(249,), stride=(2,), padding=(124,))\n",
      "  (act_4): ReLU()\n",
      "  (batch_norm_4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res_4): ResBlock1D(\n",
      "    (act): ReLU()\n",
      "    (conv1d_1): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (conv1d_2): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "    (batch_norm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (act_res_4): ReLU()\n",
      "  (conv_fin): Conv1d(256, 256, kernel_size=(249,), stride=(1,), padding=(124,))\n",
      "  (act_fin): ReLU()\n",
      "  (batch_fin): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 256, 79])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HYPERPARAMETERS\n",
    "J = 10 # max number of filters per class\n",
    "LR = 1e-3\n",
    "\n",
    "# build resent model and display the shape of feed through\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(5):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = 249, padding = 124, stride = 2))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = 249, padding = 124))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "    \n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 256, kernel_size = 249, padding = 124))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(256))\n",
    "\n",
    "print(conv_model)\n",
    "conv_model(full_x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters \n",
    "hidden_layers = 256\n",
    "embedding_dim = 256\n",
    "word_list_length = 98\n",
    "start_token = end_token = 0\n",
    "\n",
    "class LSTM_Encoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim):\n",
    "        super(LSTM_Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim, num_layers = 4, bidirectional = True)\n",
    "        \n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        if hidden is None and cell_state is None:\n",
    "            final, comp = self.lstm(x)\n",
    "        else:\n",
    "            final, comp = self.lstm(x, (hidden, cell_state))\n",
    "        hid, cell = comp\n",
    "        return final, hid, cell\n",
    "    \n",
    "    def initial_hidden_cell(self):\n",
    "        return torch.zeros(8, 1, 256), torch.zeros(8, 1, 256)\n",
    "    \n",
    "class LSTM_Decoder(nn.Module):\n",
    "    def __init__(self, h_dim, e_dim, word_list_length, max_length = 79):\n",
    "        super(LSTM_Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(word_list_length, e_dim)\n",
    "        self.attention = nn.Linear(h_dim*2, max_length)\n",
    "        self.attention_combined = nn.Linear(h_dim * 3, h_dim)\n",
    "        self.lstm = nn.LSTM(e_dim, h_dim)\n",
    "        self.out = nn.Linear(h_dim, word_list_length)\n",
    "        \n",
    "    def forward(self, x, hidden, cell_state, encoder_outputs):\n",
    "        seq_embedded = self.emb(x).view(1, 1, -1)\n",
    "        \n",
    "        attention_weights = F.softmax(self.attention(torch.cat((seq_embedded[0], hidden[0]), 1)), 1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = F.relu(self.attention_combined(torch.cat((seq_embedded[0], attention_applied[0]), 1)))\n",
    "        \n",
    "        final, states  = self.lstm(output.unsqueeze(0), (hidden, cell_state))\n",
    "        hidden, cell_state = states\n",
    "        dec_seq = self.out(final)\n",
    "        return F.log_softmax(dec_seq[0]), hidden, cell_state\n",
    "    \n",
    "    def initial_hidden_cell(self):\n",
    "        return torch.zeros(1, 1, 256), torch.zeros(1, 1, 256)\n",
    "\n",
    "def train(x, y, embedder, encoder, decoder, emb_optimizer, enc_optimizer, dec_optimizer, teacher_ratio = 0.5):\n",
    "    hidden_enc, cell_enc = encoder.initial_hidden_cell()\n",
    "    enc_outputs = torch.zeros(79, hidden_layers * 2)\n",
    "    \n",
    "    loss_fn = nn.NLLLoss()\n",
    "    loss = 0\n",
    "    \n",
    "    emb_optimizer.zero_grad()\n",
    "    enc_optimizer.zero_grad()\n",
    "    dec_optimizer.zero_grad()\n",
    "    \n",
    "    emb_x = embedder(x.unsqueeze(0)).permute(2, 0, 1)\n",
    "    \n",
    "    for i in range(len(emb_x)):\n",
    "        seq = emb_x[i].unsqueeze(0)\n",
    "        enc_out, hidden_enc, cell_enc = encoder(seq, hidden_enc, cell_enc)\n",
    "        enc_outputs[i] = enc_out[0, 0]\n",
    "    \n",
    "    hidden_dec, cell_dec = decoder.initial_hidden_cell()\n",
    "    target_lab = torch.tensor(y, dtype = torch.long)\n",
    "    decoder_input = torch.tensor([[start_token]], dtype = torch.long)\n",
    "    teacher_forcing = True if torch.rand(1) <= teacher_ratio else False\n",
    "    fin_len = 0\n",
    "    if teacher_forcing:\n",
    "        for j in range(len(target_lab)):\n",
    "            logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "            current_targ = target_lab[j].unsqueeze(0)\n",
    "            loss = loss_fn(logit, current_targ) + loss\n",
    "            decoder_input = current_targ\n",
    "            fin_len = j + 1\n",
    "            if decoder_input == end_token:\n",
    "                break\n",
    "    else:\n",
    "        for j in range(len(target_lab)):\n",
    "            logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "            _, val = logit.topk(1)\n",
    "            current_targ = target_lab[j].unsqueeze(0)\n",
    "            loss = loss_fn(logit, current_targ) + loss\n",
    "            decoder_input = val.squeeze(0).detach()\n",
    "            fin_len = j + 1\n",
    "            if decoder_input == end_token:\n",
    "                print(\"trained without teacher enforcing\")\n",
    "                break\n",
    "    \n",
    "    loss.backward()\n",
    "        \n",
    "    emb_optimizer.step()\n",
    "    enc_optimizer.step()\n",
    "    dec_optimizer.step()\n",
    "    \n",
    "    return (loss.item() / fin_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37, 60, 0], [55, 94, 0], [55, 94, 0], [3, 25, 60, 0], [55, 32, 16, 34, 53, 61, 0], [55, 95, 54, 31, 61, 18, 96, 89, 0], [3, 25, 60, 0], [55, 38, 57, 11, 17, 0], [55, 94, 11, 2, 72, 0], [78, 62, 40, 34, 53, 95, 52, 61, 3, 79, 9, 70, 56, 0], [9, 13, 68, 42, 80, 61, 9, 79, 3, 12, 33, 0], [92, 42, 61, 50, 12, 33, 89, 0], [92, 38, 90, 19, 61, 50, 6, 2, 72, 0], [55, 94, 0], [81, 41, 40, 58, 87, 61, 81, 79, 30, 0], [55, 94, 0], [9, 62, 40, 83, 53, 61, 9, 79, 3, 46, 56, 0], [78, 63, 49, 69, 39, 86, 42, 16, 34, 53, 52, 61, 77, 42, 21, 72, 0], [78, 63, 49, 69, 42, 52, 61, 69, 76, 72, 0], [5, 30, 79, 3, 14, 82, 0], [5, 0], [55, 94, 0], [55, 94, 0], [55, 94, 0], [55, 16, 47, 53, 61, 0], [55, 94, 0], [55, 38, 57, 7, 17, 7, 21, 72, 0], [55, 94, 0], [55, 94, 0], [37, 0], [3, 25, 60, 0], [55, 74, 17, 0], [55, 94, 0], [37, 60, 0], [78, 16, 47, 53, 61, 0], [55, 94, 0], [55, 94, 0], [55, 94, 0], [55, 34, 53, 61, 88, 56, 0], [30, 77, 0], [55, 34, 53, 61, 70, 4, 0], [55, 38, 47, 59, 61, 0], [55, 94, 0], [55, 94, 0], [55, 34, 59, 31, 61, 8, 56, 0], [55, 94, 20, 83, 66, 72, 0], [55, 94, 0], [55, 34, 59, 31, 61, 0], [55, 84, 61, 0], [55, 94, 0], [30, 14, 82, 0], [3, 25, 60, 0]]\n"
     ]
    }
   ],
   "source": [
    "def append_end_token(i):\n",
    "    i.append(end_token)\n",
    "    return i\n",
    "\n",
    "# define tokenizer\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "labels = list(full_y)\n",
    "for i, sentence in enumerate(labels):\n",
    "    labels[i] = sentence.split(\"; \")\n",
    "\n",
    "#print(tokenizer.get_vocab())\n",
    "token_y = [tokenizer.encode(label, is_pretokenized=True).ids for label in labels]\n",
    "for i in token_y:\n",
    "    i.append(0)\n",
    "\n",
    "#token_y = [torch.cat((i, torch.tensor([0]))) for i in token_y]\n",
    "encoder = LSTM_Encoder(hidden_layers, embedding_dim)\n",
    "decoder = LSTM_Decoder(hidden_layers, embedding_dim, word_list_length)\n",
    "\n",
    "emb_optimizer = torch.optim.Adam(conv_model.parameters(), lr = 1e-3)\n",
    "enc_optimizer = torch.optim.Adam(encoder.parameters(), lr = 1e-3)\n",
    "dec_optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.load_state_dict(torch.load('model/lstm_embedder.pt'))\n",
    "encoder.load_state_dict(torch.load('model/lstm_encoder.pt'))\n",
    "decoder.load_state_dict(torch.load(\"model/lstm_decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "3.377723107471956\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "2.478858398594554\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "2.2369191497726804\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.9743919696297128\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.786560043667117\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.4577151379742466\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.3849126616958274\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.2309476722545822\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.0717458533072433\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.1526549263122434\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.1871156830341865\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.3210894729131049\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.118645767801316\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.3421221359672708\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.1010351486294843\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "0.9553133572369295\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.3344150929673435\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.006751507488782\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.0702971874353298\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.1576311719134402\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "0.9045875657391347\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "0.9082199336491545\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.0230367105914149\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.2579805754914435\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.1457422887528694\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "0.9517808480029377\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.1227916084549128\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "0.9329015731870484\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "0.8733161161166957\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "trained without teacher enforcing\n",
      "1.0921583612815173\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/lstm_enc_dec')\n",
    "for epoch in range(30):\n",
    "    tot = 0.0\n",
    "    for j, k in zip(full_x, token_y):\n",
    "        loss = train(j, k, conv_model, encoder, decoder, emb_optimizer, enc_optimizer, dec_optimizer, teacher_ratio = 0.8)\n",
    "        tot = tot + loss\n",
    "    avg_loss = tot / len(token_y)\n",
    "    \n",
    "    print(avg_loss)\n",
    "    info_dict = {'train_loss': avg_loss }#, 'train_acc': train_accuracy, 'val_loss': avg_val_loss,'val_acc': val_accuracy}\n",
    "           \n",
    "    for tag, value in conv_model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_emb' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_emb' + '/grad', value.grad.data.cpu().numpy(), epoch)           \n",
    "    \n",
    "    for tag, value in encoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+ '_enc' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+ '_enc' + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "    \n",
    "    \n",
    "    for tag, value in decoder.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        writer.add_histogram(tag, value.data.cpu().numpy(), epoch)\n",
    "        if value.grad is None:\n",
    "            writer.add_histogram(tag+'_dec' + '/grad', 0, epoch)           \n",
    "        else:\n",
    "            writer.add_histogram(tag+'_dec' + '/grad', value.grad.data.cpu().numpy(), epoch)\n",
    "    \n",
    "\n",
    "    for tag, value in info_dict.items():\n",
    "        writer.add_scalar(tag, value, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(conv_model.state_dict(), 'model/lstm_embedder.pt')\n",
    "torch.save(encoder.state_dict(), 'model/lstm_encoder.pt')\n",
    "torch.save(decoder.state_dict(), 'model/lstm_decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth:  sinus bradycardia otherwise normal ecg [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm with sinus arrhythmia cannot rule out inferior infarct , age undetermined abnormal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm st & t wave abnormality, consider anterior ischemia prolonged qt abnormal ecg t wave inversion now evident in anterior leads qt has lengthened [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible left atrial enlargement incomplete left bundle branch block borderline ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg incomplete left bundle branch block is no longer present [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia left axis deviation pulmonary disease pattern inferior infarct , age undetermined st & t wave abnormality, consider lateral ischemia abnormal ecg sinus rhythm has replaced atrial fibrillation inverted t waves have replaced nonspecific t wave abnormality in lateral leads [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  atrial fibrillation with rapid ventricular response with premature ventricular or aberrantly conducted complexes right bundle branch block left posterior fascicular block abnormal ecg atrial fibrillation has replaced sinus rhythm qrs duration has increased [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  undetermined rhythm right bundle branch block abnormal ecg current undetermined rhythm precludes rhythm comparison, needs review qrs duration has increased qt has lengthened [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  undetermined rhythm possible right ventricular hypertrophy nonspecific t wave abnormality abnormal ecg current undetermined rhythm precludes rhythm comparison, needs review (rbbb and left posterior fascicular block) is no longer present [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic atrial pacemaker indeterminate axis pulmonary disease pattern st elevation consider anterolateral injury or acute infarct st elevation consider inferior injury or acute infarct abnormal ecg electronic atrial pacemaker has replaced electronic ventricular pacemaker [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  atrial fibrillation left axis deviation pulmonary disease pattern septal infarct , age undetermined abnormal ecg atrial fibrillation has replaced sinus rhythm nonspecific t wave abnormality has replaced inverted t waves in lateral leads [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia with short pr with premature supraventricular complexes and fusion complexes right bundle branch block cannot rule out inferior infarct , age undetermined t wave abnormality, consider lateral ischemia abnormal ecg previous ecg has undetermined rhythm, needs review right bundle branch block is now present [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia with short pr with premature supraventricular complexes right bundle branch block t wave abnormality, consider lateral ischemia abnormal ecg premature supraventricular complexes are now present [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  av sequential or dual chamber electronic pacemaker electronic ventricular pacemaker has replaced sinus rhythm vent. rate has decreased [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  av sequential or dual chamber electronic pacemaker [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm cannot rule out anterior infarct , age undetermined abnormal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible left atrial enlargement incomplete right bundle branch block borderline ecg incomplete right bundle branch block is now present [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus bradycardia [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm minimal voltage criteria for lvh, may be normal variant borderline ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus bradycardia otherwise normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus tachycardia cannot rule out anterior infarct , age undetermined abnormal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct , age undetermined abnormal ecg t wave amplitude has decreased in lateral leads [PAD]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic ventricular pacemaker previous ecg has undetermined rhythm, needs review [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct , age undetermined abnormal ecg inverted t waves have replaced nonspecific t wave abnormality in inferior leads [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm possible anterior infarct (cited on or before abnormal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct (cited on or before prolonged qt abnormal ecg t wave inversion no longer evident in lateral leads [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg criteria for septal infarct are no longer present [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm inferior infarct (cited on or before prolonged qt abnormal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm st abnormality, possible digitalis effect abnormal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  normal sinus rhythm normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  electronic ventricular pacemaker vent. rate has decreased [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ground truth:  sinus rhythm with marked sinus arrhythmia otherwise normal ecg [PAD]\n",
      "predicted:  with marked sinus arrhythmia\n",
      "predicted:  otherwise normal ecg\n",
      "predicted:  [PAD]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(full_x, token_y):\n",
    "    #print(x)\n",
    "    print(\"ground truth: \", \" \".join([tokenizer.id_to_token(i) for i in y]))\n",
    "    emb_x = conv_model(x.unsqueeze(0)).permute(2, 0, 1)\n",
    "    hidden_enc, cell_enc = encoder.initial_hidden_cell()\n",
    "    enc_outputs = torch.zeros(79, hidden_layers * 2)\n",
    "    \n",
    "    for i in range(len(emb_x)):\n",
    "        seq = emb_x[i].unsqueeze(0)\n",
    "        enc_out, hidden_enc, cell_enc = encoder(seq, hidden_enc, cell_enc)\n",
    "        enc_outputs[i] = enc_out[0, 0]       \n",
    "    \n",
    "    hidden_dec, cell_dec = decoder.initial_hidden_cell()\n",
    "    decoder_input = torch.tensor([[start_token]], dtype = torch.long)\n",
    "    for i in range(len(y)):\n",
    "        logit, hidden_dec, cell_dec = decoder(decoder_input, hidden_dec, cell_dec, enc_outputs)\n",
    "        _, val = logit.topk(1)\n",
    "        decoder_input = val.squeeze(0).detach()\n",
    "        print(\"predicted: \", tokenizer.id_to_token(decoder_input))\n",
    "        if decoder_input == end_token:\n",
    "            break\n",
    "    print(\"\\n\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 3 - Multi-Head Attention Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress, will clean later\n",
    "\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class ECGTransformerEncoder(nn.Module):\n",
    "    # Takes the ECG discrete signals sequence and maps into a probability distribution of diagnosis\n",
    "    # For working/verification purposes\n",
    "    def __init__(self, vector_size, embed_dim, n_heads, hidden_linear_dim, n_layers, dropout):\n",
    "        super(ECGTransformerEncoder, self).__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.positional_encoder = PositionalEncoder(embed_dim, dropout)\n",
    "    \n",
    "        #Since our data is already discrete numbers, might need some tweaking for this\n",
    "        self.embedder = conv_embedder\n",
    "                        #64 31              #39        64\n",
    "        \n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(embed_dim, n_heads, hidden_linear_dim, dropout),\n",
    "            n_layers)\n",
    "        \n",
    "        self.n_inputs = embed_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Simple linear decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(768, 17),\n",
    "                        Transpose(17, 2500),\n",
    "                        nn.Linear(2500, 30),\n",
    "                        nn.LogSoftmax()\n",
    "                        )\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        #self.embedder.weight.data.uniform_(-.1, .1)\n",
    "        #self.decoder.bias.data.zero_()\n",
    "        #self.decoder.weight.data.uniform_(-.1, .1)\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.embedder(x) # * math.sqrt(self.n_inputs)\n",
    "        x = x.squeeze(0)\n",
    "        #x = x.view(2500, 8)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(1) \n",
    "        #x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder 4 - FNET Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.linear_1 = nn.Linear(features, features * expansion)\n",
    "        self.linear_2 = nn.Linear(features * expansion, features)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        #self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.norm_1(x + res)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class FNETLayer(nn.Module):\n",
    "    def __init__(self, features, expansion, dropout):\n",
    "        super(FNETLayer, self).__init__()\n",
    "        self.feed_forward = FeedForwardNet(features, expansion, dropout)\n",
    "        self.norm_1 = nn.LayerNorm(features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = fft.fftn(x, dim = (-2, -1)).real\n",
    "        x = self.norm_1(x + res)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "    \n",
    "class FNETEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self, features, expansion=2, dropout=0.5, num_layers=6):\n",
    "        encoder_layer = FNETLayer(features, expansion, dropout)\n",
    "        super().__init__(encoder_layer=encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc Encoder Helper Functions/Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    # Necessary to store positional data about the input data\n",
    "    def __init__(self, embed_dim, dropout=0.1, max_len=2500, batch_size = 1):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pos_encoding = torch.zeros(max_len, 1, embed_dim)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        divisor = torch.exp(torch.arange(0, embed_dim, 2).float() * (- math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pos_encoding[:, 0, 0::2] = torch.sin(position * divisor)\n",
    "        pos_encoding[:, 0, 1::2] = torch.cos(position * divisor)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pos_encoding.repeat(1, x.shape[1], 1)\n",
    "        x = x + pos_encoding[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "   \n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the number of the last batch sample in the data set is smaller than the defined batch_batch size, mismatch problems will occur. You can modify it yourself, for example, just pass in the shape behind, and then enter it through x.szie(0).\n",
    "        return x.view(self.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class WindowEmbedder(nn.Module):\n",
    "    # Necessary to convert the signal into \"word\" vectors for transformer processing.\n",
    "    # Currently a simple group and slice method, but will modify later for multi-channel inputs\n",
    "    \n",
    "    def __init__(self, num_slices, size_of_slice):\n",
    "        super(SignalEmbedder, self).__init__()\n",
    "        self.num_slices = num_slices\n",
    "        self.size_of_slice = size_of_slice\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x[: self.num_slices * self.size_of_slice]\n",
    "        x = x.reshape((self.num_slices, self.size_of_slice))\n",
    "        return x  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder 1 - Huggingface GPT2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with child class of GPT2LMHeadModel\n",
    "\n",
    "class GPT2LMHeadModel(GPT2PreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [r\"attn.masked_bias\", r\"attn.bias\", r\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)\n",
    "        #self.transformer.forward = forward2.__get__(self.transformer, GPT2Model)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "    def parallelize(self, device_map=None):\n",
    "        self.device_map = (\n",
    "            get_device_map(len(self.transformer.h), range(torch.cuda.device_count()))\n",
    "            if device_map is None\n",
    "            else device_map\n",
    "        )\n",
    "        assert_device_map(self.device_map, len(self.transformer.h))\n",
    "        self.transformer.parallelize(self.device_map)\n",
    "        self.lm_head = self.lm_head.to(self.transformer.first_device)\n",
    "        self.model_parallel = True\n",
    "        \n",
    "    def deparallelize(self):\n",
    "        self.transformer.deparallelize()\n",
    "        self.transformer = self.transformer.to(\"cpu\")\n",
    "        self.lm_head = self.lm_head.to(\"cpu\")\n",
    "        self.model_parallel = False\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        token_type_ids = kwargs.get(\"token_type_ids\", None)\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            position_ids = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"encoder_hidden_states\": kwargs.get(\"encoder_hidden_states\", None), # The one line changed hehe\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\n",
    "            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "\n",
    "        # Set device for model parallelism\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(self.transformer.first_device)\n",
    "            hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=transformer_outputs.past_key_values,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "            cross_attentions=transformer_outputs.cross_attentions,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _reorder_cache(past: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the :obj:`past_key_values` cache if\n",
    "        :meth:`~transformers.PreTrainedModel.beam_search` or :meth:`~transformers.PreTrainedModel.beam_sample` is\n",
    "        called. This is required to match :obj:`past_key_values` with the correct beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n",
    "            for layer_past in past\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# preprocess training labels and tokenize\n",
    "train_labels = list(waveform_lead_rhythm_diag['diagnosis'])\n",
    "inputs = tokenizer(train_labels, padding = True, verbose = False, return_tensors=\"pt\")\n",
    "\n",
    "#Necessary to add for generating first word\n",
    "inputs[\"input_ids\"] = torch.cat((torch.tensor([[50256] for i in range(len(inputs[\"input_ids\"]))]), inputs[\"input_ids\"]), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((torch.tensor([[1] for i in range(len(inputs[\"attention_mask\"]))]), inputs[\"attention_mask\"]), dim=1)\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoder - FNET Encoder Huggingface Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder decoder model with GPT2 \n",
    "class CustEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder):\n",
    "        super(CustEncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pos_enb = PositionalEncoder(embed_dim = 768, batch_size = 7)\n",
    "        self.embedder = embedder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ecgs, labels = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        out = self.decoder(**labels, labels = labels[\"input_ids\"], encoder_hidden_states = x.contiguous())\n",
    "        return out\n",
    "    \n",
    "    # Should only take 1 input at a time\n",
    "    def predict_single(self, x):\n",
    "        ecgs = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder.generate(encoder_hidden_states = x.contiguous())[0]\n",
    "\n",
    "    \n",
    "    # Takes in multiple inputs\n",
    "    def predict_batch(self, x):\n",
    "        ecgs = x\n",
    "        x = self.embedder(ecgs).permute(2, 0, 1)\n",
    "        x = self.pos_enb(x).permute(1, 0, 2)\n",
    "        x = self.encoder(x)\n",
    "        output = []\n",
    "        for ecg in x:\n",
    "            output.append(tokenizer.decode(self.decoder.generate(encoder_hidden_states = ecg.unsqueeze(0).contiguous())[0]))\n",
    "        return output\n",
    "    \n",
    "    def return_enc(self):\n",
    "        return self.encoder\n",
    "\n",
    "    \n",
    "# Connect an embedder and de-embedder for training (we will then isolate the Encoder portion of this autoencoder as our embedder)\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConvAutoEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def make_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def make_decoder(self):\n",
    "        return self.decoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is for the pre-embedder\n",
    "    \n",
    "# Make embedder\n",
    "conv_model = nn.Sequential()\n",
    "init_channels = 8\n",
    "for i in range(2):\n",
    "    next_channels = 2 * init_channels\n",
    "    conv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    conv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    conv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    conv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    conv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "conv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 768, kernel_size = KER_SIZE, padding = PADDING))\n",
    "conv_model.add_module('act_fin', nn.ReLU())\n",
    "conv_model.add_module('batch_fin', nn.BatchNorm1d(768))\n",
    "\n",
    "\n",
    "# Make de-embedder\n",
    "deconv_model = nn.Sequential()\n",
    "init_channels = 768\n",
    "for i in range(2):\n",
    "    next_channels = init_channels // 2\n",
    "    deconv_model.add_module('conv_{num}'.format(num = i), nn.Conv1d(in_channels = init_channels, out_channels = next_channels, kernel_size = KER_SIZE, padding = PADDING, stride = 1))\n",
    "    deconv_model.add_module('act_{num}'.format(num = i), nn.ReLU())\n",
    "    deconv_model.add_module('batch_norm_{num}'.format(num = i), nn.BatchNorm1d(next_channels))\n",
    "    deconv_model.add_module('res_{num}'.format(num = i), ResBlock1D(num_filters = next_channels, kernel_size = KER_SIZE, padding = PADDING))\n",
    "    deconv_model.add_module('act_res_{num}'.format(num = i), nn.ReLU())\n",
    "    init_channels = next_channels\n",
    "deconv_model.add_module('conv_fin', nn.Conv1d(in_channels = init_channels, out_channels = 8, kernel_size = KER_SIZE, padding = PADDING))\n",
    "deconv_model.add_module('act_fin', nn.ReLU())\n",
    "deconv_model.add_module('batch_fin', nn.BatchNorm1d(8))\n",
    "\n",
    "auto_model = ConvAutoEncoder(conv_model, deconv_model).to(device)\n",
    "auto_optimizer = torch.optim.Adam(auto_model.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "# Training params\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    auto_optimizer.zero_grad()\n",
    "    outputs = auto_model(ecg_data)\n",
    "    loss = loss_function(outputs, ecg_data)\n",
    "    loss.backward(retain_graph=True)\n",
    "    auto_optimizer.step()\n",
    "    print(loss)\n",
    "        \n",
    "# Saving/loading weights\n",
    "torch.save(auto_model.state_dict(), 'model/autoencoder.pt')\n",
    "auto_model.load_state_dict(torch.load('model/autoencoder.pt'))\n",
    "conv_embedder = auto_model.make_encoder()\n",
    "torch.save(conv_embedder.state_dict(), \"model/embedder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define encoder, we don't need to pretrain rn\n",
    "encoder = FNETEncoder(768, expansion = 2, dropout=0.1, num_layers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.1.crossattention.masked_bias', 'h.2.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.3.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.4.crossattention.masked_bias', 'h.10.crossattention.q_attn.weight', 'h.0.ln_cross_attn.weight', 'h.3.ln_cross_attn.weight', 'h.9.crossattention.bias', 'h.1.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.7.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.6.crossattention.bias', 'h.10.crossattention.masked_bias', 'h.11.crossattention.masked_bias', 'h.5.crossattention.c_proj.bias', 'h.0.crossattention.masked_bias', 'h.0.crossattention.bias', 'h.4.crossattention.bias', 'h.9.crossattention.masked_bias', 'h.8.crossattention.q_attn.weight', 'h.9.ln_cross_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.8.crossattention.bias', 'h.9.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.7.crossattention.bias', 'h.4.ln_cross_attn.weight', 'h.8.ln_cross_attn.weight', 'h.2.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.1.ln_cross_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.8.crossattention.masked_bias', 'h.10.crossattention.bias', 'h.11.crossattention.c_proj.weight', 'h.10.ln_cross_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.2.crossattention.masked_bias', 'h.5.crossattention.bias', 'h.7.crossattention.c_proj.bias', 'h.3.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.6.ln_cross_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.6.crossattention.masked_bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.bias', 'h.9.crossattention.c_proj.weight', 'h.11.crossattention.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.weight', 'h.3.crossattention.bias', 'h.4.crossattention.q_attn.weight', 'h.7.crossattention.masked_bias', 'h.11.ln_cross_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.2856, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# define and pretrain Decoder\n",
    "decoder = GPT2LMHeadModel.from_pretrained('gpt2', config = GPT2Config(add_cross_attention = True, is_encoder_decoder = True))\n",
    "\n",
    "# pretrain decoder\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr = 1e-3)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 1\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = decoder(**inputs, labels = inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "torch.save(decoder.state_dict(), 'model/gpt2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '<|endoftext|>\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '<|endoftext|>\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '<|endoftext|>\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '<|endoftext|>\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '<|endoftext|>\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '<|endoftext|>\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define component models\n",
    "\n",
    "conv_embedder = auto_model.make_encoder()\n",
    "\n",
    "encoder = FNETEncoder(768, expansion = 2, dropout=0.1, num_layers = 6)\n",
    "\n",
    "#decoder.load_state_dict(torch.load('model/gpt2.pt'))\n",
    "\n",
    "enc_dec_model = CustEncoderDecoder(encoder, decoder, conv_embedder)\n",
    "\n",
    "enc_dec_model.predict_batch(ecg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielbang/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  ../aten/src/ATen/native/Copy.cpp:162.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7518, grad_fn=<NllLossBackward>)\n",
      "tensor(3.0280, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# train encoder decoder model!\n",
    "optimizer = torch.optim.Adam(enc_dec_model.parameters(), lr = 1e-5)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# set number of epochs\n",
    "epochs = 2\n",
    "\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = enc_dec_model((ecg_data, inputs))\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "torch.save(enc_dec_model.state_dict(), 'model/gpt2_enc_dec.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_model.load_state_dict(torch.load('model/gpt2_enc_dec.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/gpt2.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31369,   385,   865,  4597,  9517,   544,  4306,  3487,  9940,    70,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([ 9124, 11439,  9940,    70,  3781,  3487,  7813,   385, 18662,  3487,\n",
      "         9940,    70, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([ 9124, 11439,  9940,    70,  3781,  3487,  7813,   385, 18662,  3487,\n",
      "         9940,    70, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385, 18662,   351,  7498,  7813,   385,  5240,    71,  5272,\n",
      "        20730,  4306,  3487,  9940,    70, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,   351,  7813,   385,  5240,    71,  5272,\n",
      "        20730,  2314,  3896,   503, 18536,  1167,   283,   310,   220,  2479,\n",
      "         3318, 23444, 18801,  9940,    70, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,   336,   220,   256,  6769, 42364,  1483,\n",
      "         2074, 32700,   318, 15245,   544, 20573, 10662,    83, 18801,  9940,\n",
      "           70,   256,  6769,   287,  9641,   783, 10678,   287, 32700,  5983,\n",
      "        10662,    83,   468, 40038,  8524,   276, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385, 18662,   351,  7498,  7813,   385,  5240,    71,  5272,\n",
      "        20730,  4306,  3487,  9940,    70, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  1744,  1364,   379,  4454, 26537,   972,\n",
      "        17503,  1364, 18537,  8478,  2512, 38422,  9940,    70, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 17503,  1364, 18537,\n",
      "         8478,  2512,   318,   645,  2392,  1944, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385,   256, 35586,  9517,   544,  1364, 16488, 28833, 45105,\n",
      "         4369,  3912, 18536,  1167,   283,   310,   220,  2479,  3318, 23444,\n",
      "          336,   220,   256,  6769, 42364,  1483,  2074, 25653,   318, 15245,\n",
      "          544, 18801,  9940,    70,  7813,   385, 18662,   468,  6928,   379,\n",
      "         4454,   277,  2889, 40903, 37204,   256,  9813,   423,  6928, 14011,\n",
      "          431,  7790,   256,  6769, 42364,  1483,   287, 25653,  5983,  7813,\n",
      "          385, 18662,   351, 21748, 42665,   826, 18537,  8478,  2512,  1364,\n",
      "        34319,  7751, 13174,  2512,   220,   275,   361,  3372, 13174,  2512,\n",
      "          220, 18801,  9940,    70,  7813,   385, 18662,   468,  6928,  7914,\n",
      "         7435, 41001, 23503, 32174,   379,  4454,   277,  2889, 40903,   351,\n",
      "         5801,  7435, 41001,  2882,   351, 19905,  7435, 41001,   393,   450,\n",
      "         8056,  3875,  5952, 42665,   826, 18537,  8478,  2512,  1364, 34319,\n",
      "         7751, 13174,  2512,   220,   275,   361,  3372, 13174,  2512,   220,\n",
      "        18801,  9940,    70,   379,  4454,   277,  2889, 40903,   468,  6928,\n",
      "         7813,   385, 18662, 10662,  3808,  9478,   468,  3220,  3318, 23444,\n",
      "        18662,   826, 18537,  8478,  2512, 18801,  9940,    70,  1459,  3318,\n",
      "        23444, 18662,   662, 13955, 18662,  7208,  2476,  2423, 10662,  3808,\n",
      "         9478,   468,  3220, 10662,    83,   468, 40038,  8524,   276,  3318,\n",
      "        23444, 18662,  1744,   826,  7435, 41001,  8718, 23528,  6883, 14011,\n",
      "          431,  7790,   256,  6769, 42364,  1483, 18801,  9940,    70,  1459,\n",
      "         3318, 23444, 18662,   662, 13955, 18662,  7208,  2476,  2423,   374,\n",
      "        11848,    65,   290,  1364, 34319,  7751, 13174,  2512,   318,   645,\n",
      "         2392,  1944])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([ 9509,  4565,   379,  4454, 23503, 32174,   773, 13221,   378, 16488,\n",
      "        45105,  4369,  3912,   336, 22910,  2074,   281,   353,   349, 10534,\n",
      "         5095,   393, 14352,  1167,   283,   310,   336, 22910,  2074, 18536,\n",
      "         5095,   393, 14352,  1167,   283,   310, 14352, 21504, 18801,  9940,\n",
      "           70,  7914,   379,  4454, 23503, 32174,   468,  6928,  7914,  7435,\n",
      "        41001, 23503, 32174, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([  265,  4454,   277,  2889, 40903,  1364, 16488, 28833, 45105,  4369,\n",
      "         3912,   384,   457,   282,  1167,   283,   310,   220,  2479,  3318,\n",
      "        23444, 18801,  9940,    70,   379,  4454,   277,  2889, 40903,   468,\n",
      "         6928,  7813,   385, 18662, 14011,   431,  7790,   256,  6769, 42364,\n",
      "         1483,   468,  6928, 37204,   256,  9813,   287, 25653,  5983, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385,   256, 35586,  9517,   544,   351,  1790,   778,   351,\n",
      "        19905, 28614,  1151, 41001, 42665,   290, 21748, 42665,   826, 18537,\n",
      "         8478,  2512,  2314,  3896,   503, 18536,  1167,   283,   310,   220,\n",
      "         2479,  3318, 23444,   256,  6769, 42364,  1483,  2074, 25653,   318,\n",
      "        15245,   544, 18801,  9940,    70,  2180,  9940,    70,   468,  3318,\n",
      "        23444, 18662,  2476,  2423,   826, 18537,  8478,  2512,   318,   783,\n",
      "         1944, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385,   256, 35586,  9517,   544,   351,  1790,   778,   351,\n",
      "        19905, 28614,  1151, 41001, 42665,   826, 18537,  8478,  2512,   256,\n",
      "         6769, 42364,  1483,  2074, 25653,   318, 15245,   544, 18801,  9940,\n",
      "           70, 19905, 28614,  1151, 41001, 42665,   389,   783,  1944, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([  615, 35582,   393, 10668, 11847,  7914, 23503, 32174,  7914,  7435,\n",
      "        41001, 23503, 32174,   468,  6928,  7813,   385, 18662,  7435,  2494,\n",
      "          468, 11832, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([  615, 35582,   393, 10668, 11847,  7914, 23503, 32174,  3487,  7813,\n",
      "          385, 18662,  3487,  7813,   385, 18662,   351,  7813,   385,  5240,\n",
      "           71,  5272, 20730,   351,  7813,   385,  5240,    71,  5272, 20730,\n",
      "         3487,  9940,    70,  3487,  9940,    70,  3487,  7813,   385, 18662,\n",
      "         3487,  9940,    70, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([ 9124, 11439,  9940,    70,  3781,  3487,  7813,   385, 18662,  3487,\n",
      "         9940,    70, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  2314,  3896,   503, 32700,  1167,   283,\n",
      "          310,   220,  2479,  3318, 23444, 18801,  9940,    70, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  1744,  1364,   379,  4454, 26537,   972,\n",
      "        17503,   826, 18537,  8478,  2512, 38422,  9940,    70, 17503,   826,\n",
      "        18537,  8478,  2512,   318,   783,  1944, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([ 9124, 11439,  9940,    70,  3781,  7813,   385,   865,  4597,  9517,\n",
      "          544, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385, 18662,   351,  7498,  7813,   385,  5240,    71,  5272,\n",
      "        20730,  4306,  3487,  9940,    70, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662, 10926, 15004,  9987,   329,   300,    85,\n",
      "           71,   743,   307,  3487, 15304, 38422,  9940,    70, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385,   865,  4597,  9517,   544,  4306,  3487,  9940,    70,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385,   256, 35586,  9517,   544,  2314,  3896,   503, 32700,\n",
      "         1167,   283,   310,   220,  2479,  3318, 23444, 18801,  9940,    70,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([ 9124, 11439,  9940,    70,  3781,  3487,  7813,   385, 18662,  3487,\n",
      "         9940,    70, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70,  3487,  7813,   385,\n",
      "        18662,  3487,  9940,    70, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662, 18536,  1167,   283,   310,   220,  2479,\n",
      "         3318, 23444, 18801,  9940,    70,   256,  6769, 37188,   468, 11832,\n",
      "          287, 25653,  5983,  3318, 23444, 18662,   826, 16488, 28833,   826,\n",
      "         7435, 41001,  8718, 23528,  6883, 14011,   431,  7790,   336,   290,\n",
      "          256,  6769, 42364,  1483, 18801,  9940,    70,  1459,  3318, 23444,\n",
      "        18662,   662, 13955, 18662,  7208,  2476,  2423, 18269,  1487,   287,\n",
      "        10662,  3808,  9478, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([ 9509,  4565,  7435, 41001, 23503, 32174,  2180,  9940,    70,   468,\n",
      "         3318, 23444, 18662,  2476,  2423, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662, 18536,  1167,   283,   310,   220,  2479,\n",
      "         3318, 23444, 18801,  9940,    70, 37204,   256,  9813,   423,  6928,\n",
      "        14011,   431,  7790,   256,  6769, 42364,  1483,   287, 18536,  5983,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  1744, 32700,  1167,   283,   310,  9181,\n",
      "          319,   393,   878, 18801,  9940,    70, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662, 18536,  1167,   283,   310,  9181,   319,\n",
      "          393,   878, 20573, 10662,    83, 18801,  9940,    70,   256,  6769,\n",
      "          287,  9641,   645,  2392, 10678,   287, 25653,  5983, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70,  9987,   329,   384,\n",
      "          457,   282,  1167,   283,   310,   389,   645,  2392,  1944, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662, 18536,  1167,   283,   310,  9181,   319,\n",
      "          393,   878, 20573, 10662,    83, 18801,  9940,    70, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,   336, 42364,  1483,  1744,  4875,   271,\n",
      "         1245, 18801,  9940,    70, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([11265,  7813,   385, 18662,  3487,  9940,    70, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([ 9509,  4565,  7435, 41001, 23503, 32174,  7435,  2494,   468, 11832,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n",
      "tensor([31369,   385, 18662,   351,  7498,  7813,   385,  5240,    71,  5272,\n",
      "        20730,  4306,  3487,  9940,    70, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256])\n"
     ]
    }
   ],
   "source": [
    "for i in token_y:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
